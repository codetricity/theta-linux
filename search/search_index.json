{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RICOH THETA Development on Linux Overview Stream 360 video, control the RICOH THETA API, and supply power to the camera using the USB cable. It's awesome! Video to your Linux computer is 4K at 30fps with under 300ms latency. Works with the RICOH THETA V or RICOH THETA Z1. It's perfect for OpenCV object detection, autonomous drones, building analysis, AI, and TensorFlow projects. The camera can be powered from the USB cable and can switch into live streaming mode, still image, video, or bracket shooting using API commands. We explain the tools, the repos, and the API. Up to date guides, events, and a general pile of great stuff is at the theta360.guide Linux Streaming Site . It's an exciting world. Let's getting started. If you're eager and using Ubuntu 20.04 $ git clone https://github.com/ricohapi/libuvc-theta.git $ sudo apt install libjpeg-dev $ cd libuvc-theta $ mkdir build $ cd build $ cmake .. $ make $ sudo make install $ cd ../.. $ git clone https://github.com/ricohapi/libuvc-theta-sample.git $ cd libuvc-theta-sample/gst $ make # THETA must be plugged into your computer and in # live streaming mode $ ./gst_viewer This assumes you have a non-THETA webcam on /dev/video0 (your laptop or desktop cam). You may need to edit the source if your THETA is the only camera on your computer. If the build fails, you may need a few gstreamer packages. The command below installs everything. You do not need everything $ sudo apt-get install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio libgstreamer-plugins-base1.0-dev","title":"Home"},{"location":"#ricoh-theta-development-on-linux","text":"","title":"RICOH THETA Development on Linux"},{"location":"#overview","text":"Stream 360 video, control the RICOH THETA API, and supply power to the camera using the USB cable. It's awesome! Video to your Linux computer is 4K at 30fps with under 300ms latency. Works with the RICOH THETA V or RICOH THETA Z1. It's perfect for OpenCV object detection, autonomous drones, building analysis, AI, and TensorFlow projects. The camera can be powered from the USB cable and can switch into live streaming mode, still image, video, or bracket shooting using API commands. We explain the tools, the repos, and the API. Up to date guides, events, and a general pile of great stuff is at the theta360.guide Linux Streaming Site . It's an exciting world. Let's getting started. If you're eager and using Ubuntu 20.04 $ git clone https://github.com/ricohapi/libuvc-theta.git $ sudo apt install libjpeg-dev $ cd libuvc-theta $ mkdir build $ cd build $ cmake .. $ make $ sudo make install $ cd ../.. $ git clone https://github.com/ricohapi/libuvc-theta-sample.git $ cd libuvc-theta-sample/gst $ make # THETA must be plugged into your computer and in # live streaming mode $ ./gst_viewer This assumes you have a non-THETA webcam on /dev/video0 (your laptop or desktop cam). You may need to edit the source if your THETA is the only camera on your computer. If the build fails, you may need a few gstreamer packages. The command below installs everything. You do not need everything $ sudo apt-get install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio libgstreamer-plugins-base1.0-dev","title":"Overview"},{"location":"camera/","text":"You need a THETA V or Z1. These cameras live stream with UVC 1.5 in equirectangular. You can also use a THETA S to stream motionJPEG in dual-fisheye. Streaming Specifications V and Z1 USB streaming. 4K\uff0cH264: 3840\u00d71920/29.97fps/120Mbps 2K\uff0cH264: 1920\u00d7960/29.97fps/42Mbps The modes for the sample program are specified in thetauvc.c . You can specify the resolution in gst_viewer.c . An example of changing the resolution is here . Sleep and Power Overview The camera has three power states: Power off Power on Sleep You can use the THETA API over a USB cable to control power off, sleep, and awake. To turn on the camera from power off, you can use a C library to power cycle the USB ports on a small board computer such as a Jetson Nano. This is an unofficial community workaround and is not supported by RICOH. Function Method Sleep USB API Awake USB API Power Off USB API Power On workaround with C library Sleep and Wake If your application can provide power to the THETA or if the THETA does not have to be dormant for a long time, it is better to use sleep and awake. You can use the USB API to control sleep and awake. We used the following process to test sleep and awake: Disable auto power off delay and disable auto sleep - you only need to do this once. The camera will save the setting. Put camera to sleep Wake camera from sleep Check if camera status is ready. If the camera is still asleep, send the wake camera again. If you do not have any problems waking the camera up from sleep on the first attempt, you may not need step 4. Power Off and Power On If you require power off and power on for applications such as placing the THETA on a robot, then shipping the robot to another site, you can use a Nano or Raspberry Pi to power on the THETA. Turn Camera Off The camera can be turned off with the USB API. $ ptpcam -R 0x1013 Turn Camera On Using Jetson Nano There is no official way to turn on the camera once it is in a power off state. We recommend that you use sleep and awake. If this is not an option, you can turn the camera on by power cycling the USB ports of the Jetson Nano. This example uses libusb. Sample Code for Jetson Nano /**************** * Tested on Jetson Nano running JetPack 4.4 * The RICOH THETA V and Z1 will turn on from a power off state when * the USB cable is plugged into a port. This example will power cycle * the USB ports of the Nano. You must have libusb-1.0-0-dev installed * on the Nano and link to it. * * Additional information on using libusb_control_transfer is below. * https://www.cs.unm.edu/~hjelmn/libusb_hotplug_api/group__syncio.html *****************/ #include <stdio.h> #include <string.h> #include <stdlib.h> #include <stdint.h> #include <unistd.h> #include <libusb-1.0/libusb.h> unsigned vid = 0x0bda; unsigned pid = 0x5411; int power_cycle(libusb_device_handle *hub_devh) { int ret = -1; /*ep0 vendor command enable*/ ret = libusb_control_transfer(hub_devh, 0x40, 0x02, 0x01, ((0x0B<<8)|(0xDA)), 0, 0, 100000); if (ret < 0) { printf(\"[error]:ep0 vendor command enable fail.\\n\"); return ret; } /*ep0 vendor command disable*/ libusb_control_transfer(hub_devh, 0x40, 0x1, 0x08, 0, NULL, 0, 100); libusb_control_transfer(hub_devh, 0x40, 0x3, 0x08, 0, NULL, 0, 100); libusb_control_transfer(hub_devh, 0x40, 0x02, 0x00, ((0x0B<<8)|(0xDA)), 0, 0, 100000); return ret; } int main(int argc, char *argv[]) { int ret=0; libusb_device_handle *hub_devh; libusb_context *context; ret = libusb_init(&context); if (ret != 0){ printf(\"[error]:libusb init fail.\\n\"); return ret; } hub_devh = libusb_open_device_with_vid_pid(context, vid, pid); if (!hub_devh) { printf(\"[error]:open device %04x:%04x fail.\\n\", vid, pid); return -1; } ret = power_cycle(hub_devh); return ret; } You can test the code by first saving the code in a file called, reset_jetson_usb_power.c then follow these steps: $ sudo apt-get install libusb-1.0-0-dev $ gcc -o power_cycle reset_jetson_usb_power.c -lusb-1.0 $ sudo ./power_cycle Turn Camera On Using Raspberry Pi install libusb-dev sudo apt-get install libusb-dev install hub-ctrl.c git clone https://github.com/codetricity/hub-ctrl.c make and then install in /usr/local/bin create shell script to cycle power Save the following into /usr/local/sbin/cycle-power.sh . #!/bin/bash /usr/local/bin/hub-ctrl -h 0 -P 2 -p 0 sleep 2 /usr/local/bin/hub-ctrl -h 0 -P 2 -p 1 Optional - Enable script to run without sudo password This is a potential security risk. If you want to avoid having to enter a password and are comfortable with the risk, then follow the steps in this article .","title":"Camera"},{"location":"camera/#streaming-specifications","text":"V and Z1 USB streaming. 4K\uff0cH264: 3840\u00d71920/29.97fps/120Mbps 2K\uff0cH264: 1920\u00d7960/29.97fps/42Mbps The modes for the sample program are specified in thetauvc.c . You can specify the resolution in gst_viewer.c . An example of changing the resolution is here .","title":"Streaming Specifications"},{"location":"camera/#sleep-and-power-overview","text":"The camera has three power states: Power off Power on Sleep You can use the THETA API over a USB cable to control power off, sleep, and awake. To turn on the camera from power off, you can use a C library to power cycle the USB ports on a small board computer such as a Jetson Nano. This is an unofficial community workaround and is not supported by RICOH. Function Method Sleep USB API Awake USB API Power Off USB API Power On workaround with C library","title":"Sleep and Power Overview"},{"location":"camera/#sleep-and-wake","text":"If your application can provide power to the THETA or if the THETA does not have to be dormant for a long time, it is better to use sleep and awake. You can use the USB API to control sleep and awake. We used the following process to test sleep and awake: Disable auto power off delay and disable auto sleep - you only need to do this once. The camera will save the setting. Put camera to sleep Wake camera from sleep Check if camera status is ready. If the camera is still asleep, send the wake camera again. If you do not have any problems waking the camera up from sleep on the first attempt, you may not need step 4.","title":"Sleep and Wake"},{"location":"camera/#power-off-and-power-on","text":"If you require power off and power on for applications such as placing the THETA on a robot, then shipping the robot to another site, you can use a Nano or Raspberry Pi to power on the THETA.","title":"Power Off and Power On"},{"location":"camera/#turn-camera-off","text":"The camera can be turned off with the USB API. $ ptpcam -R 0x1013","title":"Turn Camera Off"},{"location":"camera/#turn-camera-on-using-jetson-nano","text":"There is no official way to turn on the camera once it is in a power off state. We recommend that you use sleep and awake. If this is not an option, you can turn the camera on by power cycling the USB ports of the Jetson Nano. This example uses libusb.","title":"Turn Camera On Using Jetson Nano"},{"location":"camera/#sample-code-for-jetson-nano","text":"/**************** * Tested on Jetson Nano running JetPack 4.4 * The RICOH THETA V and Z1 will turn on from a power off state when * the USB cable is plugged into a port. This example will power cycle * the USB ports of the Nano. You must have libusb-1.0-0-dev installed * on the Nano and link to it. * * Additional information on using libusb_control_transfer is below. * https://www.cs.unm.edu/~hjelmn/libusb_hotplug_api/group__syncio.html *****************/ #include <stdio.h> #include <string.h> #include <stdlib.h> #include <stdint.h> #include <unistd.h> #include <libusb-1.0/libusb.h> unsigned vid = 0x0bda; unsigned pid = 0x5411; int power_cycle(libusb_device_handle *hub_devh) { int ret = -1; /*ep0 vendor command enable*/ ret = libusb_control_transfer(hub_devh, 0x40, 0x02, 0x01, ((0x0B<<8)|(0xDA)), 0, 0, 100000); if (ret < 0) { printf(\"[error]:ep0 vendor command enable fail.\\n\"); return ret; } /*ep0 vendor command disable*/ libusb_control_transfer(hub_devh, 0x40, 0x1, 0x08, 0, NULL, 0, 100); libusb_control_transfer(hub_devh, 0x40, 0x3, 0x08, 0, NULL, 0, 100); libusb_control_transfer(hub_devh, 0x40, 0x02, 0x00, ((0x0B<<8)|(0xDA)), 0, 0, 100000); return ret; } int main(int argc, char *argv[]) { int ret=0; libusb_device_handle *hub_devh; libusb_context *context; ret = libusb_init(&context); if (ret != 0){ printf(\"[error]:libusb init fail.\\n\"); return ret; } hub_devh = libusb_open_device_with_vid_pid(context, vid, pid); if (!hub_devh) { printf(\"[error]:open device %04x:%04x fail.\\n\", vid, pid); return -1; } ret = power_cycle(hub_devh); return ret; } You can test the code by first saving the code in a file called, reset_jetson_usb_power.c then follow these steps: $ sudo apt-get install libusb-1.0-0-dev $ gcc -o power_cycle reset_jetson_usb_power.c -lusb-1.0 $ sudo ./power_cycle","title":"Sample Code for Jetson Nano"},{"location":"camera/#turn-camera-on-using-raspberry-pi","text":"","title":"Turn Camera On Using Raspberry Pi"},{"location":"camera/#install-libusb-dev","text":"sudo apt-get install libusb-dev","title":"install libusb-dev"},{"location":"camera/#install-hub-ctrlc","text":"git clone https://github.com/codetricity/hub-ctrl.c make and then install in /usr/local/bin","title":"install hub-ctrl.c"},{"location":"camera/#create-shell-script-to-cycle-power","text":"Save the following into /usr/local/sbin/cycle-power.sh . #!/bin/bash /usr/local/bin/hub-ctrl -h 0 -P 2 -p 0 sleep 2 /usr/local/bin/hub-ctrl -h 0 -P 2 -p 1","title":"create shell script to cycle power"},{"location":"camera/#optional-enable-script-to-run-without-sudo-password","text":"This is a potential security risk. If you want to avoid having to enter a password and are comfortable with the risk, then follow the steps in this article .","title":"Optional - Enable script to run without sudo password"},{"location":"demos/","text":"Ongoing Tests with Linux Streaming Using Nvidia Jetson Nano live streaming from a THETA V. Processing done with Python3, OpenCV 4.4. Scroll down for code. DetectNet Running live on Jetson Nano with RICOH THETA Z1. DetectNet applied to both single frame with SSD Mobilenet-v2 to assess accuracy and to live stream to assess framerate. Works good on both. Video demo with Jetson Nano . See Jetson Nano inference benchmarks . Code is available in the at https://github.com/dusty-nv/jetson-inference There is super small text in the green box that says, \"person\". The system accurately detected the only person in the image. It is 88.6 percent confident that I am a person. Nice. Despite the distorted view of my feet, the program does detect the human form. Even at night, in low-light conditions with me on the side of the shutter button, the program did detect me. However, there were many frames where I was not detected. To proceed, you will likely need a database of fisheye or equirectangular images to build your own model. Sample Code import jetson.inference import jetson.utils net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5) camera = jetson.utils.gstCamera(1280, 720, \"/dev/video0\") display = jetson.utils.glDisplay() while display.IsOpen(): img, width, height = camera.CaptureRGBA() detections = net.Detect(img, width, height) display.RenderOnce(img, width, height) display.SetTitle(\"RICOH THETA Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS())) OpenCV Python Works on live stream. Procedure install libuvc-theta install libuv-theta-sample install v4l2loopback load kernel modules for v4l2loopback and verify that /dev/video0 or equivalent shows THETA stream run Python script with cv2 Recommend you recompile OpenCV 4.4 from source code. May take 2.5 hours if you compile on the Nano. Simple Python cv2 Test Frame resize test. import cv2 cap = cv2.VideoCapture(0) # Check if the webcam is opened correctly if not cap.isOpened(): raise IOError(\"Cannot open webcam\") while True: ret, frame = cap.read() frame = cv2.resize(frame, None, fx=0.25, fy=0.25, interpolation=cv2.INTER_AREA) cv2.imshow('Input', frame) c = cv2.waitKey(1) if c == 27: break cap.release() cv2.destroyAllWindows() Build OpenCV One script to install OpenCV 4.3 is from AastaNV here . The script I used is from mdegans here Canny Edge Detection Test Code for OpenCV Demo with Canny from RICOH THETA V . This is the edge detection demo with the white lines on black background. video demo import sys import argparse import cv2 import numpy as np def parse_cli_args(): parser = argparse.ArgumentParser() parser.add_argument(\"--video_device\", dest=\"video_device\", help=\"Video device # of USB webcam (/dev/video?) [0]\", default=0, type=int) arguments = parser.parse_args() return arguments # On versions of L4T previous to L4T 28.1, flip-method=2 # Use the Jetson onboard camera def open_onboard_camera(): return cv2.VideoCapture(0) # Open an external usb camera /dev/videoX def open_camera_device(device_number): return cv2.VideoCapture(device_number) def read_cam(video_capture): if video_capture.isOpened(): windowName = \"main_canny\" cv2.namedWindow(windowName, cv2.WINDOW_NORMAL) cv2.resizeWindow(windowName,1280,720) cv2.moveWindow(windowName,0,0) cv2.setWindowTitle(windowName,\"RICOH THETA OpenCV Python Demo\") showWindow=3 # Show all stages showHelp = True font = cv2.FONT_HERSHEY_PLAIN helpText=\"'Esc' to Quit, '1' for Camera Feed, '2' for Canny Detection, '3' for All Stages. '4' to hide help\" edgeThreshold=40 showFullScreen = False while True: if cv2.getWindowProperty(windowName, 0) < 0: # Check to see if the user closed the window # This will fail if the user closed the window; Nasties get printed to the console break; ret_val, frame = video_capture.read(); hsv=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) blur=cv2.GaussianBlur(hsv,(7,7),1.5) edges=cv2.Canny(blur,0,edgeThreshold) if showWindow == 3: # Need to show the 4 stages # Composite the 2x2 window # Feed from the camera is RGB, the others gray # To composite, convert gray images to color. # All images must be of the same type to display in a window frameRs=cv2.resize(frame, (640,360)) hsvRs=cv2.resize(hsv,(640,360)) vidBuf = np.concatenate((frameRs, cv2.cvtColor(hsvRs,cv2.COLOR_GRAY2BGR)), axis=1) blurRs=cv2.resize(blur,(640,360)) edgesRs=cv2.resize(edges,(640,360)) vidBuf1 = np.concatenate( (cv2.cvtColor(blurRs,cv2.COLOR_GRAY2BGR),cv2.cvtColor(edgesRs,cv2.COLOR_GRAY2BGR)), axis=1) vidBuf = np.concatenate( (vidBuf, vidBuf1), axis=0) if showWindow==1: # Show Camera Frame displayBuf = frame elif showWindow == 2: # Show Canny Edge Detection displayBuf = edges elif showWindow == 3: # Show All Stages displayBuf = vidBuf if showHelp == True: cv2.putText(displayBuf, helpText, (11,20), font, 1.0, (32,32,32), 4, cv2.LINE_AA) cv2.putText(displayBuf, helpText, (10,20), font, 1.0, (240,240,240), 1, cv2.LINE_AA) cv2.imshow(windowName,displayBuf) key=cv2.waitKey(10) if key == 27: # Check for ESC key cv2.destroyAllWindows() break ; elif key==49: # 1 key, show frame cv2.setWindowTitle(windowName,\"Camera Feed\") showWindow=1 elif key==50: # 2 key, show Canny cv2.setWindowTitle(windowName,\"Canny Edge Detection\") showWindow=2 elif key==51: # 3 key, show Stages cv2.setWindowTitle(windowName,\"Camera, Gray scale, Gaussian Blur, Canny Edge Detection\") showWindow=3 elif key==52: # 4 key, toggle help showHelp = not showHelp elif key==44: # , lower canny edge threshold edgeThreshold=max(0,edgeThreshold-1) print ('Canny Edge Threshold Maximum: ',edgeThreshold) elif key==46: # , raise canny edge threshold edgeThreshold=edgeThreshold+1 print ('Canny Edge Threshold Maximum: ', edgeThreshold) elif key==74: # Toggle fullscreen; This is the F3 key on this particular keyboard # Toggle full screen mode if showFullScreen == False : cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN) else: cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL) showFullScreen = not showFullScreen else: print (\"camera open failed\") if __name__ == '__main__': arguments = parse_cli_args() print(\"Called with args:\") print(arguments) print(\"OpenCV version: {}\".format(cv2.__version__)) print(\"Device Number:\",arguments.video_device) if arguments.video_device==0: video_capture=open_onboard_camera() else: video_capture=open_camera_device(arguments.video_device) read_cam(video_capture) video_capture.release() cv2.destroyAllWindows() OpenPose Works on live stream with Jetpack 4.3, not 4.4.","title":"Demos"},{"location":"demos/#ongoing-tests-with-linux-streaming","text":"Using Nvidia Jetson Nano live streaming from a THETA V. Processing done with Python3, OpenCV 4.4. Scroll down for code.","title":"Ongoing Tests with Linux Streaming"},{"location":"demos/#detectnet","text":"Running live on Jetson Nano with RICOH THETA Z1. DetectNet applied to both single frame with SSD Mobilenet-v2 to assess accuracy and to live stream to assess framerate. Works good on both. Video demo with Jetson Nano . See Jetson Nano inference benchmarks . Code is available in the at https://github.com/dusty-nv/jetson-inference There is super small text in the green box that says, \"person\". The system accurately detected the only person in the image. It is 88.6 percent confident that I am a person. Nice. Despite the distorted view of my feet, the program does detect the human form. Even at night, in low-light conditions with me on the side of the shutter button, the program did detect me. However, there were many frames where I was not detected. To proceed, you will likely need a database of fisheye or equirectangular images to build your own model.","title":"DetectNet"},{"location":"demos/#sample-code","text":"import jetson.inference import jetson.utils net = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5) camera = jetson.utils.gstCamera(1280, 720, \"/dev/video0\") display = jetson.utils.glDisplay() while display.IsOpen(): img, width, height = camera.CaptureRGBA() detections = net.Detect(img, width, height) display.RenderOnce(img, width, height) display.SetTitle(\"RICOH THETA Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS()))","title":"Sample Code"},{"location":"demos/#opencv-python","text":"Works on live stream.","title":"OpenCV Python"},{"location":"demos/#procedure","text":"install libuvc-theta install libuv-theta-sample install v4l2loopback load kernel modules for v4l2loopback and verify that /dev/video0 or equivalent shows THETA stream run Python script with cv2 Recommend you recompile OpenCV 4.4 from source code. May take 2.5 hours if you compile on the Nano.","title":"Procedure"},{"location":"demos/#simple-python-cv2-test","text":"Frame resize test. import cv2 cap = cv2.VideoCapture(0) # Check if the webcam is opened correctly if not cap.isOpened(): raise IOError(\"Cannot open webcam\") while True: ret, frame = cap.read() frame = cv2.resize(frame, None, fx=0.25, fy=0.25, interpolation=cv2.INTER_AREA) cv2.imshow('Input', frame) c = cv2.waitKey(1) if c == 27: break cap.release() cv2.destroyAllWindows()","title":"Simple Python cv2 Test"},{"location":"demos/#build-opencv","text":"One script to install OpenCV 4.3 is from AastaNV here . The script I used is from mdegans here","title":"Build OpenCV"},{"location":"demos/#canny-edge-detection-test","text":"Code for OpenCV Demo with Canny from RICOH THETA V . This is the edge detection demo with the white lines on black background. video demo import sys import argparse import cv2 import numpy as np def parse_cli_args(): parser = argparse.ArgumentParser() parser.add_argument(\"--video_device\", dest=\"video_device\", help=\"Video device # of USB webcam (/dev/video?) [0]\", default=0, type=int) arguments = parser.parse_args() return arguments # On versions of L4T previous to L4T 28.1, flip-method=2 # Use the Jetson onboard camera def open_onboard_camera(): return cv2.VideoCapture(0) # Open an external usb camera /dev/videoX def open_camera_device(device_number): return cv2.VideoCapture(device_number) def read_cam(video_capture): if video_capture.isOpened(): windowName = \"main_canny\" cv2.namedWindow(windowName, cv2.WINDOW_NORMAL) cv2.resizeWindow(windowName,1280,720) cv2.moveWindow(windowName,0,0) cv2.setWindowTitle(windowName,\"RICOH THETA OpenCV Python Demo\") showWindow=3 # Show all stages showHelp = True font = cv2.FONT_HERSHEY_PLAIN helpText=\"'Esc' to Quit, '1' for Camera Feed, '2' for Canny Detection, '3' for All Stages. '4' to hide help\" edgeThreshold=40 showFullScreen = False while True: if cv2.getWindowProperty(windowName, 0) < 0: # Check to see if the user closed the window # This will fail if the user closed the window; Nasties get printed to the console break; ret_val, frame = video_capture.read(); hsv=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) blur=cv2.GaussianBlur(hsv,(7,7),1.5) edges=cv2.Canny(blur,0,edgeThreshold) if showWindow == 3: # Need to show the 4 stages # Composite the 2x2 window # Feed from the camera is RGB, the others gray # To composite, convert gray images to color. # All images must be of the same type to display in a window frameRs=cv2.resize(frame, (640,360)) hsvRs=cv2.resize(hsv,(640,360)) vidBuf = np.concatenate((frameRs, cv2.cvtColor(hsvRs,cv2.COLOR_GRAY2BGR)), axis=1) blurRs=cv2.resize(blur,(640,360)) edgesRs=cv2.resize(edges,(640,360)) vidBuf1 = np.concatenate( (cv2.cvtColor(blurRs,cv2.COLOR_GRAY2BGR),cv2.cvtColor(edgesRs,cv2.COLOR_GRAY2BGR)), axis=1) vidBuf = np.concatenate( (vidBuf, vidBuf1), axis=0) if showWindow==1: # Show Camera Frame displayBuf = frame elif showWindow == 2: # Show Canny Edge Detection displayBuf = edges elif showWindow == 3: # Show All Stages displayBuf = vidBuf if showHelp == True: cv2.putText(displayBuf, helpText, (11,20), font, 1.0, (32,32,32), 4, cv2.LINE_AA) cv2.putText(displayBuf, helpText, (10,20), font, 1.0, (240,240,240), 1, cv2.LINE_AA) cv2.imshow(windowName,displayBuf) key=cv2.waitKey(10) if key == 27: # Check for ESC key cv2.destroyAllWindows() break ; elif key==49: # 1 key, show frame cv2.setWindowTitle(windowName,\"Camera Feed\") showWindow=1 elif key==50: # 2 key, show Canny cv2.setWindowTitle(windowName,\"Canny Edge Detection\") showWindow=2 elif key==51: # 3 key, show Stages cv2.setWindowTitle(windowName,\"Camera, Gray scale, Gaussian Blur, Canny Edge Detection\") showWindow=3 elif key==52: # 4 key, toggle help showHelp = not showHelp elif key==44: # , lower canny edge threshold edgeThreshold=max(0,edgeThreshold-1) print ('Canny Edge Threshold Maximum: ',edgeThreshold) elif key==46: # , raise canny edge threshold edgeThreshold=edgeThreshold+1 print ('Canny Edge Threshold Maximum: ', edgeThreshold) elif key==74: # Toggle fullscreen; This is the F3 key on this particular keyboard # Toggle full screen mode if showFullScreen == False : cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN) else: cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL) showFullScreen = not showFullScreen else: print (\"camera open failed\") if __name__ == '__main__': arguments = parse_cli_args() print(\"Called with args:\") print(arguments) print(\"OpenCV version: {}\".format(cv2.__version__)) print(\"Device Number:\",arguments.video_device) if arguments.video_device==0: video_capture=open_onboard_camera() else: video_capture=open_camera_device(arguments.video_device) read_cam(video_capture) video_capture.release() cv2.destroyAllWindows()","title":"Canny Edge Detection Test"},{"location":"demos/#openpose","text":"Works on live stream with Jetpack 4.3, not 4.4.","title":"OpenPose"},{"location":"equipment/","text":"Hardware Requirements for Linux and the RICOH THETA Jetson Nano - Reference Platform Our reference platform is the NVIDIA Jetson Nano, ref . We are using B01, but A02 should also work. running JetPack 4.4, which is Ubuntu 18.04. The Nano is an ARM A57 with a 128-core Maxwell GPU, 4GB 64-bit LPDDR4. The nano is powered by a 5V 4A barrel connector, not the microUSB which is 5V 2A. Our Nano has an external fan on the PWM header and a 64GB microSD card. Parts NVIDIA Jetson Nano Developer Kit B01 SMAKIN DC 5V/4A power supply with barrel connector Waveshare 5V PWM fan - cheaper option - we used this one as we are frugal. It worked. Noctua 5V PWM fan - better option, around $15 - most people use this one. For Z1 streaming 10' USB-C live streaming cable - it works for me, but it is over the recommended length. I only have the long cable for convenience. You should use as short a cable as possible. NVIDIA Jetson Xavier The Xavier is better for testing. However, it is more expensive. If your budget permits, it is better to get the Xavier. You may have problems with 4K AI processing with the Nano. On Jetson Xavier, auto plugin selection of the gstreamer seems to be not working well, replacing \"decodebin ! autovideosink sync=false\" to \"nvv4l2decoder ! nv3dsink sync=false\" will solve the problem. Edit this line in the sample code and recompile. x86 Linux We've also tested the libuvc-theta (streaming) and libuvc-theta-sample (streaming sample application) on x86 64bit Linux using Ubuntu 20.04 LTS, Focal Fossa. Watch this build video walkthrough . A video showing latency on x86 is here . We've tested v4l2loopback with gst_loopback on a low-end Pentium x86 computer. It works fine. Thanks to commuity member Yu You for this fix to gst_view.c. Note the addition of qos=false to the pipeline. This is currently on line 190 . if (strcmp(cmd_name, \"gst_loopback\") == 0) pipe_proc = \"decodebin ! autovideoconvert ! \" \"video/x-raw,format=I420 ! identity drop-allocation=true !\" \"v4l2sink device=/dev/video0 qos=false sync=false\"; Screenshot of loopback running on /dev/video0 , tested with vlc. Addtional x86 Information If you're having problems after making the modification described above, you can try to temporarily disable your dedicated graphics card on x86. In our tests on Ubuntu 20, Focal Fossa, the gstreamer vaapi plugin can't use the hardware decoder on the NVIDIA GPU with the proprietary NVIDIA driver. The causes gstreamer to use a software decoder instead. This will likely cause many frame drops on your system. You can verify this by setting the GST_DEBUG environment variable to 2 or 3 and then running gst_loopback. You will likely see many frame drop messages. There are two possible workarounds: Use the nvdec plugin Although the nvdec plugin is a part of the gstreamer-plugins-bad, it is not included in binary distribution due to license problem. Thus, you have to build the plugin by yourself. You also need to modify the pipeline of the gst_loopback accordingly. Use hardware decoder on the iGPU You may need additional setup to run X server on the iGPU, You can try the X.Org driver. This is a video test clip of a THETA Z1 running with the X.Org driver on Intel i7-6800K CPU and NVIDIA GeForce GTX 950 GPU. You can check the graphics driver with one of these commands. $ glxinfo -B or $ sudo lshw -c video Raspberry Pi The Raspberry Pi will work great with the USB API. However, you will not have a good experience streaming 4K, even with the Raspberry Pi 4. The Raspberry Pi's H.264 hardware decoder does not support 4K resolution even on the Raspberry Pi4. In addition, older Pis' (Pi to Pi3) memory bandwidth(32bit DDR2) is too poor to handle even FHD stream from THETA V/Z1. Heat and Cooling of Linux Computer You need to cool the Nano. Without a fan, you may get thermal throttling when live streaming with AI processing. The fan is 5V pwm. I've also used a 12V fan before I ordered the 5V fan from Amazon. Hardware Acceleration You will need to use hardware acceleration to get reasonable performance. To verify that you are using GPU acceleration, you can use tegrastats on Jetson and nvidia-smi on x86. Jetson You can either use tegrastats or jetson-stats to see information on your CPU and GPU to identify performance bottlenecks. tegrastats On NVIDIA Jetson, tegrastats is useful for seeing information on the GPU. In the example below, I've inserted line breaks to make the output easier to read. The output is shown before streaming starts. craig@jetson:~$ tegrastats RAM 1122/3964MB (lfb 28x4MB) SWAP 211/1982MB (cached 20MB) CPU [5%@102,9%@102,0%@102,0%@102] EMC_FREQ 0% GR3D_FREQ 0% PLL@25.5C CPU@27C PMIC@100C GPU@27.5C AO@35C thermal@27.5C POM_5V_IN 1805/1805 POM_5V_GPU 0/0 POM_5V_CPU 123/123 Let's start the stream and review it again. RAM 1288/3964MB (lfb 28x4MB) SWAP 210/1982MB (cached 20MB) CPU [100%@1479,89%@1479,85%@1479,86%@1479] EMC_FREQ 0% GR3D_FREQ 35% PLL@32C CPU@35C PMIC@100C GPU@30.5C AO@40.5C thermal@32.25C POM_5V_IN 5607/5561 POM_5V_GPU 118/98 POM_5V_CPU 2843/2791 The GR3D_FREQ and POM_5V_GPU provide information on the GPU. GR3D is the Jetson GPU engine. More information on tegrastatus is here . jetson-stats Another nice package is jetson-stats . You can verify if your base libraries such as OpenCV have features such as CUDA enabled. Prior to streaming, your system should show very little load. Once streaming starts, the load on your GPU should increase. The example below shows OpenCV and a Python script for canny edge detection. The example below is using OpenCV to convert the color space. x86 gstreamer plug-in You can check to see if the nvdec plug-in is installed with: $ gst-inspect-1.0 nvdec If you see this, the plug-in is not installed. No such element or plugin 'nvdec' If nvdec and nvenc are installed, you should see this: $ gst-inspect-1.0 | grep nvenc nvenc: nvh264enc: NVENC H.264 Video Encoder $ gst-inspect-1.0 | grep nvdec nvdec: nvdec: NVDEC video decoder There are several online tutorials for installing nvdec and nvenc. LifeStyle transfer: How to install Nvidia Gstreamer plugins (nvenc, nvdec) on Ubuntu? by Taras Lishchenko README from gst-plugins-bad/sys/nvenc gist from corenel plugin build example $ NVENCODE_CFLAGS=\"-I/home/craig/Development/gstreamer/gst-plugins-bad/sys/nvenc\" ./autogen.sh --disable-gtk-doc --with-cuda-prefix=\"/usr/local/cuda\" Monitoring Tools Prior to starting the stream. $ nvidia-smi Mon Sep 14 06:14:55 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.100 Driver Version: 440.100 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 950 Off | 00000000:02:00.0 On | N/A | | 1% 52C P5 14W / 99W | 355MiB / 1999MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1051 G /usr/lib/xorg/Xorg 52MiB | | 0 1619 G /usr/lib/xorg/Xorg 113MiB | | 0 1820 G /usr/bin/gnome-shell 102MiB | | 0 2822 G ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files 73MiB | +-----------------------------------------------------------------------------+ With gst-viewer running. $ nvidia-smi Tue Sep 29 16:29:33 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.66 Driver Version: 450.66 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 950 Off | 00000000:02:00.0 On | N/A | | 9% 56C P0 28W / 99W | 543MiB / 1999MiB | 9% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1100 G /usr/lib/xorg/Xorg 51MiB | | 0 N/A N/A 1588 G /usr/lib/xorg/Xorg 188MiB | | 0 N/A N/A 1719 G /usr/bin/gnome-shell 144MiB | | 0 N/A N/A 2132 G ...AAAAAAAAA= --shared-files 71MiB | | 0 N/A N/A 6606 G ...AAAAAAAAA= --shared-files 73MiB | +-----------------------------------------------------------------------------+","title":"Equipment"},{"location":"equipment/#hardware-requirements-for-linux-and-the-ricoh-theta","text":"","title":"Hardware Requirements for Linux and the RICOH THETA"},{"location":"equipment/#jetson-nano-reference-platform","text":"Our reference platform is the NVIDIA Jetson Nano, ref . We are using B01, but A02 should also work. running JetPack 4.4, which is Ubuntu 18.04. The Nano is an ARM A57 with a 128-core Maxwell GPU, 4GB 64-bit LPDDR4. The nano is powered by a 5V 4A barrel connector, not the microUSB which is 5V 2A. Our Nano has an external fan on the PWM header and a 64GB microSD card.","title":"Jetson Nano - Reference Platform"},{"location":"equipment/#parts","text":"NVIDIA Jetson Nano Developer Kit B01 SMAKIN DC 5V/4A power supply with barrel connector Waveshare 5V PWM fan - cheaper option - we used this one as we are frugal. It worked. Noctua 5V PWM fan - better option, around $15 - most people use this one. For Z1 streaming 10' USB-C live streaming cable - it works for me, but it is over the recommended length. I only have the long cable for convenience. You should use as short a cable as possible.","title":"Parts"},{"location":"equipment/#nvidia-jetson-xavier","text":"The Xavier is better for testing. However, it is more expensive. If your budget permits, it is better to get the Xavier. You may have problems with 4K AI processing with the Nano. On Jetson Xavier, auto plugin selection of the gstreamer seems to be not working well, replacing \"decodebin ! autovideosink sync=false\" to \"nvv4l2decoder ! nv3dsink sync=false\" will solve the problem. Edit this line in the sample code and recompile.","title":"NVIDIA Jetson Xavier"},{"location":"equipment/#x86-linux","text":"We've also tested the libuvc-theta (streaming) and libuvc-theta-sample (streaming sample application) on x86 64bit Linux using Ubuntu 20.04 LTS, Focal Fossa. Watch this build video walkthrough . A video showing latency on x86 is here . We've tested v4l2loopback with gst_loopback on a low-end Pentium x86 computer. It works fine. Thanks to commuity member Yu You for this fix to gst_view.c. Note the addition of qos=false to the pipeline. This is currently on line 190 . if (strcmp(cmd_name, \"gst_loopback\") == 0) pipe_proc = \"decodebin ! autovideoconvert ! \" \"video/x-raw,format=I420 ! identity drop-allocation=true !\" \"v4l2sink device=/dev/video0 qos=false sync=false\"; Screenshot of loopback running on /dev/video0 , tested with vlc.","title":"x86 Linux"},{"location":"equipment/#addtional-x86-information","text":"If you're having problems after making the modification described above, you can try to temporarily disable your dedicated graphics card on x86. In our tests on Ubuntu 20, Focal Fossa, the gstreamer vaapi plugin can't use the hardware decoder on the NVIDIA GPU with the proprietary NVIDIA driver. The causes gstreamer to use a software decoder instead. This will likely cause many frame drops on your system. You can verify this by setting the GST_DEBUG environment variable to 2 or 3 and then running gst_loopback. You will likely see many frame drop messages. There are two possible workarounds: Use the nvdec plugin Although the nvdec plugin is a part of the gstreamer-plugins-bad, it is not included in binary distribution due to license problem. Thus, you have to build the plugin by yourself. You also need to modify the pipeline of the gst_loopback accordingly. Use hardware decoder on the iGPU You may need additional setup to run X server on the iGPU, You can try the X.Org driver. This is a video test clip of a THETA Z1 running with the X.Org driver on Intel i7-6800K CPU and NVIDIA GeForce GTX 950 GPU. You can check the graphics driver with one of these commands. $ glxinfo -B or $ sudo lshw -c video","title":"Addtional x86 Information"},{"location":"equipment/#raspberry-pi","text":"The Raspberry Pi will work great with the USB API. However, you will not have a good experience streaming 4K, even with the Raspberry Pi 4. The Raspberry Pi's H.264 hardware decoder does not support 4K resolution even on the Raspberry Pi4. In addition, older Pis' (Pi to Pi3) memory bandwidth(32bit DDR2) is too poor to handle even FHD stream from THETA V/Z1.","title":"Raspberry Pi"},{"location":"equipment/#heat-and-cooling-of-linux-computer","text":"You need to cool the Nano. Without a fan, you may get thermal throttling when live streaming with AI processing. The fan is 5V pwm. I've also used a 12V fan before I ordered the 5V fan from Amazon.","title":"Heat and Cooling of Linux Computer"},{"location":"equipment/#hardware-acceleration","text":"You will need to use hardware acceleration to get reasonable performance. To verify that you are using GPU acceleration, you can use tegrastats on Jetson and nvidia-smi on x86.","title":"Hardware Acceleration"},{"location":"equipment/#jetson","text":"You can either use tegrastats or jetson-stats to see information on your CPU and GPU to identify performance bottlenecks.","title":"Jetson"},{"location":"equipment/#tegrastats","text":"On NVIDIA Jetson, tegrastats is useful for seeing information on the GPU. In the example below, I've inserted line breaks to make the output easier to read. The output is shown before streaming starts. craig@jetson:~$ tegrastats RAM 1122/3964MB (lfb 28x4MB) SWAP 211/1982MB (cached 20MB) CPU [5%@102,9%@102,0%@102,0%@102] EMC_FREQ 0% GR3D_FREQ 0% PLL@25.5C CPU@27C PMIC@100C GPU@27.5C AO@35C thermal@27.5C POM_5V_IN 1805/1805 POM_5V_GPU 0/0 POM_5V_CPU 123/123 Let's start the stream and review it again. RAM 1288/3964MB (lfb 28x4MB) SWAP 210/1982MB (cached 20MB) CPU [100%@1479,89%@1479,85%@1479,86%@1479] EMC_FREQ 0% GR3D_FREQ 35% PLL@32C CPU@35C PMIC@100C GPU@30.5C AO@40.5C thermal@32.25C POM_5V_IN 5607/5561 POM_5V_GPU 118/98 POM_5V_CPU 2843/2791 The GR3D_FREQ and POM_5V_GPU provide information on the GPU. GR3D is the Jetson GPU engine. More information on tegrastatus is here .","title":"tegrastats"},{"location":"equipment/#jetson-stats","text":"Another nice package is jetson-stats . You can verify if your base libraries such as OpenCV have features such as CUDA enabled. Prior to streaming, your system should show very little load. Once streaming starts, the load on your GPU should increase. The example below shows OpenCV and a Python script for canny edge detection. The example below is using OpenCV to convert the color space.","title":"jetson-stats"},{"location":"equipment/#x86","text":"","title":"x86"},{"location":"equipment/#gstreamer-plug-in","text":"You can check to see if the nvdec plug-in is installed with: $ gst-inspect-1.0 nvdec If you see this, the plug-in is not installed. No such element or plugin 'nvdec' If nvdec and nvenc are installed, you should see this: $ gst-inspect-1.0 | grep nvenc nvenc: nvh264enc: NVENC H.264 Video Encoder $ gst-inspect-1.0 | grep nvdec nvdec: nvdec: NVDEC video decoder There are several online tutorials for installing nvdec and nvenc. LifeStyle transfer: How to install Nvidia Gstreamer plugins (nvenc, nvdec) on Ubuntu? by Taras Lishchenko README from gst-plugins-bad/sys/nvenc gist from corenel plugin build example $ NVENCODE_CFLAGS=\"-I/home/craig/Development/gstreamer/gst-plugins-bad/sys/nvenc\" ./autogen.sh --disable-gtk-doc --with-cuda-prefix=\"/usr/local/cuda\"","title":"gstreamer plug-in"},{"location":"equipment/#monitoring-tools","text":"Prior to starting the stream. $ nvidia-smi Mon Sep 14 06:14:55 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 440.100 Driver Version: 440.100 CUDA Version: 10.2 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | |===============================+======================+======================| | 0 GeForce GTX 950 Off | 00000000:02:00.0 On | N/A | | 1% 52C P5 14W / 99W | 355MiB / 1999MiB | 0% Default | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: GPU Memory | | GPU PID Type Process name Usage | |=============================================================================| | 0 1051 G /usr/lib/xorg/Xorg 52MiB | | 0 1619 G /usr/lib/xorg/Xorg 113MiB | | 0 1820 G /usr/bin/gnome-shell 102MiB | | 0 2822 G ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files 73MiB | +-----------------------------------------------------------------------------+ With gst-viewer running. $ nvidia-smi Tue Sep 29 16:29:33 2020 +-----------------------------------------------------------------------------+ | NVIDIA-SMI 450.66 Driver Version: 450.66 CUDA Version: 11.0 | |-------------------------------+----------------------+----------------------+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 GeForce GTX 950 Off | 00000000:02:00.0 On | N/A | | 9% 56C P0 28W / 99W | 543MiB / 1999MiB | 9% Default | | | | N/A | +-------------------------------+----------------------+----------------------+ +-----------------------------------------------------------------------------+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| | 0 N/A N/A 1100 G /usr/lib/xorg/Xorg 51MiB | | 0 N/A N/A 1588 G /usr/lib/xorg/Xorg 188MiB | | 0 N/A N/A 1719 G /usr/bin/gnome-shell 144MiB | | 0 N/A N/A 2132 G ...AAAAAAAAA= --shared-files 71MiB | | 0 N/A N/A 6606 G ...AAAAAAAAA= --shared-files 73MiB | +-----------------------------------------------------------------------------+","title":"Monitoring Tools"},{"location":"examples/","text":"Usage Examples stream to YouTube with ffmpeg from Paul Gullett . post ffmpeg -f lavfi -i anullsrc \\ -f v4l2 -s 3480x1920 -r 10 -i /dev/video0 \\ -vcodec libx264 -pix_fmt yuv420p -preset ultrafast \\ -strict experimental -r 25 -g 20 -b:v 2500k \\ -codec:a libmp3lame -ar 44100 -b:a 11025 -bufsize 512k \\ -f flv rtmp://a.rtmp.youtube.com/live2/secret-key As my knowledge of ffmpeg is weak, I simplified Paul's video pipeline. ffmpeg -f lavfi -i anullsrc -f v4l2 -s 1920x960 -r 10 -i /dev/video2 \\ -vcodec libx264 -pix_fmt yuv420p \\ -b:v 2500k \\ -codec:a libmp3lame -ar 44100 -b:a 11025 -bufsize 512k \\ -f flv rtmp://a.rtmp.youtube.com/live2/$SECRET_KEY stream to another computer with gstreamer by zdydek . post in gst_viewer.c pipe_proc = \" rtph264pay name=pay0 pt=96 ! udpsink host=127.0.0.1 port=5000 sync=false \"; with gst-rtsp-server ./test-launch \"( udpsrc port=5000 ! application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)H264 ! rtph264depay ! h264parse ! rtph264pay name=pay0 pt=96 )\" Receive on ROS. GSCAM_CONFIG=\"rtspsrc location=rtspt://10.0.16.1:8554/test latency=400 drop-on-latency=true ! application/x-rtp, encoding-name=H264 ! rtph264depay ! decodebin ! queue ! videoconvert\" roslaunch gscam_nodelet.launch Simplified computer to computer streaming with rtsp and gstreamer This was tested going from an x86 machine to a Jetson Nano. The THETA Z1 is connected to the x86 Linux machine. It is not working with the Jetson as the sender. On x86 computer sending THETA video. Modify the pipeline in gst_viewer.c This example has the IP address hardcoded in. Switch to a variable in your code. pipe_proc = \" decodebin ! jpegenc ! rtpjpegpay ! udpsink host=192.168.2.100 port=5000 qos=false sync=false\"; If you are looking for the IP address of the receiver, you can use arp-scan on the command line. Example: sudo arp-scan --interface=eth0 --localnet On the receiving device, if the receiver is a NVIDIA Jetson Nano. $ cat receive_udp.sh gst-launch-1.0 udpsrc port=5000 ! application/x-rtp,encoding-name=JPEG,payload=26 ! rtpjpegdepay ! jpegdec ! videoscale ! video/x-raw,width=640,height=320 ! nveglglessink If you're on x86, change nveglglessink to autovideosink. You may want to make the width and height bigger as well. Save to File by Les Wu aka snafu666 . post Using the v4l2loopback capability and thetaV loopback example, here are 2 example gstreamer pipelines to grab the video: As a lossless huffman encoded raw file: gst-launch-1.0 v4l2src device=/dev/video99 ! video/x-raw,framerate=30/1 \\ ! videoconvert \\ ! videoscale \\ ! avenc_huffyuv \\ ! avimux \\ ! filesink location=raw.hfyu And with default h.264 encoding on a Jetson: gst-launch-1.0 v4l2src device=/dev/video99 ! video/x-raw,framerate=30/1 \\ ! nvvidconv \\ ! omxh264enc \\ ! h264parse ! matroskamux \\ ! filesink location=vid99.mkv Pro tip, when you install v4l2loopback, use the video_nr option to create the video device somewhere high so it does not get displaced by PnP of other cameras. The Huffyuv format is a large file format. VLC can play it . Here's a shot of me playing a file that I generated with Les's pipeline.","title":"Examples"},{"location":"examples/#usage-examples","text":"","title":"Usage Examples"},{"location":"examples/#stream-to-youtube-with-ffmpeg","text":"from Paul Gullett . post ffmpeg -f lavfi -i anullsrc \\ -f v4l2 -s 3480x1920 -r 10 -i /dev/video0 \\ -vcodec libx264 -pix_fmt yuv420p -preset ultrafast \\ -strict experimental -r 25 -g 20 -b:v 2500k \\ -codec:a libmp3lame -ar 44100 -b:a 11025 -bufsize 512k \\ -f flv rtmp://a.rtmp.youtube.com/live2/secret-key As my knowledge of ffmpeg is weak, I simplified Paul's video pipeline. ffmpeg -f lavfi -i anullsrc -f v4l2 -s 1920x960 -r 10 -i /dev/video2 \\ -vcodec libx264 -pix_fmt yuv420p \\ -b:v 2500k \\ -codec:a libmp3lame -ar 44100 -b:a 11025 -bufsize 512k \\ -f flv rtmp://a.rtmp.youtube.com/live2/$SECRET_KEY","title":"stream to YouTube with ffmpeg"},{"location":"examples/#stream-to-another-computer-with-gstreamer","text":"by zdydek . post in gst_viewer.c pipe_proc = \" rtph264pay name=pay0 pt=96 ! udpsink host=127.0.0.1 port=5000 sync=false \"; with gst-rtsp-server ./test-launch \"( udpsrc port=5000 ! application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)H264 ! rtph264depay ! h264parse ! rtph264pay name=pay0 pt=96 )\" Receive on ROS. GSCAM_CONFIG=\"rtspsrc location=rtspt://10.0.16.1:8554/test latency=400 drop-on-latency=true ! application/x-rtp, encoding-name=H264 ! rtph264depay ! decodebin ! queue ! videoconvert\" roslaunch gscam_nodelet.launch","title":"stream to another computer with gstreamer"},{"location":"examples/#simplified-computer-to-computer-streaming-with-rtsp-and-gstreamer","text":"This was tested going from an x86 machine to a Jetson Nano. The THETA Z1 is connected to the x86 Linux machine. It is not working with the Jetson as the sender. On x86 computer sending THETA video. Modify the pipeline in gst_viewer.c This example has the IP address hardcoded in. Switch to a variable in your code. pipe_proc = \" decodebin ! jpegenc ! rtpjpegpay ! udpsink host=192.168.2.100 port=5000 qos=false sync=false\"; If you are looking for the IP address of the receiver, you can use arp-scan on the command line. Example: sudo arp-scan --interface=eth0 --localnet On the receiving device, if the receiver is a NVIDIA Jetson Nano. $ cat receive_udp.sh gst-launch-1.0 udpsrc port=5000 ! application/x-rtp,encoding-name=JPEG,payload=26 ! rtpjpegdepay ! jpegdec ! videoscale ! video/x-raw,width=640,height=320 ! nveglglessink If you're on x86, change nveglglessink to autovideosink. You may want to make the width and height bigger as well.","title":"Simplified computer to computer streaming with rtsp and gstreamer"},{"location":"examples/#save-to-file","text":"by Les Wu aka snafu666 . post Using the v4l2loopback capability and thetaV loopback example, here are 2 example gstreamer pipelines to grab the video: As a lossless huffman encoded raw file: gst-launch-1.0 v4l2src device=/dev/video99 ! video/x-raw,framerate=30/1 \\ ! videoconvert \\ ! videoscale \\ ! avenc_huffyuv \\ ! avimux \\ ! filesink location=raw.hfyu And with default h.264 encoding on a Jetson: gst-launch-1.0 v4l2src device=/dev/video99 ! video/x-raw,framerate=30/1 \\ ! nvvidconv \\ ! omxh264enc \\ ! h264parse ! matroskamux \\ ! filesink location=vid99.mkv Pro tip, when you install v4l2loopback, use the video_nr option to create the video device somewhere high so it does not get displaced by PnP of other cameras. The Huffyuv format is a large file format. VLC can play it . Here's a shot of me playing a file that I generated with Les's pipeline.","title":"Save to File"},{"location":"help/","text":"Getting Help Updated Docs and Events Community discussion - Linux Streaming Community discussion - USB API If you want to talk to someone, send email to jcasman@oppkey.com FAQ Can I stream indefinitely? The THETA Z1 can power off the USB-C port and stream at the same time. Using USB 3.0 or better, the charge increases in our tests. The camera does get hot. Upgrade to the latest firmware. If possible, attach a small fan to your tripod and point it at the body of the THETA. The V drains slowly. It will last about 8 hours. You may be able to bypass the battery, but this is not tested. /dev/video0 freezes on x86 Change line 190 of gst_viewer.c. if (strcmp(cmd_name, \"gst_loopback\") == 0) pipe_proc = \"decodebin ! autovideoconvert ! \" \"video/x-raw,format=I420 ! identity drop-allocation=true !\" \"v4l2sink device=/dev/video0 qos=false sync=false\"; The THETA is not appearing on /dev/video0 Install v4l2loopback . How do I reduce the default 4K stream to 2K to improve AI processing? If your AI processing is going to slowly, try to reduce resolution from 4K to 2K. You likely need to do this on Jetson Nano as 4K often hangs due to limited resources on Nano. In gst_viewer.c, change line 248 from THETAUVC_MODE_UHD_2997 to THETAUVC_MODE_FHD_2997. Refer to thetauvc.c#L55 for definition. I can't use it on Xavier Change to: \"nvv4l2decoder ! nv3dsink sync=false\"","title":"Help"},{"location":"help/#getting-help","text":"Updated Docs and Events Community discussion - Linux Streaming Community discussion - USB API If you want to talk to someone, send email to jcasman@oppkey.com","title":"Getting Help"},{"location":"help/#faq","text":"","title":"FAQ"},{"location":"help/#can-i-stream-indefinitely","text":"The THETA Z1 can power off the USB-C port and stream at the same time. Using USB 3.0 or better, the charge increases in our tests. The camera does get hot. Upgrade to the latest firmware. If possible, attach a small fan to your tripod and point it at the body of the THETA. The V drains slowly. It will last about 8 hours. You may be able to bypass the battery, but this is not tested.","title":"Can I stream indefinitely?"},{"location":"help/#devvideo0-freezes-on-x86","text":"Change line 190 of gst_viewer.c. if (strcmp(cmd_name, \"gst_loopback\") == 0) pipe_proc = \"decodebin ! autovideoconvert ! \" \"video/x-raw,format=I420 ! identity drop-allocation=true !\" \"v4l2sink device=/dev/video0 qos=false sync=false\";","title":"/dev/video0 freezes on x86"},{"location":"help/#the-theta-is-not-appearing-on-devvideo0","text":"Install v4l2loopback .","title":"The THETA is not appearing on /dev/video0"},{"location":"help/#how-do-i-reduce-the-default-4k-stream-to-2k-to-improve-ai-processing","text":"If your AI processing is going to slowly, try to reduce resolution from 4K to 2K. You likely need to do this on Jetson Nano as 4K often hangs due to limited resources on Nano. In gst_viewer.c, change line 248 from THETAUVC_MODE_UHD_2997 to THETAUVC_MODE_FHD_2997. Refer to thetauvc.c#L55 for definition.","title":"How do I reduce the default 4K stream to 2K to improve AI processing?"},{"location":"help/#i-cant-use-it-on-xavier","text":"Change to: \"nvv4l2decoder ! nv3dsink sync=false\"","title":"I can't use it on Xavier"},{"location":"meetup/","text":"Notes Archive September 1, 2020 - First Linux Meetup","title":"Meetup Archive"},{"location":"meetup/#notes-archive","text":"September 1, 2020 - First Linux Meetup","title":"Notes Archive"},{"location":"optimization/","text":"gstreamer optimization on x86 We reduced latency from the default 550ms to 220ms. The latency is measured from the camera to the screen and may be higher with machine vision that may need to move the frame from the GPU to system memory. We achieved this improvement by using two gstreamer plug-ins: nvdec hardware decoding plug-in for NVIDIA GPUs glimagesink OpenGL plug-in Overview nvdec uses dedicated NVIDIA GPU hardware decoding features and fast copy to move frames between system and GPU memory. The THETA H.264 stream is decoded on the GPU and outputs buffers in raw format on the GPU. Instead of downloading the frame from the GPU to system memory, we use glimagesink to display the OpenGL textures to the computer monitor without having to transfer the frame to system memory. To use v4l2loopback, we show how to use gldownload to transfer the frame into system memory. Although this technique increases latency, it appears to be faster than streaming without hardware decoding on our test system. Audience If you already have streaming working with the THETA on your x86 Linux machine and want to experiment with reducing latency, this article will guide you through installing and configuring hardware decoding on the GPU. If you are using NVIDIA Jetson boards, you do not need this article. On Nano hardware, you are likely already using hardware acceleration. If you are using NVIDIA Jetson Xavier hardware, the hardware decoder is nvv4l2decoder and is included in JetPack, the Jetson OS you download from NVIDIA. On Jetson, the sink is nv3dsink. If you are using x86 and do not have streaming working at all, you should first try this pipeline. pipe_proc = \" decodebin ! autovideosink sync=false qos=false\"; Note that both sync and qos are false. If you have enabled the pipeline above and your framerate is still extremely slow, you can try installing all the gstreamer plug-ins using apt from binaries. If you're still stuck with unusable framerates, this article on using the dedicated video decoder on the GPU may help. However, due to the number of steps involved in installing the gstreamer plug-in, the primary target audience is someone that already has live streaming working and is interested in trying to reduce latency. Tests nvdec and glimagesink pipe_proc = \"nvdec ! glimagesink qos=false sync=false\"; foreground: 59.182 THETA video: 58.932 Latency: 250ms Default decodebin and autovideosink pipe_proc = \" decodebin ! autovideosink sync=false\"; foreground: 691 THETA video: 141 Latency: 550ms Result: Latency Reduced by 50% Equipment Intel i7-6800K NVIDIA GTX 950 GPU RICOH THETA Z1 with firmware 1.60.1 Software Ubuntu 20.04 NVIDIA Linux graphics driver 455.23 CUDA Version: 11.1 gstreamer 1.16.2 NVIDIA Video Codec 11.0.10 Overview of Steps verify that you don't have nvdec installed. If you have it installed, you can skip most of this document and go to the section on the gstreamer pipline configuration of gst_viewer.c Download and install gst-plugins-bad Install NVIDIA CODEC SDK Modify gst_viewer pipeline to use the nvdec plug-in for hardware decoding and glimagesink for display to the screen Tips Verify if you have nvdec installed. $ gst-inspect-1.0 nvdec No such element or plugin 'nvdec' If nvdec is installed, you will see this: $ gst-inspect-1.0 | grep nvdec nvdec: nvdec: NVDEC video decoder Download the gst-plugins-bad After you clone the repo, you need to checkout the branch that is the same as the version of gstreamer you have installed. Clone repo. git clone git://anongit.freedesktop.org/gstreamer/gst-plugins-bad cd gst-plugins-bad/ # verify gstreamer version $ gst-inspect-1.0 --version gst-inspect-1.0 version 1.16.2 GStreamer 1.16.2 $ git checkout 1.16.2 HEAD is now at a6f26408f Release 1.16.2 # verify that you're on the correct branch $ git branch * (HEAD detached at 1.16.2) master Install NVIDIA CODEC SDK Download NVIDIA CODEC SDK . Unzip to /path/to/video/codec/sdk cd /path/to/video/codec/sdk cp /usr/local/cuda/include/cuda.h /path/to/gst-plugins-bad/sys/nvenc cp Interface/nvEncodeAPI.h /path/to/gst-plugins-bad/sys/nvenc cp Interface/cuviddec.h /path/to/gst-plugins-bad/sys/nvdec cp Interface/nvcuvid.h /path/to/gst-plugins-bad/sys/nvdec Build and Install Plug-in Configure and build. $ NVENCODE_CFLAGS=\"-I/home/craig/Development/gstreamer/gst-plugins-bad/sys/nvenc\" ./autogen.sh --disable-gtk-doc --with-cuda-prefix=\"/usr/local/cuda\" cd sys/nvenc make sudo cp .libs/libgstnvenc.so /usr/lib/x86_64-linux-gnu/gstreamer-1.0/ cd ../nvdec make sudo cp .libs/libgstnvdec.so /usr/lib/x86_64-linux-gnu/gstreamer-1.0/ Confirm that autogen configured project to build nvdec. Verify Install Configure gst_viewer.c Pipeline is roughly around line 192. GitHub permalink is here . else // original pipeline // pipe_proc = \" decodebin ! autovideosink sync=false\"; // use gstreamer plug-in for hardware acceleration pipe_proc = \"nvdec ! glimagesink qos=false sync=false\"; Results Original Pipeline The left video is a Logitech C920 USB webcam. The right video is the THETA. Original pipeline. There is a lag on the THETA video when I move my hand. nvdec pipeline The THETA video stream is now much closer to the latency of the NVIDIA C920. Configuration with v4l2loopack on /dev/video* To use nvdec with v4l2loopback, I needed to download the OpenGL textures from the GPU to video frames. This introduced some latency. However, testing with vlc still showed improvement over the standard pipeline. if (strcmp(cmd_name, \"gst_loopback\") == 0) // original pipeline // pipe_proc = \"decodebin ! autovideoconvert ! \" // \"video/x-raw,format=I420 ! identity drop-allocation=true !\" // \"v4l2sink device=/dev/video2 qos=false sync=false\"; // //modified pipeline below pipe_proc = \"nvdec ! gldownload ! videoconvert n-thread=0 ! \" \"video/x-raw,format=I420 ! identity drop-allocation=true !\" \"v4l2sink device=/dev/video2 qos=false sync=false\"; More information on using gldownload is available here . v4l2loopback and vlc example vlc is accessing the camera on /dev/video2 . I'm doing the test at night in a darkened room. NVIDIA Jetson The Jetson is likely already using hardware accleration. You can get more examples on using gstreamer with nvv4l2decoder , nvvidconv , and nv3dsink in the NVIDIA Accelerated GStreamer guide. Hopefuly, this link works . If the link is broken, use Google search or go to the NVIDIA Jetson Linux Developer Guide and go to the Multimedia section. References README for gst-plugins-bad nvenc How to install NVIDIA Gstreamer plugins (nvenc, nvdec) on Ubuntu by Taras Lishchenko on LifeStyleTransfer Install NVDEC and NVENC on Gstreamer plugins by Corenel on Gist NVIDIA Hardware accelerated video Encoding/Decoding (nvcodec) - Gstreamer by Naresh Ganesan on Medium","title":"Optimization"},{"location":"optimization/#gstreamer-optimization-on-x86","text":"We reduced latency from the default 550ms to 220ms. The latency is measured from the camera to the screen and may be higher with machine vision that may need to move the frame from the GPU to system memory. We achieved this improvement by using two gstreamer plug-ins: nvdec hardware decoding plug-in for NVIDIA GPUs glimagesink OpenGL plug-in","title":"gstreamer optimization on x86"},{"location":"optimization/#overview","text":"nvdec uses dedicated NVIDIA GPU hardware decoding features and fast copy to move frames between system and GPU memory. The THETA H.264 stream is decoded on the GPU and outputs buffers in raw format on the GPU. Instead of downloading the frame from the GPU to system memory, we use glimagesink to display the OpenGL textures to the computer monitor without having to transfer the frame to system memory. To use v4l2loopback, we show how to use gldownload to transfer the frame into system memory. Although this technique increases latency, it appears to be faster than streaming without hardware decoding on our test system.","title":"Overview"},{"location":"optimization/#audience","text":"If you already have streaming working with the THETA on your x86 Linux machine and want to experiment with reducing latency, this article will guide you through installing and configuring hardware decoding on the GPU. If you are using NVIDIA Jetson boards, you do not need this article. On Nano hardware, you are likely already using hardware acceleration. If you are using NVIDIA Jetson Xavier hardware, the hardware decoder is nvv4l2decoder and is included in JetPack, the Jetson OS you download from NVIDIA. On Jetson, the sink is nv3dsink. If you are using x86 and do not have streaming working at all, you should first try this pipeline. pipe_proc = \" decodebin ! autovideosink sync=false qos=false\"; Note that both sync and qos are false. If you have enabled the pipeline above and your framerate is still extremely slow, you can try installing all the gstreamer plug-ins using apt from binaries. If you're still stuck with unusable framerates, this article on using the dedicated video decoder on the GPU may help. However, due to the number of steps involved in installing the gstreamer plug-in, the primary target audience is someone that already has live streaming working and is interested in trying to reduce latency.","title":"Audience"},{"location":"optimization/#tests","text":"","title":"Tests"},{"location":"optimization/#nvdec-and-glimagesink","text":"pipe_proc = \"nvdec ! glimagesink qos=false sync=false\"; foreground: 59.182 THETA video: 58.932 Latency: 250ms","title":"nvdec and glimagesink"},{"location":"optimization/#default-decodebin-and-autovideosink","text":"pipe_proc = \" decodebin ! autovideosink sync=false\"; foreground: 691 THETA video: 141 Latency: 550ms","title":"Default decodebin and autovideosink"},{"location":"optimization/#result-latency-reduced-by-50","text":"","title":"Result: Latency Reduced by 50%"},{"location":"optimization/#equipment","text":"Intel i7-6800K NVIDIA GTX 950 GPU RICOH THETA Z1 with firmware 1.60.1","title":"Equipment"},{"location":"optimization/#software","text":"Ubuntu 20.04 NVIDIA Linux graphics driver 455.23 CUDA Version: 11.1 gstreamer 1.16.2 NVIDIA Video Codec 11.0.10","title":"Software"},{"location":"optimization/#overview-of-steps","text":"verify that you don't have nvdec installed. If you have it installed, you can skip most of this document and go to the section on the gstreamer pipline configuration of gst_viewer.c Download and install gst-plugins-bad Install NVIDIA CODEC SDK Modify gst_viewer pipeline to use the nvdec plug-in for hardware decoding and glimagesink for display to the screen","title":"Overview of Steps"},{"location":"optimization/#tips","text":"","title":"Tips"},{"location":"optimization/#verify-if-you-have-nvdec-installed","text":"$ gst-inspect-1.0 nvdec No such element or plugin 'nvdec' If nvdec is installed, you will see this: $ gst-inspect-1.0 | grep nvdec nvdec: nvdec: NVDEC video decoder","title":"Verify if you have nvdec installed."},{"location":"optimization/#download-the-gst-plugins-bad","text":"After you clone the repo, you need to checkout the branch that is the same as the version of gstreamer you have installed. Clone repo. git clone git://anongit.freedesktop.org/gstreamer/gst-plugins-bad cd gst-plugins-bad/ # verify gstreamer version $ gst-inspect-1.0 --version gst-inspect-1.0 version 1.16.2 GStreamer 1.16.2 $ git checkout 1.16.2 HEAD is now at a6f26408f Release 1.16.2 # verify that you're on the correct branch $ git branch * (HEAD detached at 1.16.2) master","title":"Download the gst-plugins-bad"},{"location":"optimization/#install-nvidia-codec-sdk","text":"Download NVIDIA CODEC SDK . Unzip to /path/to/video/codec/sdk cd /path/to/video/codec/sdk cp /usr/local/cuda/include/cuda.h /path/to/gst-plugins-bad/sys/nvenc cp Interface/nvEncodeAPI.h /path/to/gst-plugins-bad/sys/nvenc cp Interface/cuviddec.h /path/to/gst-plugins-bad/sys/nvdec cp Interface/nvcuvid.h /path/to/gst-plugins-bad/sys/nvdec","title":"Install NVIDIA CODEC SDK"},{"location":"optimization/#build-and-install-plug-in","text":"Configure and build. $ NVENCODE_CFLAGS=\"-I/home/craig/Development/gstreamer/gst-plugins-bad/sys/nvenc\" ./autogen.sh --disable-gtk-doc --with-cuda-prefix=\"/usr/local/cuda\" cd sys/nvenc make sudo cp .libs/libgstnvenc.so /usr/lib/x86_64-linux-gnu/gstreamer-1.0/ cd ../nvdec make sudo cp .libs/libgstnvdec.so /usr/lib/x86_64-linux-gnu/gstreamer-1.0/ Confirm that autogen configured project to build nvdec.","title":"Build and Install Plug-in"},{"location":"optimization/#verify-install","text":"","title":"Verify Install"},{"location":"optimization/#configure-gst_viewerc","text":"Pipeline is roughly around line 192. GitHub permalink is here . else // original pipeline // pipe_proc = \" decodebin ! autovideosink sync=false\"; // use gstreamer plug-in for hardware acceleration pipe_proc = \"nvdec ! glimagesink qos=false sync=false\";","title":"Configure gst_viewer.c"},{"location":"optimization/#results","text":"","title":"Results"},{"location":"optimization/#original-pipeline","text":"The left video is a Logitech C920 USB webcam. The right video is the THETA. Original pipeline. There is a lag on the THETA video when I move my hand.","title":"Original Pipeline"},{"location":"optimization/#nvdec-pipeline","text":"The THETA video stream is now much closer to the latency of the NVIDIA C920.","title":"nvdec pipeline"},{"location":"optimization/#configuration-with-v4l2loopack-on-devvideo","text":"To use nvdec with v4l2loopback, I needed to download the OpenGL textures from the GPU to video frames. This introduced some latency. However, testing with vlc still showed improvement over the standard pipeline. if (strcmp(cmd_name, \"gst_loopback\") == 0) // original pipeline // pipe_proc = \"decodebin ! autovideoconvert ! \" // \"video/x-raw,format=I420 ! identity drop-allocation=true !\" // \"v4l2sink device=/dev/video2 qos=false sync=false\"; // //modified pipeline below pipe_proc = \"nvdec ! gldownload ! videoconvert n-thread=0 ! \" \"video/x-raw,format=I420 ! identity drop-allocation=true !\" \"v4l2sink device=/dev/video2 qos=false sync=false\"; More information on using gldownload is available here .","title":"Configuration with v4l2loopack on /dev/video*"},{"location":"optimization/#v4l2loopback-and-vlc-example","text":"vlc is accessing the camera on /dev/video2 . I'm doing the test at night in a darkened room.","title":"v4l2loopback and vlc example"},{"location":"optimization/#nvidia-jetson","text":"The Jetson is likely already using hardware accleration. You can get more examples on using gstreamer with nvv4l2decoder , nvvidconv , and nv3dsink in the NVIDIA Accelerated GStreamer guide. Hopefuly, this link works . If the link is broken, use Google search or go to the NVIDIA Jetson Linux Developer Guide and go to the Multimedia section.","title":"NVIDIA Jetson"},{"location":"optimization/#references","text":"README for gst-plugins-bad nvenc How to install NVIDIA Gstreamer plugins (nvenc, nvdec) on Ubuntu by Taras Lishchenko on LifeStyleTransfer Install NVDEC and NVENC on Gstreamer plugins by Corenel on Gist NVIDIA Hardware accelerated video Encoding/Decoding (nvcodec) - Gstreamer by Naresh Ganesan on Medium","title":"References"},{"location":"software/","text":"Software Requirements All Software for Live Streaming You need to download the two GitHub repos below and compile the driver and sample code. libuvc-theta libuvc-theta-sample If you want to use /dev/video0 , you will also need v4l2loopback In addition, there are numerous dependencies to compile the tools listed above. However, have no fear, we will walk you through it. How To Compile and Install all Required Software Build and install on x86 Ubuntu 20.04 Jetson Nano with OpenCV and VLC on /dev/video0 Compile libuvc-theta on Jetson Nano - silent screencast Build and run v4l2loopback on Jetson Nano . Needed for /dev/video0 Getting Stream on /dev/video0 Steps: compile and install libuvc-theta compile and install libuvc-theta-sample compile and install v4l2loopack run gst_loopback from libuvc-theta-sample access the correct video device with OpenCV or any video 4 Linux 2 application such as VLC. The video device is specified in the source code . Compile and Install v4l2loopback $ git clone https://github.com/umlaeute/v4l2loopback.git $ cd v4l2loopback $ make $ sudo make install $ sudo depmod -a Load and use This assumes that you have adjusted the video device in gst_viewer.c . $ sudo modprobe v4l2loopback $ cd path_to_gst_loopback_directory $ ./gst_loopback $ cvlc v4l2:///dev/video2 VLC media player 3.0.9.2 Vetinari (revision 3.0.9.2-0-gd4c1aefe4d) [0000556fc2bd6db0] dummy interface: using the dummy interface module... How to Load v4l2loopback automatically In the file /etc/modules-load.d/modules.conf add a new line v4l2loopback . $ pwd /etc/modules-load.d craig@jetson:/etc/modules-load.d$ cat modules.conf # /etc/modules: kernel modules to load at boot time. # # This file contains the names of kernel modules that should be loaded # at boot time, one per line. Lines beginning with \"#\" are ignored. # bluedroid_pm, supporting module for bluetooth bluedroid_pm # modules for camera HAL nvhost_vi # nvgpu module nvgpu # for RICOH THETA live streaming # v4l2loopback device on /dev/video0. specify in gst_viewer.c v4l2loopback craig@jetson:/etc/modules-load.d$ Check kernel module load $ lsmod Module Size Used by bnep 16562 2 zram 26166 4 overlay 48691 0 spidev 13282 0 v4l2loopback 37383 0 nvgpu 1579891 18 bluedroid_pm 13912 0 ip_tables 19441 0 x_tables 28951 1 ip_tables craig@jetson:/etc/modules-load.d$ v4l2loopback tests and examples gst-launch-1.0 pipeline $ gst-launch-1.0 v4l2src device=/dev/video2 ! video/x-raw,framerate=30/1 ! xvimagesink Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Setting pipeline to PLAYING ... New clock: GstSystemClock VLC command line example $ cvlc v4l2:///dev/video2 VLC media player 3.0.9.2 Vetinari (revision 3.0.9.2-0-gd4c1aefe4d) [000055573aea4db0] dummy interface: using the dummy interface module... Use v4l2-ctl to get video device output I\u2019ve modified the source to stream 2K video. $ v4l2-ctl --list-formats-ext --device /dev/video2 ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YU12' (Planar YUV 4:2:0) Size: Discrete 1920x960 Interval: Discrete 0.033s (30.000 fps) USB API libptp - next section for detailed walkthrough","title":"Software"},{"location":"software/#software-requirements","text":"","title":"Software Requirements"},{"location":"software/#all-software-for-live-streaming","text":"You need to download the two GitHub repos below and compile the driver and sample code. libuvc-theta libuvc-theta-sample If you want to use /dev/video0 , you will also need v4l2loopback In addition, there are numerous dependencies to compile the tools listed above. However, have no fear, we will walk you through it.","title":"All Software for Live Streaming"},{"location":"software/#how-to-compile-and-install-all-required-software","text":"Build and install on x86 Ubuntu 20.04 Jetson Nano with OpenCV and VLC on /dev/video0 Compile libuvc-theta on Jetson Nano - silent screencast Build and run v4l2loopback on Jetson Nano . Needed for /dev/video0","title":"How To Compile and Install all Required Software"},{"location":"software/#getting-stream-on-devvideo0","text":"Steps: compile and install libuvc-theta compile and install libuvc-theta-sample compile and install v4l2loopack run gst_loopback from libuvc-theta-sample access the correct video device with OpenCV or any video 4 Linux 2 application such as VLC. The video device is specified in the source code .","title":"Getting Stream on /dev/video0"},{"location":"software/#compile-and-install-v4l2loopback","text":"$ git clone https://github.com/umlaeute/v4l2loopback.git $ cd v4l2loopback $ make $ sudo make install $ sudo depmod -a","title":"Compile and Install v4l2loopback"},{"location":"software/#load-and-use","text":"This assumes that you have adjusted the video device in gst_viewer.c . $ sudo modprobe v4l2loopback $ cd path_to_gst_loopback_directory $ ./gst_loopback $ cvlc v4l2:///dev/video2 VLC media player 3.0.9.2 Vetinari (revision 3.0.9.2-0-gd4c1aefe4d) [0000556fc2bd6db0] dummy interface: using the dummy interface module...","title":"Load and use"},{"location":"software/#how-to-load-v4l2loopback-automatically","text":"In the file /etc/modules-load.d/modules.conf add a new line v4l2loopback . $ pwd /etc/modules-load.d craig@jetson:/etc/modules-load.d$ cat modules.conf # /etc/modules: kernel modules to load at boot time. # # This file contains the names of kernel modules that should be loaded # at boot time, one per line. Lines beginning with \"#\" are ignored. # bluedroid_pm, supporting module for bluetooth bluedroid_pm # modules for camera HAL nvhost_vi # nvgpu module nvgpu # for RICOH THETA live streaming # v4l2loopback device on /dev/video0. specify in gst_viewer.c v4l2loopback craig@jetson:/etc/modules-load.d$","title":"How to Load v4l2loopback automatically"},{"location":"software/#check-kernel-module-load","text":"$ lsmod Module Size Used by bnep 16562 2 zram 26166 4 overlay 48691 0 spidev 13282 0 v4l2loopback 37383 0 nvgpu 1579891 18 bluedroid_pm 13912 0 ip_tables 19441 0 x_tables 28951 1 ip_tables craig@jetson:/etc/modules-load.d$","title":"Check kernel module load"},{"location":"software/#v4l2loopback-tests-and-examples","text":"","title":"v4l2loopback tests and examples"},{"location":"software/#gst-launch-10-pipeline","text":"$ gst-launch-1.0 v4l2src device=/dev/video2 ! video/x-raw,framerate=30/1 ! xvimagesink Setting pipeline to PAUSED ... Pipeline is live and does not need PREROLL ... Setting pipeline to PLAYING ... New clock: GstSystemClock","title":"gst-launch-1.0 pipeline"},{"location":"software/#vlc-command-line-example","text":"$ cvlc v4l2:///dev/video2 VLC media player 3.0.9.2 Vetinari (revision 3.0.9.2-0-gd4c1aefe4d) [000055573aea4db0] dummy interface: using the dummy interface module...","title":"VLC command line example"},{"location":"software/#use-v4l2-ctl-to-get-video-device-output","text":"I\u2019ve modified the source to stream 2K video. $ v4l2-ctl --list-formats-ext --device /dev/video2 ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YU12' (Planar YUV 4:2:0) Size: Discrete 1920x960 Interval: Discrete 0.033s (30.000 fps)","title":"Use v4l2-ctl to get video device output"},{"location":"software/#usb-api","text":"libptp - next section for detailed walkthrough","title":"USB API"},{"location":"usb_api/","text":"Overview RICOH THETA cameras can be controlled and powered indefinitely over a USB cable using the THETA USB API . This is an extension of Media Transfer Protocol (MTP). Any library or application that uses MTP can access the camera. This document explains the most common applications, libraries and techniques to use the RICOH THETA USB API from Linux. Advantages The USB API has the following advantages over the Wi-Fi API: wake camera from sleep put camera to sleep power camer off switch to live streaming mode switch from live streaming mode to still image or video mode theoretical faster transfer speed with USB 3.0 (for the Z1) or USB 2.0 for the V and other models. In actual use, it appears that the USB does transfer faster than Wi-Fi in most cases as there is usually Wi-Fi signal interference or degradation. it is possible to use an unsupported workaround to turn the camera on over the USB cable. This technique doesn't use the USB API and is not supported by RICOH. The technique is explained in the camera section . Hardware and OS We tested the USB API on the following platforms: x86 on Ubuntu 20.04. In the past, we used 18.04, 16.04, and 14.04. NVIDIA Jetson Nano with JetPack 4.4 (Ubuntu 18.04). Othere versions of hardware and software should work. Raspberry Pi 3 with Raspian 10, buster. Any version and any model should work. Note that the Rasbperry Pi 3 and earlier models cannot stream the THETA over a USB cable. MTP software This document covers two different strategies: libptp - either as library or with command line ptpcam gphoto2 - command line and Python bindings to library The most common technique is to use the command line program of libptp called ptpcam. You can put ptpcam in a bash shell script or run a system process from the language you are using. For example, in Python, you can use the subprocess module. In Dart, you can use the Process class . I am using the process_run package. As a demonstration of the USB API, I put a GUI wrapper around ptpcam using Flutter and Dart. The next demo shows a common use of putting the camera to sleep and waking it up. There are several workarounds and fixes that we're using. Feel free to ask questions. libptp and ptpcam libptp2 builds on libusb . The latest version is 2-1.2.0, which was last updated on 2016-01-12. The package builds against an older version of libusb, not the current version that ships with Ubuntu and Raspian. To get around this, you can either install an older version of libusb from source or install libusb-compat-0.1 in addition to libusb-1xx-dev. Do not install libusb-compat-0.1 and libusb-0.1x on the same system. Certain Linux distributions such as ArchLinux/Manjaro have packages for libusb-compat . For Ubuntu 20.04 and 18.04 (JetPack 4.4), I was able to build the package with libusb-dev. For Raspian 10 buster, I used the source code for libusb-compat. As there are many steps and possible places where you may get stuck feel free to post a question in our forum . Download libptp source libptp - Picture Transfer Protocol lib Get the newest version, which is 2-1.2 right now. build libptp $ ./configure $ make If you have a build error when compiling libusb, you may need to install the development libraries for libusb. install libusb-dev For Ubuntu 18.04, 20.04 on x86 and JetPack 4.4 on Nano. $ sudo apt install libusb-dev You may not need this step if you already have the libusb development libraries installed. Example on x86 Ubuntu 20.04. $ sudo apt-get install libusb-dev For Raspberry Pi Version. Distributor ID: Raspbian Description: Raspbian GNU/Linux 10 (buster) Release: 10 Codename: buster Get the libusb compatibility layer. install libptp $ sudo make install On x86 Ubuntu 20.04. $ tar zxvf libptp2-1.2.0.tar.gz libptp2-1.2.0/ ./configure ran with no problems make ran with no problems sudo make install ran with no problems $ pwd /usr/local/lib $ ls -l libptp2.* -rw-r--r-- 1 root root 352640 Aug 31 11:54 libptp2.a -rwxr-xr-x 1 root root 941 Aug 31 11:54 libptp2.la lrwxrwxrwx 1 root root 16 Aug 31 11:54 libptp2.so -> libptp2.so.1.1.5 lrwxrwxrwx 1 root root 16 Aug 31 11:54 libptp2.so.1 -> libptp2.so.1.1.5 -rwxr-xr-x 1 root root 249352 Aug 31 11:54 libptp2.so.1.1.5 set /usr/local/lib in library path The default location of the libptp install is /usr/local/lib . Make sure that this is in your library path. If it isn't, add it to a file such as libc.conf in /etc/ld.so.conf/ . $ cd /etc/ld.so.conf.d/ $ ls $ cat libc.conf run ldconfig Load the library configuration. $ sudo /sbin/ldconfig -v On x86 Ubuntu 20.04. $ cd /etc/ld.so.conf.d/ $ l fakeroot-x86_64-linux-gnu.conf x86_64-linux-gnu.conf i386-linux-gnu.conf zz_i386-biarch-compat.conf libc.conf $ cat libc.conf # libc default configuration /usr/local/lib $ sudo ldconfig $ Test ptpcam Connect RICOH THETA to Jetson with a USB cable. Version of 2-1.2 of libptp has a bug in it. Although ptpcam does take pictures and function normally, you will see an error about capture status. On x86 Ubuntu. $ ptpcam --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e $ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 60 model name : Intel(R) Pentium(R) CPU G3258 @ 3.20GHz Fix problem with libptp response Go to line 77 of ptp.h and change PTP_USB_INT_PACKET_LEN to 28 . After modification, the code will look like this. Using USB API with ptpcam (libptp) test ptpcam response again Take a still image picture with ptpcam --capture . Set camera to live streaming mode Check on camera mode. $ ptpcam --show-property=0x5013 Set to live streaming mode. $ ptpcam --set-property=0x5013 --val=0x8005 Using the official RICOH USB API documentation , you can verify that 0x8005 is live streaming mode. The camera LED should show that the THETA is in LIVE mode. In our tests, the RICOH THETA Z1 could charge while streaming over a USB 3.0 port (blue insert). After hours of streaming, the Z1 LED looks like this. The response codes are shown below. 0x0001 = single-shot shooting 0x0003 = Interval shooting 0x8002 = Movie shooting 0x8003 = Interval composite shooting 0x8004 = Multi bracket shooting 0x8005 = Live streaming 0x8006 = Interval shooting - tripod stabilizatio is off (top/bottom correction and stitching optimized) 0x8007 = Interval shooting - tripod stabilization is on If you set the camera back to still image, single shot mode, you will see this response. $ ptpcam --set-property=0x5013 --val=0x0001 Camera: RICOH THETA V 'Still Capture Mode' is set to: 0x8005 (-32763) Changing property value to 0x0001 [(null)] succeeded. Wake Camera From Sleep In this test, I have the Z1 power off disabled. I left the camera in sleep mode overnight. When I woke up in the morning, I woke the Z1 up using an ssh session into the Jetson Nano and running this command. $ ptpcam --set-property=0xD80E --val=0x00 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 1 Changing property value to 0x00 [(null)] succeeded. I tested the camera with the info command. $ ptpcam --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e In my initial tests, I had to run the info command twice after I woke the camera up from sleep. The first time, I could not open the session. I got this error. $ ptpcam --info ERROR: Could not open session! In the future, I'll run more tests using the camera FunctionalMode to check status. This is another example with x86. Initially, the camera is asleep. craig@cube:~$ ptpcam --info Camera information ================== ERROR: Could not open session! craig@cube:~$ ptpcam --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e craig@cube:~$ ptpcam --set-property=0xd80e --val=0 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 1 Changing property value to 0 [(null)] succeeded. At this point, the camera is awake. Put camera to sleep $ ptpcam --set-property=0xd80e --val=0x01 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0 Changing property value to 0x01 [(null)] succeeded. The camera is asleep. Auto Power Off Delay Disable auto power off. $ ptpcam --set-property=0xd81b=0 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0 Verify that auto power off is disabled. $ ptpcam --show-property=0xd81b Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0 Shutdown Camera This will completely power off the camera and put into lowest battery mode. $ ptpcam -R 0x1013 Camera: RICOH THETA Z1 Sending generic request: reqCode=0x1013, params=[0x00000000,0x00000000,0x00000000,0x00000000,0x00000000] PTP: I/O error ERROR: Could not close session! To turn the camera back on, you must disconnect and then reconnect the USB cable of the camera. You can also replicate this process in software. Put Camera in Still Image Mode You may want to take a detailed picture of the scene based on triggers from the live stream. To do this, you need to take the camera out of live streaming mode and put it into still image mode. In the example below, I wrapped ptpcam in a script that explains the hexcode properties of the mode settings. This helps me with testing. $ ptpcam --set-property=0x5013 --val=0x0001 Camera: RICOH THETA Z1 'Still Capture Mode' is set to: [Normal] Changing property value to 0x0001 [(null)] succeeded. 0x0001 = single-shot shooting 0x0003 = Interval shooting 0x8002 = Movie shooting 0x8003 = Interval composite shooting 0x8004 = Multi bracket shooting 0x8005 = Live streaming 0x8006 = Interval shooting - tripod stabilizatio is off (top/bottom correction and stitching optimized) 0x8007 = Interval shooting - tripod stabilization is on You can verify the mode of with 0x5013. $ ptpcam --show-property=0x5013 Camera: RICOH THETA Z1 'Still Capture Mode' is set to: [Normal] Compare this with the result when the camera is in live streaming mode. $ ptpcam --show-property=0x5013 Camera: RICOH THETA Z1 'Still Capture Mode' is set to: 0x8005 (-32763) 0x0001 = single-shot shooting 0x0003 = Interval shooting 0x8002 = Movie shooting 0x8003 = Interval composite shooting 0x8004 = Multi bracket shooting 0x8005 = Live streaming 0x8006 = Interval shooting - tripod stabilizatio is off (top/bottom correction and stitching optimized) 0x8007 = Interval shooting - tripod stabilization is on Start Video Capture This records video to file. ptpcam -R 0x101c,0,0,1 Using Raw PTP Commands Get camera info . $ ptpcam -R 0x1001 Camera: RICOH THETA Z1 Sending generic request: reqCode=0x1001, params=[0x00000000,0x00000000,0x00000000,0x00000000,0x00000000] 64 00 06 00 00 00 6e 00 00 00 00 33 00 00 00 01 - d.....n....3.... Using Multiple Cameras with ptpcam Test Environment Ubuntu 20.04 on x86 libptp and ptpcam. compiled from source with patches. v 2-1.2.0 (assuming you have this working. If not please post again) Camera with dev id 42 is Z1 with firmware 1.50.1 camera with dev id 41 is V with fimrware 3.40.1 craig@cube:~$ ptpcam --list-devices Listing devices... bus/dev vendorID/prodID device model 003/042 0x05CA/0x036D RICOH THETA Z1 003/041 0x05CA/0x2714 RICOH THETA V $ ptpcam --dev=042 --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e craig@cube:~$ ptpcam --dev=041 --info Camera information ================== Model: RICOH THETA V manufacturer: Ricoh Company, Ltd. serial number: '00105377' device version: 3.40.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e craig@cube:~$ ptpcam --dev=041 --capture Initiating captue... Object added 0x00000226 Capture completed successfully! craig@cube:~$ ptpcam --dev=042 --capture Initiating captue... Object added 0x00000011 Capture completed successfully! Slightly more complex, list files: ~$ ptpcam --dev=042 --list-files Listing files... Camera: RICOH THETA Z1 Handler: Size: Captured: name: 0x0000000e: 9086912 2019-01-01 08:03 R0010001.JPG 0x0000000f: 7968843 2019-01-01 08:00 R0010002.JPG 0x00000010: 7990763 2019-01-01 08:01 R0010003.JPG 0x00000011: 8008310 2019-01-01 08:03 R0010004.JPG I have several hundred pictures on the V, but it showed it. $ ptpcam --dev=041 --list-files Listing files... Camera: RICOH THETA V Handler: Size: Captured: name: 0x00000142: 4152882 2020-06-17 20:59 R0010273.JPG 0x00000143: 3979605 2020-06-17 21:03 R0010274.JPG 0x00000147: 4413502 2020-06-17 21:43 R0010277.JPG ... Test with Two Cameras in Streaming $ ptpcam --dev=041 --set-property=0x5013 --val=0x8005 Camera: RICOH THETA V (bus 0, dev 41) 'Still Capture Mode' is set to: [Normal] Changing property value to 0x8005 [(null)] succeeded. craig@cube:~$ $ ptpcam --dev=042 --set-property=0x5013 --val=0x8005 Camera: RICOH THETA Z1 (bus 0, dev 42) 'Still Capture Mode' is set to: [Normal] Changing property value to 0x8005 [(null)] succeeded. At this stage, I now have two cameras streaming into the same devices. I need to do more tests to manipulate both streams. however, if your application is handing the stream and image processing already, then you should be good to go. Other Ways to Grab Device ID You can also grab the device ID with lsusb or libusb. Compare the device IDs to ptpcam --list-devices . The IDs should be the same. gphoto2 Command Line Fixing Command Line Error - Could not claim the USB device You may get this error. $ gphoto2 --capture-image *** Error *** An error occurred in the io-library ('Could not claim the USB device'): Could not claim interface 0 (Device or resource busy). Make sure no other program (gvfs-gphoto2-volume-monitor) or kernel module (such as sdc2xx, stv680, spca50x) is using the device and you have read/write access to the device. ERROR: Could not capture image. ERROR: Could not capture. *** Error (-53: 'Could not claim the USB device') *** Fix for current session is to kill gvfs-gphoto2-volume-monitor and gvfsd-gphoto2 spawner. $ ps aux |grep gvfs ... craig 2422 0.0 0.0 442504 13528 ? Sl 08:19 0:00 /usr/libexec/gvfsd-gphoto2 --spawner :1.3 /org/gtk/gvfs/exec_spaw/1 ... craig 1969 0.0 0.0 249860 10032 ? Ssl 08:19 0:00 /usr/libexec/gvfs-gphoto2-volume-monitor ... $ kill 2422 $ kill 1969 $ gphoto2 --capture-image New file is in location /store_00020001/DCIM/100RICOH/R0010376.JPG on the camera $ Removing gvfs-backend permanently If you don't mount the THETA as a storage device with gphoto, you can remove gvfs-backend. This is a workaround for the conflict when you use gphoto2 from the command line to talk to the THETA. $ sudo apt remove gvfs-backends [sudo] password for craig: Reading package lists... Done Building dependency tree Reboot to test. After reboot. $ gphoto2 -l There is 1 folder in folder '/'. - store_00020001 There is 1 folder in folder '/store_00020001'. - DCIM There are 2 folders in folder '/store_00020001/DCIM'. - 100RICOH - SingleLensShooting There is 1 folder in folder '/store_00020001/DCIM/100RICOH'. - HDR07-22_18-13 There are 0 folders in folder '/store_00020001/DCIM/100RICOH/HDR07-22_18-13'. There are 0 folders in folder '/store_00020001/DCIM/SingleLensShooting'. It works! Check Camera Mode (still image, video, streaming) StillCaptureMode API reference $ gphoto2 --get-config=5013 Label: Still Capture Mode Readonly: 0 Type: MENU Current: 1 Choice: 0 1 Choice: 1 3 Choice: 2 32770 Choice: 3 32771 Choice: 4 32772 Choice: 5 32773 Choice: 6 32774 Choice: 7 32775 END craig@craig-desktop:~$ unmount camera If the camera is mounted, the commands may not work. set to video mode Using the API reference , we can see that video mode is hex 0x8002 or 32770 in base 10. $ gphoto2 --set-config=5013=32770 start video $ gphoto2 --set-config movie=1 stop video this tip contributed by hugues $ gphoto2 --set-config=/main/actions/opcode=0x1018,0xFFFFFFFF start video and stop after specified time $ gphoto2 --set-config movie=1 --wait-event=2s --set-config movie=0 Python bindings From community member mhenrie original post \"\"\" USB api for added performance over http Theta api reference: https://developers.theta360.com/en/docs/v2/usb_reference/ Unable to get mtp or ptp to connect to the camera; After some pain was able to get gphoto2 working \"\"\" import os import time import gphoto2 as gp # Properties SHUTTER_SPEED = 'd00f' EXPOSURE_INDEX = '500f' F_NUMBER = '5007' AUDIO_VOLUME = '502c' COLOR_TEMPERATURE = 'd813' EXPOSURE_PROGRAM_MODE = '500e' # milliseconds TIMEOUT = 10 TIMEOUT_CAPTURE_DNG = 10000 def wait_for_event(camera, timeout=TIMEOUT, event_type=gp.GP_EVENT_TIMEOUT): \"\"\" Wait for event_type to to be triggered. :param camera: :param timeout: :param event_type: :return: event_data \"\"\" while True: _event_type, event_data = camera.wait_for_event(timeout) if _event_type == gp.GP_EVENT_TIMEOUT: return if _event_type == event_type: return event_data def set_config_by_index(config, index): \"\"\"Set config using choice index\"\"\" value = config.get_choice(index) config.set_value(value) return config # def list_files(camera, path='/'): # result = [] # # get files # for name, value in camera.folder_list_files(path): # result.append(os.path.join(path, name)) # # read folders # folders = [] # for name, value in camera.folder_list_folders(path): # folders.append(name) # # recurse over subfolders # for name in folders: # result.extend(list_files(camera, os.path.join(path, name))) # return result # # # def get_file_info(camera, path): # folder, name = os.path.split(path) # return camera.file_get_info(folder, name) class CameraUsb(object): \"\"\" Define API for multiple exposure \"\"\" def __init__(self, verbose=False): self.verbose = verbose self.camera = gp.Camera() self.camera_config = None self.status_config = None self.other_config = None self.shutter_speed_config = None self.shutter_speed_options = [] def init(self): \"\"\" Set manual exposure and other defaults :return: config \"\"\" try: self.camera_config = self.camera.get_config() except gp.GPhoto2Error: raise RuntimeError(\"Unable to connect to Camera\") self.other_config = self.camera_config.get_child_by_name('other') # Manual/f-stop/iso exposure_program_mode = self.other_config.get_child_by_name(EXPOSURE_PROGRAM_MODE) if not exposure_program_mode.get_value() == '1': print('Setting camera to Manual exposure program') exposure_program_mode.set_value('1') self.camera.set_config(self.camera_config) wait_for_event(self.camera) # When switching exposure program, we need to refresh the configs self.camera_config = self.camera.get_config() self.other_config = self.camera_config.get_child_by_name('other') self.status_config = self.camera_config.get_child_by_name('status') self.shutter_speed_config = self.other_config.get_child_by_name(SHUTTER_SPEED) self.shutter_speed_options = [str(x) for x in self.shutter_speed_config.get_choices()] if len(self.shutter_speed_options) != 61: raise RuntimeError('Unble to determine shutter speed options; restart app') fstop = self.other_config.get_child_by_name(F_NUMBER) fstop.set_value('560') iso = self.other_config.get_child_by_name(EXPOSURE_INDEX) iso.set_value('80') self.camera.set_config(self.camera_config) wait_for_event(self.camera) def get_info(self): \"\"\" :return: Dict containing serialnumber, batterylevel, remainingpictures, etc \"\"\" if not self.camera_config: self.init() battery_level = self.status_config.get_child_by_name('batterylevel').get_value() # Convert '67%' to int battery_level = int(''.join([x for x in battery_level if x.isdigit()])) info = {'serialnumber': self.status_config.get_child_by_name('serialnumber').get_value(), 'cameramodel': self.status_config.get_child_by_name('cameramodel').get_value(), 'deviceversion': self.status_config.get_child_by_name('deviceversion').get_value(), 'batterylevel': battery_level, 'remainingpictures': int(self.camera.get_storageinfo()[0].freeimages)} return info def take_picture(self, shutter_speed_index=None, color_temperature=None, volume=None): \"\"\" Set camera options and take picture Blocking :param shutter_speed_index: int in range 0-60 (0 fastest shutter) :param color_temperature: in in range 2500-10000 by 100 increment :param volume: int in range 0-100 :return: (jpg_path, dng_path) \"\"\" t1 = time.time() if not self.camera_config: self.init() if shutter_speed_index is not None: self.shutter_speed_config.set_value(self.shutter_speed_options[shutter_speed_index]) if color_temperature is not None: self.other_config.get_child_by_name(COLOR_TEMPERATURE).set_value(color_temperature) if volume is not None: self.other_config.get_child_by_name(AUDIO_VOLUME).set_value(str(volume)) self.camera.set_config(self.camera_config) # We need this even though no event is triggered wait_for_event(self.camera) gp_jpg_path = self.camera.capture(gp.GP_CAPTURE_IMAGE) gp_dng_path = wait_for_event(self.camera, timeout=TIMEOUT_CAPTURE_DNG, event_type=gp.GP_EVENT_FILE_ADDED) if not gp_dng_path: raise RuntimeError('Unable to copy DNG') jpg_path = os.path.join(gp_jpg_path.folder, gp_jpg_path.name) dng_path = os.path.join(gp_dng_path.folder, gp_dng_path.name) print('Capture took %0.03f sec' % (time.time() - t1, )) return jpg_path, dng_path def download_file(self, src_path, dst_path, delete=True): \"\"\"Copy the file from the camera src_path to local dst_path\"\"\" t1 = time.time() src_folder, src_name = os.path.split(src_path) src_file = self.camera.file_get(src_folder, src_name, gp.GP_FILE_TYPE_NORMAL) print('Download %s ->\\n\\t%s' % (src_path, dst_path)) src_file.save(dst_path) wait_for_event(self.camera) print('Download took %0.03f sec' % (time.time() - t1, )) if delete: t1 = time.time() print('Delete %s' % src_path) self.camera.file_delete(src_folder, src_name) wait_for_event(self.camera) print('Delete took %0.03f sec' % (time.time() - t1, )) def _unittest(): \"\"\"test a short exposure sequence\"\"\" # temporary directory dst_template = '/tmp/theta/capture.%04d.%s' t1 = time.time() camera = CameraUsb() camera.init() print(camera.get_info()) frame = 1 jpg_path, dng_path = camera.take_picture(0) print(jpg_path, dng_path) camera.download_file(dng_path, dst_template % (frame, 'dng')) frame += 1 jpg_path, dng_path = camera.take_picture(24) print(jpg_path, dng_path) camera.download_file(dng_path, dst_template % (frame, 'dng')) frame += 1 jpg_path, dng_path = camera.take_picture(42) print(jpg_path, dng_path) camera.download_file(dng_path, dst_template % (frame, 'dng')) frame += 1 print('Done in %0.03f sec' % (time.time() - t1, )) if __name__ == \"__main__\": _unittest()","title":"USB API"},{"location":"usb_api/#overview","text":"RICOH THETA cameras can be controlled and powered indefinitely over a USB cable using the THETA USB API . This is an extension of Media Transfer Protocol (MTP). Any library or application that uses MTP can access the camera. This document explains the most common applications, libraries and techniques to use the RICOH THETA USB API from Linux.","title":"Overview"},{"location":"usb_api/#advantages","text":"The USB API has the following advantages over the Wi-Fi API: wake camera from sleep put camera to sleep power camer off switch to live streaming mode switch from live streaming mode to still image or video mode theoretical faster transfer speed with USB 3.0 (for the Z1) or USB 2.0 for the V and other models. In actual use, it appears that the USB does transfer faster than Wi-Fi in most cases as there is usually Wi-Fi signal interference or degradation. it is possible to use an unsupported workaround to turn the camera on over the USB cable. This technique doesn't use the USB API and is not supported by RICOH. The technique is explained in the camera section .","title":"Advantages"},{"location":"usb_api/#hardware-and-os","text":"We tested the USB API on the following platforms: x86 on Ubuntu 20.04. In the past, we used 18.04, 16.04, and 14.04. NVIDIA Jetson Nano with JetPack 4.4 (Ubuntu 18.04). Othere versions of hardware and software should work. Raspberry Pi 3 with Raspian 10, buster. Any version and any model should work. Note that the Rasbperry Pi 3 and earlier models cannot stream the THETA over a USB cable.","title":"Hardware and OS"},{"location":"usb_api/#mtp-software","text":"This document covers two different strategies: libptp - either as library or with command line ptpcam gphoto2 - command line and Python bindings to library The most common technique is to use the command line program of libptp called ptpcam. You can put ptpcam in a bash shell script or run a system process from the language you are using. For example, in Python, you can use the subprocess module. In Dart, you can use the Process class . I am using the process_run package. As a demonstration of the USB API, I put a GUI wrapper around ptpcam using Flutter and Dart. The next demo shows a common use of putting the camera to sleep and waking it up. There are several workarounds and fixes that we're using. Feel free to ask questions.","title":"MTP software"},{"location":"usb_api/#libptp-and-ptpcam","text":"libptp2 builds on libusb . The latest version is 2-1.2.0, which was last updated on 2016-01-12. The package builds against an older version of libusb, not the current version that ships with Ubuntu and Raspian. To get around this, you can either install an older version of libusb from source or install libusb-compat-0.1 in addition to libusb-1xx-dev. Do not install libusb-compat-0.1 and libusb-0.1x on the same system. Certain Linux distributions such as ArchLinux/Manjaro have packages for libusb-compat . For Ubuntu 20.04 and 18.04 (JetPack 4.4), I was able to build the package with libusb-dev. For Raspian 10 buster, I used the source code for libusb-compat. As there are many steps and possible places where you may get stuck feel free to post a question in our forum .","title":"libptp and ptpcam"},{"location":"usb_api/#download-libptp-source","text":"libptp - Picture Transfer Protocol lib Get the newest version, which is 2-1.2 right now.","title":"Download libptp source"},{"location":"usb_api/#build-libptp","text":"$ ./configure $ make If you have a build error when compiling libusb, you may need to install the development libraries for libusb.","title":"build libptp"},{"location":"usb_api/#install-libusb-dev","text":"For Ubuntu 18.04, 20.04 on x86 and JetPack 4.4 on Nano. $ sudo apt install libusb-dev You may not need this step if you already have the libusb development libraries installed. Example on x86 Ubuntu 20.04. $ sudo apt-get install libusb-dev","title":"install libusb-dev"},{"location":"usb_api/#for-raspberry-pi","text":"Version. Distributor ID: Raspbian Description: Raspbian GNU/Linux 10 (buster) Release: 10 Codename: buster Get the libusb compatibility layer.","title":"For Raspberry Pi"},{"location":"usb_api/#install-libptp","text":"$ sudo make install On x86 Ubuntu 20.04. $ tar zxvf libptp2-1.2.0.tar.gz libptp2-1.2.0/ ./configure ran with no problems make ran with no problems sudo make install ran with no problems $ pwd /usr/local/lib $ ls -l libptp2.* -rw-r--r-- 1 root root 352640 Aug 31 11:54 libptp2.a -rwxr-xr-x 1 root root 941 Aug 31 11:54 libptp2.la lrwxrwxrwx 1 root root 16 Aug 31 11:54 libptp2.so -> libptp2.so.1.1.5 lrwxrwxrwx 1 root root 16 Aug 31 11:54 libptp2.so.1 -> libptp2.so.1.1.5 -rwxr-xr-x 1 root root 249352 Aug 31 11:54 libptp2.so.1.1.5","title":"install libptp"},{"location":"usb_api/#set-usrlocallib-in-library-path","text":"The default location of the libptp install is /usr/local/lib . Make sure that this is in your library path. If it isn't, add it to a file such as libc.conf in /etc/ld.so.conf/ . $ cd /etc/ld.so.conf.d/ $ ls $ cat libc.conf","title":"set /usr/local/lib in library path"},{"location":"usb_api/#run-ldconfig","text":"Load the library configuration. $ sudo /sbin/ldconfig -v On x86 Ubuntu 20.04. $ cd /etc/ld.so.conf.d/ $ l fakeroot-x86_64-linux-gnu.conf x86_64-linux-gnu.conf i386-linux-gnu.conf zz_i386-biarch-compat.conf libc.conf $ cat libc.conf # libc default configuration /usr/local/lib $ sudo ldconfig $","title":"run ldconfig"},{"location":"usb_api/#test-ptpcam","text":"Connect RICOH THETA to Jetson with a USB cable. Version of 2-1.2 of libptp has a bug in it. Although ptpcam does take pictures and function normally, you will see an error about capture status. On x86 Ubuntu. $ ptpcam --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e $ cat /proc/cpuinfo processor : 0 vendor_id : GenuineIntel cpu family : 6 model : 60 model name : Intel(R) Pentium(R) CPU G3258 @ 3.20GHz","title":"Test ptpcam"},{"location":"usb_api/#fix-problem-with-libptp-response","text":"Go to line 77 of ptp.h and change PTP_USB_INT_PACKET_LEN to 28 . After modification, the code will look like this.","title":"Fix problem with libptp response"},{"location":"usb_api/#using-usb-api-with-ptpcam-libptp","text":"","title":"Using USB API with ptpcam (libptp)"},{"location":"usb_api/#test-ptpcam-response-again","text":"Take a still image picture with ptpcam --capture .","title":"test ptpcam response again"},{"location":"usb_api/#set-camera-to-live-streaming-mode","text":"Check on camera mode. $ ptpcam --show-property=0x5013 Set to live streaming mode. $ ptpcam --set-property=0x5013 --val=0x8005 Using the official RICOH USB API documentation , you can verify that 0x8005 is live streaming mode. The camera LED should show that the THETA is in LIVE mode. In our tests, the RICOH THETA Z1 could charge while streaming over a USB 3.0 port (blue insert). After hours of streaming, the Z1 LED looks like this. The response codes are shown below. 0x0001 = single-shot shooting 0x0003 = Interval shooting 0x8002 = Movie shooting 0x8003 = Interval composite shooting 0x8004 = Multi bracket shooting 0x8005 = Live streaming 0x8006 = Interval shooting - tripod stabilizatio is off (top/bottom correction and stitching optimized) 0x8007 = Interval shooting - tripod stabilization is on If you set the camera back to still image, single shot mode, you will see this response. $ ptpcam --set-property=0x5013 --val=0x0001 Camera: RICOH THETA V 'Still Capture Mode' is set to: 0x8005 (-32763) Changing property value to 0x0001 [(null)] succeeded.","title":"Set camera to live streaming mode"},{"location":"usb_api/#wake-camera-from-sleep","text":"In this test, I have the Z1 power off disabled. I left the camera in sleep mode overnight. When I woke up in the morning, I woke the Z1 up using an ssh session into the Jetson Nano and running this command. $ ptpcam --set-property=0xD80E --val=0x00 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 1 Changing property value to 0x00 [(null)] succeeded. I tested the camera with the info command. $ ptpcam --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e In my initial tests, I had to run the info command twice after I woke the camera up from sleep. The first time, I could not open the session. I got this error. $ ptpcam --info ERROR: Could not open session! In the future, I'll run more tests using the camera FunctionalMode to check status. This is another example with x86. Initially, the camera is asleep. craig@cube:~$ ptpcam --info Camera information ================== ERROR: Could not open session! craig@cube:~$ ptpcam --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e craig@cube:~$ ptpcam --set-property=0xd80e --val=0 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 1 Changing property value to 0 [(null)] succeeded. At this point, the camera is awake.","title":"Wake Camera From Sleep"},{"location":"usb_api/#put-camera-to-sleep","text":"$ ptpcam --set-property=0xd80e --val=0x01 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0 Changing property value to 0x01 [(null)] succeeded. The camera is asleep.","title":"Put camera to sleep"},{"location":"usb_api/#auto-power-off-delay","text":"Disable auto power off. $ ptpcam --set-property=0xd81b=0 Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0 Verify that auto power off is disabled. $ ptpcam --show-property=0xd81b Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0","title":"Auto Power Off Delay"},{"location":"usb_api/#shutdown-camera","text":"This will completely power off the camera and put into lowest battery mode. $ ptpcam -R 0x1013 Camera: RICOH THETA Z1 Sending generic request: reqCode=0x1013, params=[0x00000000,0x00000000,0x00000000,0x00000000,0x00000000] PTP: I/O error ERROR: Could not close session! To turn the camera back on, you must disconnect and then reconnect the USB cable of the camera. You can also replicate this process in software.","title":"Shutdown Camera"},{"location":"usb_api/#put-camera-in-still-image-mode","text":"You may want to take a detailed picture of the scene based on triggers from the live stream. To do this, you need to take the camera out of live streaming mode and put it into still image mode. In the example below, I wrapped ptpcam in a script that explains the hexcode properties of the mode settings. This helps me with testing. $ ptpcam --set-property=0x5013 --val=0x0001 Camera: RICOH THETA Z1 'Still Capture Mode' is set to: [Normal] Changing property value to 0x0001 [(null)] succeeded. 0x0001 = single-shot shooting 0x0003 = Interval shooting 0x8002 = Movie shooting 0x8003 = Interval composite shooting 0x8004 = Multi bracket shooting 0x8005 = Live streaming 0x8006 = Interval shooting - tripod stabilizatio is off (top/bottom correction and stitching optimized) 0x8007 = Interval shooting - tripod stabilization is on You can verify the mode of with 0x5013. $ ptpcam --show-property=0x5013 Camera: RICOH THETA Z1 'Still Capture Mode' is set to: [Normal] Compare this with the result when the camera is in live streaming mode. $ ptpcam --show-property=0x5013 Camera: RICOH THETA Z1 'Still Capture Mode' is set to: 0x8005 (-32763) 0x0001 = single-shot shooting 0x0003 = Interval shooting 0x8002 = Movie shooting 0x8003 = Interval composite shooting 0x8004 = Multi bracket shooting 0x8005 = Live streaming 0x8006 = Interval shooting - tripod stabilizatio is off (top/bottom correction and stitching optimized) 0x8007 = Interval shooting - tripod stabilization is on","title":"Put Camera in Still Image Mode"},{"location":"usb_api/#start-video-capture","text":"This records video to file. ptpcam -R 0x101c,0,0,1","title":"Start Video Capture"},{"location":"usb_api/#using-raw-ptp-commands","text":"Get camera info . $ ptpcam -R 0x1001 Camera: RICOH THETA Z1 Sending generic request: reqCode=0x1001, params=[0x00000000,0x00000000,0x00000000,0x00000000,0x00000000] 64 00 06 00 00 00 6e 00 00 00 00 33 00 00 00 01 - d.....n....3....","title":"Using Raw PTP Commands"},{"location":"usb_api/#using-multiple-cameras-with-ptpcam","text":"","title":"Using Multiple Cameras with ptpcam"},{"location":"usb_api/#test-environment","text":"Ubuntu 20.04 on x86 libptp and ptpcam. compiled from source with patches. v 2-1.2.0 (assuming you have this working. If not please post again) Camera with dev id 42 is Z1 with firmware 1.50.1 camera with dev id 41 is V with fimrware 3.40.1 craig@cube:~$ ptpcam --list-devices Listing devices... bus/dev vendorID/prodID device model 003/042 0x05CA/0x036D RICOH THETA Z1 003/041 0x05CA/0x2714 RICOH THETA V $ ptpcam --dev=042 --info Camera information ================== Model: RICOH THETA Z1 manufacturer: Ricoh Company, Ltd. serial number: '10010104' device version: 1.50.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e craig@cube:~$ ptpcam --dev=041 --info Camera information ================== Model: RICOH THETA V manufacturer: Ricoh Company, Ltd. serial number: '00105377' device version: 3.40.1 extension ID: 0x00000006 extension description: (null) extension version: 0x006e craig@cube:~$ ptpcam --dev=041 --capture Initiating captue... Object added 0x00000226 Capture completed successfully! craig@cube:~$ ptpcam --dev=042 --capture Initiating captue... Object added 0x00000011 Capture completed successfully! Slightly more complex, list files: ~$ ptpcam --dev=042 --list-files Listing files... Camera: RICOH THETA Z1 Handler: Size: Captured: name: 0x0000000e: 9086912 2019-01-01 08:03 R0010001.JPG 0x0000000f: 7968843 2019-01-01 08:00 R0010002.JPG 0x00000010: 7990763 2019-01-01 08:01 R0010003.JPG 0x00000011: 8008310 2019-01-01 08:03 R0010004.JPG I have several hundred pictures on the V, but it showed it. $ ptpcam --dev=041 --list-files Listing files... Camera: RICOH THETA V Handler: Size: Captured: name: 0x00000142: 4152882 2020-06-17 20:59 R0010273.JPG 0x00000143: 3979605 2020-06-17 21:03 R0010274.JPG 0x00000147: 4413502 2020-06-17 21:43 R0010277.JPG ...","title":"Test Environment"},{"location":"usb_api/#test-with-two-cameras-in-streaming","text":"$ ptpcam --dev=041 --set-property=0x5013 --val=0x8005 Camera: RICOH THETA V (bus 0, dev 41) 'Still Capture Mode' is set to: [Normal] Changing property value to 0x8005 [(null)] succeeded. craig@cube:~$ $ ptpcam --dev=042 --set-property=0x5013 --val=0x8005 Camera: RICOH THETA Z1 (bus 0, dev 42) 'Still Capture Mode' is set to: [Normal] Changing property value to 0x8005 [(null)] succeeded. At this stage, I now have two cameras streaming into the same devices. I need to do more tests to manipulate both streams. however, if your application is handing the stream and image processing already, then you should be good to go.","title":"Test with Two Cameras in Streaming"},{"location":"usb_api/#other-ways-to-grab-device-id","text":"You can also grab the device ID with lsusb or libusb. Compare the device IDs to ptpcam --list-devices . The IDs should be the same.","title":"Other Ways to Grab Device ID"},{"location":"usb_api/#gphoto2","text":"","title":"gphoto2"},{"location":"usb_api/#command-line","text":"","title":"Command Line"},{"location":"usb_api/#fixing-command-line-error-could-not-claim-the-usb-device","text":"You may get this error. $ gphoto2 --capture-image *** Error *** An error occurred in the io-library ('Could not claim the USB device'): Could not claim interface 0 (Device or resource busy). Make sure no other program (gvfs-gphoto2-volume-monitor) or kernel module (such as sdc2xx, stv680, spca50x) is using the device and you have read/write access to the device. ERROR: Could not capture image. ERROR: Could not capture. *** Error (-53: 'Could not claim the USB device') *** Fix for current session is to kill gvfs-gphoto2-volume-monitor and gvfsd-gphoto2 spawner. $ ps aux |grep gvfs ... craig 2422 0.0 0.0 442504 13528 ? Sl 08:19 0:00 /usr/libexec/gvfsd-gphoto2 --spawner :1.3 /org/gtk/gvfs/exec_spaw/1 ... craig 1969 0.0 0.0 249860 10032 ? Ssl 08:19 0:00 /usr/libexec/gvfs-gphoto2-volume-monitor ... $ kill 2422 $ kill 1969 $ gphoto2 --capture-image New file is in location /store_00020001/DCIM/100RICOH/R0010376.JPG on the camera $","title":"Fixing Command Line Error - Could not claim the USB device"},{"location":"usb_api/#removing-gvfs-backend-permanently","text":"If you don't mount the THETA as a storage device with gphoto, you can remove gvfs-backend. This is a workaround for the conflict when you use gphoto2 from the command line to talk to the THETA. $ sudo apt remove gvfs-backends [sudo] password for craig: Reading package lists... Done Building dependency tree Reboot to test. After reboot. $ gphoto2 -l There is 1 folder in folder '/'. - store_00020001 There is 1 folder in folder '/store_00020001'. - DCIM There are 2 folders in folder '/store_00020001/DCIM'. - 100RICOH - SingleLensShooting There is 1 folder in folder '/store_00020001/DCIM/100RICOH'. - HDR07-22_18-13 There are 0 folders in folder '/store_00020001/DCIM/100RICOH/HDR07-22_18-13'. There are 0 folders in folder '/store_00020001/DCIM/SingleLensShooting'. It works!","title":"Removing gvfs-backend permanently"},{"location":"usb_api/#check-camera-mode-still-image-video-streaming","text":"StillCaptureMode API reference $ gphoto2 --get-config=5013 Label: Still Capture Mode Readonly: 0 Type: MENU Current: 1 Choice: 0 1 Choice: 1 3 Choice: 2 32770 Choice: 3 32771 Choice: 4 32772 Choice: 5 32773 Choice: 6 32774 Choice: 7 32775 END craig@craig-desktop:~$","title":"Check Camera Mode (still image, video, streaming)"},{"location":"usb_api/#unmount-camera","text":"If the camera is mounted, the commands may not work.","title":"unmount camera"},{"location":"usb_api/#set-to-video-mode","text":"Using the API reference , we can see that video mode is hex 0x8002 or 32770 in base 10. $ gphoto2 --set-config=5013=32770","title":"set to video mode"},{"location":"usb_api/#start-video","text":"$ gphoto2 --set-config movie=1","title":"start video"},{"location":"usb_api/#stop-video","text":"this tip contributed by hugues $ gphoto2 --set-config=/main/actions/opcode=0x1018,0xFFFFFFFF","title":"stop video"},{"location":"usb_api/#start-video-and-stop-after-specified-time","text":"$ gphoto2 --set-config movie=1 --wait-event=2s --set-config movie=0","title":"start video and stop after specified time"},{"location":"usb_api/#python-bindings","text":"From community member mhenrie original post \"\"\" USB api for added performance over http Theta api reference: https://developers.theta360.com/en/docs/v2/usb_reference/ Unable to get mtp or ptp to connect to the camera; After some pain was able to get gphoto2 working \"\"\" import os import time import gphoto2 as gp # Properties SHUTTER_SPEED = 'd00f' EXPOSURE_INDEX = '500f' F_NUMBER = '5007' AUDIO_VOLUME = '502c' COLOR_TEMPERATURE = 'd813' EXPOSURE_PROGRAM_MODE = '500e' # milliseconds TIMEOUT = 10 TIMEOUT_CAPTURE_DNG = 10000 def wait_for_event(camera, timeout=TIMEOUT, event_type=gp.GP_EVENT_TIMEOUT): \"\"\" Wait for event_type to to be triggered. :param camera: :param timeout: :param event_type: :return: event_data \"\"\" while True: _event_type, event_data = camera.wait_for_event(timeout) if _event_type == gp.GP_EVENT_TIMEOUT: return if _event_type == event_type: return event_data def set_config_by_index(config, index): \"\"\"Set config using choice index\"\"\" value = config.get_choice(index) config.set_value(value) return config # def list_files(camera, path='/'): # result = [] # # get files # for name, value in camera.folder_list_files(path): # result.append(os.path.join(path, name)) # # read folders # folders = [] # for name, value in camera.folder_list_folders(path): # folders.append(name) # # recurse over subfolders # for name in folders: # result.extend(list_files(camera, os.path.join(path, name))) # return result # # # def get_file_info(camera, path): # folder, name = os.path.split(path) # return camera.file_get_info(folder, name) class CameraUsb(object): \"\"\" Define API for multiple exposure \"\"\" def __init__(self, verbose=False): self.verbose = verbose self.camera = gp.Camera() self.camera_config = None self.status_config = None self.other_config = None self.shutter_speed_config = None self.shutter_speed_options = [] def init(self): \"\"\" Set manual exposure and other defaults :return: config \"\"\" try: self.camera_config = self.camera.get_config() except gp.GPhoto2Error: raise RuntimeError(\"Unable to connect to Camera\") self.other_config = self.camera_config.get_child_by_name('other') # Manual/f-stop/iso exposure_program_mode = self.other_config.get_child_by_name(EXPOSURE_PROGRAM_MODE) if not exposure_program_mode.get_value() == '1': print('Setting camera to Manual exposure program') exposure_program_mode.set_value('1') self.camera.set_config(self.camera_config) wait_for_event(self.camera) # When switching exposure program, we need to refresh the configs self.camera_config = self.camera.get_config() self.other_config = self.camera_config.get_child_by_name('other') self.status_config = self.camera_config.get_child_by_name('status') self.shutter_speed_config = self.other_config.get_child_by_name(SHUTTER_SPEED) self.shutter_speed_options = [str(x) for x in self.shutter_speed_config.get_choices()] if len(self.shutter_speed_options) != 61: raise RuntimeError('Unble to determine shutter speed options; restart app') fstop = self.other_config.get_child_by_name(F_NUMBER) fstop.set_value('560') iso = self.other_config.get_child_by_name(EXPOSURE_INDEX) iso.set_value('80') self.camera.set_config(self.camera_config) wait_for_event(self.camera) def get_info(self): \"\"\" :return: Dict containing serialnumber, batterylevel, remainingpictures, etc \"\"\" if not self.camera_config: self.init() battery_level = self.status_config.get_child_by_name('batterylevel').get_value() # Convert '67%' to int battery_level = int(''.join([x for x in battery_level if x.isdigit()])) info = {'serialnumber': self.status_config.get_child_by_name('serialnumber').get_value(), 'cameramodel': self.status_config.get_child_by_name('cameramodel').get_value(), 'deviceversion': self.status_config.get_child_by_name('deviceversion').get_value(), 'batterylevel': battery_level, 'remainingpictures': int(self.camera.get_storageinfo()[0].freeimages)} return info def take_picture(self, shutter_speed_index=None, color_temperature=None, volume=None): \"\"\" Set camera options and take picture Blocking :param shutter_speed_index: int in range 0-60 (0 fastest shutter) :param color_temperature: in in range 2500-10000 by 100 increment :param volume: int in range 0-100 :return: (jpg_path, dng_path) \"\"\" t1 = time.time() if not self.camera_config: self.init() if shutter_speed_index is not None: self.shutter_speed_config.set_value(self.shutter_speed_options[shutter_speed_index]) if color_temperature is not None: self.other_config.get_child_by_name(COLOR_TEMPERATURE).set_value(color_temperature) if volume is not None: self.other_config.get_child_by_name(AUDIO_VOLUME).set_value(str(volume)) self.camera.set_config(self.camera_config) # We need this even though no event is triggered wait_for_event(self.camera) gp_jpg_path = self.camera.capture(gp.GP_CAPTURE_IMAGE) gp_dng_path = wait_for_event(self.camera, timeout=TIMEOUT_CAPTURE_DNG, event_type=gp.GP_EVENT_FILE_ADDED) if not gp_dng_path: raise RuntimeError('Unable to copy DNG') jpg_path = os.path.join(gp_jpg_path.folder, gp_jpg_path.name) dng_path = os.path.join(gp_dng_path.folder, gp_dng_path.name) print('Capture took %0.03f sec' % (time.time() - t1, )) return jpg_path, dng_path def download_file(self, src_path, dst_path, delete=True): \"\"\"Copy the file from the camera src_path to local dst_path\"\"\" t1 = time.time() src_folder, src_name = os.path.split(src_path) src_file = self.camera.file_get(src_folder, src_name, gp.GP_FILE_TYPE_NORMAL) print('Download %s ->\\n\\t%s' % (src_path, dst_path)) src_file.save(dst_path) wait_for_event(self.camera) print('Download took %0.03f sec' % (time.time() - t1, )) if delete: t1 = time.time() print('Delete %s' % src_path) self.camera.file_delete(src_folder, src_name) wait_for_event(self.camera) print('Delete took %0.03f sec' % (time.time() - t1, )) def _unittest(): \"\"\"test a short exposure sequence\"\"\" # temporary directory dst_template = '/tmp/theta/capture.%04d.%s' t1 = time.time() camera = CameraUsb() camera.init() print(camera.get_info()) frame = 1 jpg_path, dng_path = camera.take_picture(0) print(jpg_path, dng_path) camera.download_file(dng_path, dst_template % (frame, 'dng')) frame += 1 jpg_path, dng_path = camera.take_picture(24) print(jpg_path, dng_path) camera.download_file(dng_path, dst_template % (frame, 'dng')) frame += 1 jpg_path, dng_path = camera.take_picture(42) print(jpg_path, dng_path) camera.download_file(dng_path, dst_template % (frame, 'dng')) frame += 1 print('Done in %0.03f sec' % (time.time() - t1, )) if __name__ == \"__main__\": _unittest()","title":"Python bindings"},{"location":"meetup_archive/2020_09_01/","text":"September 1, 2020 - First Linux Meetup 45 people signed up and 17 people attended. Due to the US health situation, the meetup was online only using Zoom Pro. It lasted 20 minutes past the planned 60 minutes due to active questions. In addition to configuration questions for libuvc-theta and libuvc-theta-sample, we showed two Linux streaming demos from a Jetson Nano with real-time processing using Python OpenCV. Slides A copy of the slides used in the meetup are here Community Projects - ROS and OpenCV ROS package for RICOH THETA S RICOH THETA S usage package with Ubuntu 16.04 and ROS Kinetic Robotic Intelligence Lab (RobInLab) ROS package rgbd stereo 360 camera rgbd_stereo_360camera convert fisheye images to equirectangular projection obtain depth image from stereo equirectangular image pair using basic SGBM algorithm to estimate disparity depth information using WLS filter in addition to SGBM algorithm Q and A How do I load v4l2loopback automatically when I reboot? In the file /etc/modules-load.d/modules.conf add a new line v4l2loopback . How do I save video to file with the USB API? If you are using the USB to save video to file (not streaming), this command works with the V and Z1. ptpcam -R 0x101c,0,0,1 Can I use OpenVSLAM? It works with a video from file. We didn't test it in the past with a live stream because we didn't have a way to get the live video onto Linux. Please test it now and report back. Discussion . Demo 3 in the video below is with a RICOH THETA V at 1920x960 with FPS of 10. Video How do I access camera acceleration or change of position? You need to use a plug-in to access camera sensor data. Discussion Related info is in this video for a real-time demo. Article summary in English. GitHub repo here . Integrated with TensorFlow Lite using internal camera OS NDK. There is a related article here that shows camera Yaw and Pitch on the OLED of the Z1. gist Do you have plans to additional deep learning? We may based on additional feedback. A good next step is to assess the VOC-360 image dataset that can be used to train models for 360 images. More information on the model and how to download the dataset is here . TensorFlow demo running inside the camera is here . FDDB-360 contains 17,052 fisheye-looking images and a total of 26,640 annotated faces. Did you use DetectNet on 4K frames? (from Craig) For DetectNet, I was only able to test it at 2K on the Jetson Nano due to limited resources. I believe it works at 4k on Jetson Xavier. But, I don't have a Jetson Xavier due to cost Is motionJPEG higher per frame bitrate? Craig: The MotionJPEG is lower fps and higher latency, but it is convenient. There's a demo video of MotionJPEG here: https://youtu.be/5eSdqEudu5s The code is available for testing. Though MotionJPEG is not recommended for AI processing. However, feel free to give it a try. Can we use other THETAs? or only V? Craig: Z1 works Can the THETA S be used for live streaming? From craig to Everyone: 10:39 AM The V and Z1 are the only models that support live streaming of equirectangular The THETA S streams motionJPEG. It should work with Linux in dual-fisheye the V and Z1 stream in UVC 1.5 in equirectangular From john to Everyone: 10:41 AM yes, it works There is latency of 2-3 seconds with Ubuntu 20.04 John \u2192 Had latency of 2-3 seconds \u2192 Couldn\u2019t text it on 16-18 \u2192 How to reduce the latency, what changes need to be made on Ubuntu 16/18? Craig: I used Ubuntu 20. Discrete GPU, John might be using software rendering (if it cannot use GPU). Craig uses x86, did not use software rendering From john to Everyone: 10:55 AM I saw your description in the link and tested for Ubuntu 20.04, but with latency. From john to Everyone: 10:55 AM But couldn't make it work in Ubuntu 16 and 18. Do you know of what modification is required? From craig to Everyone: 10:55 AM I didn't have latency. did you install the gstreamer full plug-in pack? After meetup, ran this test to show latency. you mention it doesn\u2019t work with Raspberry Pi now. We were wondering the reason. Is it because the OS or the CPU architecture (i.e. arm64) ? From Luke Lu to Everyone: 11:01 AM Thanks for your tutorial. In your forum, you mention it doesn\u2019t work with Raspberry Pi now. We were wondering the reason. Is it because the OS or the CPU architecture (i.e. arm64) ? Craig: Cut down resolution and try again. Craig couldn\u2019t get it to work. Nano has DDR4, Pi3 has DDR3. Hardware acceleration. H.264 acceleration doesn\u2019t do decoding. Cut stream down to 2k and try again. Try Pi4 (newest), 2k, assess to see if it works. Speed of decoding is the limiting factor. Do I need to recompile to change the resolution After the meetup, we ran this test using command line arguments. Using the test, you can specify the resolution with: $ ./gst_loopback --format 4K start, hit any key to stop $ ./gst_loopback --format 2K start, hit any key to stop If the THETA is on /dev/video2 , you can confirm resolution with: $ v4l2-ctl --device /dev/video2 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YU12' (Planar YUV 4:2:0) Size: Discrete 1920x960 Interval: Discrete 0.033s (30.000 fps) Or if in 4K, $ v4l2-ctl --device /dev/video2 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YU12' (Planar YUV 4:2:0) Size: Discrete 3840x1920 Interval: Discrete 0.033s (30.000 fps)","title":"September 1, 2020 - First Linux Meetup"},{"location":"meetup_archive/2020_09_01/#september-1-2020-first-linux-meetup","text":"45 people signed up and 17 people attended. Due to the US health situation, the meetup was online only using Zoom Pro. It lasted 20 minutes past the planned 60 minutes due to active questions. In addition to configuration questions for libuvc-theta and libuvc-theta-sample, we showed two Linux streaming demos from a Jetson Nano with real-time processing using Python OpenCV.","title":"September 1, 2020 - First Linux Meetup"},{"location":"meetup_archive/2020_09_01/#slides","text":"A copy of the slides used in the meetup are here","title":"Slides"},{"location":"meetup_archive/2020_09_01/#community-projects-ros-and-opencv","text":"","title":"Community Projects - ROS and OpenCV"},{"location":"meetup_archive/2020_09_01/#ros-package-for-ricoh-theta-s","text":"RICOH THETA S usage package with Ubuntu 16.04 and ROS Kinetic Robotic Intelligence Lab (RobInLab) ROS package","title":"ROS package for RICOH THETA S"},{"location":"meetup_archive/2020_09_01/#rgbd-stereo-360-camera","text":"rgbd_stereo_360camera convert fisheye images to equirectangular projection obtain depth image from stereo equirectangular image pair using basic SGBM algorithm to estimate disparity depth information using WLS filter in addition to SGBM algorithm","title":"rgbd stereo 360 camera"},{"location":"meetup_archive/2020_09_01/#q-and-a","text":"","title":"Q and A"},{"location":"meetup_archive/2020_09_01/#how-do-i-load-v4l2loopback-automatically-when-i-reboot","text":"In the file /etc/modules-load.d/modules.conf add a new line v4l2loopback .","title":"How do I load v4l2loopback automatically when I reboot?"},{"location":"meetup_archive/2020_09_01/#how-do-i-save-video-to-file-with-the-usb-api","text":"If you are using the USB to save video to file (not streaming), this command works with the V and Z1. ptpcam -R 0x101c,0,0,1","title":"How do I save video to file with the USB API?"},{"location":"meetup_archive/2020_09_01/#can-i-use-openvslam","text":"It works with a video from file. We didn't test it in the past with a live stream because we didn't have a way to get the live video onto Linux. Please test it now and report back. Discussion . Demo 3 in the video below is with a RICOH THETA V at 1920x960 with FPS of 10. Video","title":"Can I use OpenVSLAM?"},{"location":"meetup_archive/2020_09_01/#how-do-i-access-camera-acceleration-or-change-of-position","text":"You need to use a plug-in to access camera sensor data. Discussion Related info is in this video for a real-time demo. Article summary in English. GitHub repo here . Integrated with TensorFlow Lite using internal camera OS NDK. There is a related article here that shows camera Yaw and Pitch on the OLED of the Z1. gist","title":"How do I access camera acceleration or change of position?"},{"location":"meetup_archive/2020_09_01/#do-you-have-plans-to-additional-deep-learning","text":"We may based on additional feedback. A good next step is to assess the VOC-360 image dataset that can be used to train models for 360 images. More information on the model and how to download the dataset is here . TensorFlow demo running inside the camera is here . FDDB-360 contains 17,052 fisheye-looking images and a total of 26,640 annotated faces.","title":"Do you have plans to additional deep learning?"},{"location":"meetup_archive/2020_09_01/#did-you-use-detectnet-on-4k-frames","text":"(from Craig) For DetectNet, I was only able to test it at 2K on the Jetson Nano due to limited resources. I believe it works at 4k on Jetson Xavier. But, I don't have a Jetson Xavier due to cost","title":"Did you use DetectNet on 4K frames?"},{"location":"meetup_archive/2020_09_01/#is-motionjpeg-higher-per-frame-bitrate","text":"Craig: The MotionJPEG is lower fps and higher latency, but it is convenient. There's a demo video of MotionJPEG here: https://youtu.be/5eSdqEudu5s The code is available for testing. Though MotionJPEG is not recommended for AI processing. However, feel free to give it a try.","title":"Is motionJPEG higher per frame bitrate?"},{"location":"meetup_archive/2020_09_01/#can-we-use-other-thetas-or-only-v","text":"Craig: Z1 works","title":"Can we use other THETAs? or only V?"},{"location":"meetup_archive/2020_09_01/#can-the-theta-s-be-used-for-live-streaming","text":"From craig to Everyone: 10:39 AM The V and Z1 are the only models that support live streaming of equirectangular The THETA S streams motionJPEG. It should work with Linux in dual-fisheye the V and Z1 stream in UVC 1.5 in equirectangular From john to Everyone: 10:41 AM yes, it works","title":"Can the THETA S be used for live streaming?"},{"location":"meetup_archive/2020_09_01/#there-is-latency-of-2-3-seconds-with-ubuntu-2004","text":"John \u2192 Had latency of 2-3 seconds \u2192 Couldn\u2019t text it on 16-18 \u2192 How to reduce the latency, what changes need to be made on Ubuntu 16/18? Craig: I used Ubuntu 20. Discrete GPU, John might be using software rendering (if it cannot use GPU). Craig uses x86, did not use software rendering From john to Everyone: 10:55 AM I saw your description in the link and tested for Ubuntu 20.04, but with latency. From john to Everyone: 10:55 AM But couldn't make it work in Ubuntu 16 and 18. Do you know of what modification is required? From craig to Everyone: 10:55 AM I didn't have latency. did you install the gstreamer full plug-in pack? After meetup, ran this test to show latency.","title":"There is latency of 2-3 seconds with Ubuntu 20.04"},{"location":"meetup_archive/2020_09_01/#you-mention-it-doesnt-work-with-raspberry-pi-now-we-were-wondering-the-reason-is-it-because-the-os-or-the-cpu-architecture-ie-arm64","text":"From Luke Lu to Everyone: 11:01 AM Thanks for your tutorial. In your forum, you mention it doesn\u2019t work with Raspberry Pi now. We were wondering the reason. Is it because the OS or the CPU architecture (i.e. arm64) ? Craig: Cut down resolution and try again. Craig couldn\u2019t get it to work. Nano has DDR4, Pi3 has DDR3. Hardware acceleration. H.264 acceleration doesn\u2019t do decoding. Cut stream down to 2k and try again. Try Pi4 (newest), 2k, assess to see if it works. Speed of decoding is the limiting factor.","title":"you mention it doesn\u2019t work with Raspberry Pi now. We were wondering the reason.  Is it because the OS or the CPU architecture (i.e. arm64) ?"},{"location":"meetup_archive/2020_09_01/#do-i-need-to-recompile-to-change-the-resolution","text":"After the meetup, we ran this test using command line arguments. Using the test, you can specify the resolution with: $ ./gst_loopback --format 4K start, hit any key to stop $ ./gst_loopback --format 2K start, hit any key to stop If the THETA is on /dev/video2 , you can confirm resolution with: $ v4l2-ctl --device /dev/video2 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YU12' (Planar YUV 4:2:0) Size: Discrete 1920x960 Interval: Discrete 0.033s (30.000 fps) Or if in 4K, $ v4l2-ctl --device /dev/video2 --list-formats-ext ioctl: VIDIOC_ENUM_FMT Type: Video Capture [0]: 'YU12' (Planar YUV 4:2:0) Size: Discrete 3840x1920 Interval: Discrete 0.033s (30.000 fps)","title":"Do I need to recompile to change the resolution"}]}