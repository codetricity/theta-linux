{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"RICOH THETA Development on Linux","text":""},{"location":"#overview","title":"Overview","text":"<p>Stream 360 video, control the RICOH THETA API, and supply power to the camera using the USB cable.  It's awesome!</p> <p>Video to your Linux computer is 4K at 30fps with under 300ms latency.  Works with the RICOH THETA V or RICOH THETA Z1.  It's perfect for OpenCV object detection, autonomous drones, building analysis, AI, and TensorFlow projects.</p> <p>The camera can be powered from the USB cable and can switch into live streaming mode, still image, video, or bracket shooting using API commands. We explain the tools, the repos, and the  API.</p> <p>Up to date guides, events, and a general pile of great stuff is at the theta360.guide Linux Streaming Site.</p> <p>It's an exciting world.  Let's getting started.</p> <p>If you're eager and using Ubuntu 20.04 or 22.04</p> <p><pre><code>git clone https://github.com/ricohapi/libuvc-theta.git\nsudo apt install libjpeg-dev\ncd libuvc-theta\nmkdir build\ncd build\ncmake ..\nmake\nsudo make install\ncd ../..\ngit clone https://github.com/ricohapi/libuvc-theta-sample.git\ncd libuvc-theta-sample/gst\nmake\n\n# THETA must be plugged into your computer and in \n# live streaming mode\n\n$ ./gst_viewer\n</code></pre> This assumes you have a non-THETA webcam on <code>/dev/video0</code> (your laptop or desktop cam). You may need to edit the source if your THETA is the only camera on your computer.</p> <p>If the build fails, you may need a few gstreamer packages. The command below installs everything.  You do not need everything</p> <pre><code>sudo apt-get install libgstreamer1.0-0 gstreamer1.0-plugins-base gstreamer1.0-plugins-good gstreamer1.0-plugins-bad gstreamer1.0-plugins-ugly gstreamer1.0-libav gstreamer1.0-doc gstreamer1.0-tools gstreamer1.0-x gstreamer1.0-alsa gstreamer1.0-gl gstreamer1.0-gtk3 gstreamer1.0-qt5 gstreamer1.0-pulseaudio libgstreamer-plugins-base1.0-dev\n</code></pre>"},{"location":"camera/","title":"Camera","text":"<p>You need a THETA V, Z1, or X.  These cameras live stream with UVC 1.5 in equirectangular. You can also use a THETA S to stream motionJPEG in dual-fisheye. The SC2 cannot stream over a USB cable.  However, all models, including the SC2, can show a live preview in motionJPEG over WiFi. Refer to ricoh_theta_mjpeg_opencv for a mjpeg example with OpenCV. The THETA X can also display live preview motionJPEG over Ethernet.  I recommend you use the USB cable with the X. The live preview is of lower resolution and framerate.</p> <p>As of January 2024, most people are using the Z1 or the X for streaming over a USB cable</p>"},{"location":"camera/#streaming-specifications","title":"Streaming Specifications","text":"<p>V and Z1 USB streaming.</p> <ul> <li>4K\uff0cH264: 3840\u00d71920/29.97fps/120Mbps</li> <li>2K\uff0cH264: 1920\u00d7960/29.97fps/42Mbps</li> </ul> <p>X USB streaming:</p> <ul> <li>4K 3840\u00d71920/30fps/100Mbps</li> </ul> <p>As of January 2025, there is a notice on the RICOH360 site for the THETA X live streaming:</p> <p>The maximum time for streaming 4K/30 fps live streaming is approx 25 minutes (at an ambient temperature of 25\u00b0C). The firmware update adds 2K mode, which means that the maximum time for 2K/30 fps live streaming can be distributed for even longer periods of time. If the temperature in the camera rises rapidly due to the surrounding environment or shooting conditions, the shooting time will be even shorter.</p> <p>The modes for the sample program are specified in thetauvc.c.</p> <p>You can specify the resolution in gst_viewer.c.  An example of changing the resolution is here.</p>"},{"location":"camera/#long-term-streaming","title":"Long-Term Streaming","text":"<p>With firmware 1.60.1 or newer, the Z1 can stream indefinitely. The battery will charge when streaming at 4K.  To stream indefinitely, you need the proper equipment.  </p> <p>The USB port supplying charge to the RICOH THETA needs to supply approximately 900mA of current.</p> <p>In my tests, most USB 3.0, 3.1, and 3.2 ports on Linux computers did not supply the required electrical current.</p> <p>If your computer does not supply 900mA of charge while streaming data, you will need to use a powered hub with the proper specification.</p> <p>There are different standards for Battery Charging 1.2 for the USB electrical specification. You will need BC 1.2 CDP to provide 1.5A data plus charge.  The THETA Z1 will only consume 0.9A of the 1.5A capacity.</p> <p></p> <p>It's likely that USB Type-C (not USB 3.0 with a USB-C connector) and USB PD can also deliver over 900mA, but I did not test these.  Note that my Asus Zephyrus laptop has USB-C connector ports directly the laptop body, but these physical ports comply to the USB 3.2 specification, not USB-C.  USB 3.2 does not require USB Power Delivery.</p> <p>From the table below, it would appear that USB 3.2 Gen 2 would deliver the required electrical current.  However, I wasn't able to keep the Z1 charged indefinitely at 4K with my ROG Zephyrus G14 GA401.</p> <p></p> <p>Here's the specifications on my laptop.</p> <p></p> <p>After testing streaming on 5 different computers, I could only achieve continuous non-stop streaming on one computer.</p> <p>From our community tests, it appears that the USB 3.0 (or 3.1, 3.2) port needs to support Battery Charging 1.2 or equivalent.  The Z1 consumes 0.6Amp to 0.9Amps while streaming 4K.  Without a powered hub, the Raspberry Pi and Jetson Nano cannot stream indefinitely.</p> <p>I tested an old Anker 7-Port USB 3.0 hub that I had on my desk with a Jetson Nano. The Z1 battery charge increased while it was streaming at 4K.  The Z1 camera body remained comfortably warm when I touched it, not hot.</p> <p>In the test setup below, the Anker USB hub is plugged into a power strip. The hub is above my keyboard.</p> <p></p> <p>The hub that I am using only has one port that is BC 1.2 compliant.  I needed to use the BC 1.2 port. </p> <p></p> <p>If I plugged the Z1 into the other 6 ports, the camera would only consume 0.440 Amps. This is the same electrical current consumption that I saw on the USB 3.1 and 3.2 ports of the computers I tested.  With the Z1 drawing 0.440 Amps, the battery will drain during streaming.</p> <p>Using the BC 1.2 port, the camera draws up to 0.8 Amps to 0.9 Amps while charging and streaming.</p> <p></p> <p>After the camera passes 90 percent charge, the charging rate appears to slow down. After a long 4K streaming session on the Nano, the Z1 is staying happy at 95 percent charge.</p> <p></p>"},{"location":"camera/#long-term-streaming-platform-tests","title":"Long-term Streaming Platform Tests","text":"Platform Result Acer Predator 300 laptop with onboard USB 3.1 ports success. battery charged while streaming Jetson Nano with external powered USB hub with BC 1.2 success. battery charged. Jetson Nano using onboard USB 3 ports fail. battery drained. Desktop computer with Intel X99 Wellsburg motherboard and USB 3.1 ports fail. battery drained Asus Zephyrus laptop with USB 3.2 ports fail. battery drained desktop computer with Intel B85 motherboard and USB 3.0 ports fail. battery drained"},{"location":"camera/#sleep-and-power-overview","title":"Sleep and Power Overview","text":"<p>The camera has three power states:</p> <ol> <li>Power off</li> <li>Power on</li> <li>Sleep</li> </ol> <p>You can use the THETA API over a USB cable to control power off, sleep, and awake. To turn on the camera from power off, you can use a C library to power cycle the USB ports on a small board computer such as a Jetson Nano. This is an unofficial community workaround and is not supported by RICOH.</p> Function Method Sleep USB API Awake USB API Power Off USB API Power On workaround with C library"},{"location":"camera/#sleep-and-wake","title":"Sleep and Wake","text":"<p>If your application can provide power to the THETA or if the THETA does not have to be dormant for a long time, it is better to use sleep and awake. You can use the USB API to control sleep and awake. </p> <p>We used the following process to test sleep and awake:</p> <ol> <li>Disable auto power off delay and disable auto sleep - you only need to do this once. The camera will save the setting.</li> <li>Put camera to sleep</li> <li>Wake camera from sleep</li> <li>Check if camera status is ready. If the camera is still asleep, send the wake camera again.</li> </ol> <p>If you do not have any problems waking the camera up from sleep on the first attempt, you may not need step 4.</p>"},{"location":"camera/#power-off-and-power-on","title":"Power Off and Power On","text":"<p>If you require power off and power on for applications such as placing the THETA on  a robot, then shipping the robot to another site, you can use a Nano  or Raspberry Pi to power on the THETA.</p>"},{"location":"camera/#turn-camera-off","title":"Turn Camera Off","text":"<p>The camera can be turned off with the USB API.</p> <pre><code>$ ptpcam -R 0x1013\n</code></pre>"},{"location":"camera/#turn-camera-on-using-jetson-nano","title":"Turn Camera On Using Jetson Nano","text":"<p>There is no official way to turn on the camera once it is in a power off state. We recommend that you use sleep and awake.  If this is not an option, you can turn the camera on by power cycling the USB ports of the Jetson Nano.</p> <p>This example uses libusb.</p>"},{"location":"camera/#sample-code-for-jetson-nano","title":"Sample Code for Jetson Nano","text":"<pre><code>/**************** \n* Tested on Jetson Nano running JetPack 4.4\n* The RICOH THETA V and Z1 will turn on from a power off state when \n* the USB cable is plugged into a port. This example  will power cycle\n* the USB ports of the Nano.  You must have libusb-1.0-0-dev installed\n* on the Nano and link to it.\n* \n* Additional information on using libusb_control_transfer is below. \n* https://www.cs.unm.edu/~hjelmn/libusb_hotplug_api/group__syncio.html\n*****************/\n\n#include &lt;stdio.h&gt;\n#include &lt;string.h&gt;\n#include &lt;stdlib.h&gt;\n#include &lt;stdint.h&gt;\n#include &lt;unistd.h&gt;\n#include &lt;libusb-1.0/libusb.h&gt;\n\nunsigned        vid = 0x0bda;\nunsigned        pid = 0x5411;\n\n\nint power_cycle(libusb_device_handle *hub_devh)\n{\n        int ret = -1;\n\n        /*ep0 vendor command enable*/\n        ret = libusb_control_transfer(hub_devh, 0x40, 0x02, 0x01, ((0x0B&lt;&lt;8)|(0xDA)), 0, 0, 100000);\n        if (ret &lt; 0) {\n                printf(\"[error]:ep0 vendor command enable fail.\\n\");\n                return ret;\n        }\n\n        /*ep0 vendor command disable*/\n        libusb_control_transfer(hub_devh, 0x40, 0x1, 0x08, 0, NULL, 0, 100);\n        libusb_control_transfer(hub_devh, 0x40, 0x3, 0x08, 0, NULL, 0, 100);\n        libusb_control_transfer(hub_devh, 0x40, 0x02, 0x00, ((0x0B&lt;&lt;8)|(0xDA)), 0, 0, 100000);\n\n        return ret;\n}\n\nint main(int argc, char *argv[])\n{\n    int ret=0;\n    libusb_device_handle    *hub_devh;\n    libusb_context      *context;\n\n    ret = libusb_init(&amp;context);\n    if (ret != 0){\n        printf(\"[error]:libusb init fail.\\n\");\n        return ret;\n    }\n\n    hub_devh = libusb_open_device_with_vid_pid(context, vid, pid);\n    if (!hub_devh) {\n        printf(\"[error]:open device %04x:%04x fail.\\n\", vid, pid);\n        return -1;\n    }\n    ret = power_cycle(hub_devh);\n    return ret;\n}\n</code></pre> <p>You can test the code by first saving the code in a file called,  <code>reset_jetson_usb_power.c</code> then follow these steps:</p> <pre><code>$ sudo apt-get install libusb-1.0-0-dev\n$ gcc -o power_cycle reset_jetson_usb_power.c -lusb-1.0\n$ sudo ./power_cycle\n</code></pre>"},{"location":"camera/#turn-camera-on-using-raspberry-pi","title":"Turn Camera On Using Raspberry Pi","text":""},{"location":"camera/#install-libusb-dev","title":"install libusb-dev","text":"<p><code>sudo apt-get install libusb-dev</code></p>"},{"location":"camera/#install-hub-ctrlc","title":"install hub-ctrl.c","text":"<p><code>git clone https://github.com/codetricity/hub-ctrl.c</code></p> <ul> <li>make and then install in <code>/usr/local/bin</code></li> </ul>"},{"location":"camera/#create-shell-script-to-cycle-power","title":"create shell script to cycle power","text":"<p>Save the following into <code>/usr/local/sbin/cycle-power.sh</code>.</p> <pre><code>#!/bin/bash\n/usr/local/bin/hub-ctrl -h 0 -P 2 -p 0 \nsleep 2\n/usr/local/bin/hub-ctrl -h 0 -P 2 -p 1\n</code></pre>"},{"location":"camera/#optional-enable-script-to-run-without-sudo-password","title":"Optional - Enable script to run without sudo password","text":"<p>This is a potential security risk.  If you want to avoid having to enter a password and are comfortable with the risk, then follow the steps in this article.</p>"},{"location":"camera/#additional-tips-for-raspberry-pi-4","title":"Additional Tips for Raspberry Pi 4","text":"<p>Thanks to Shun Yamashita of fulldepth for this solution to cycle the power on the Raspberry Pi 4 USB ports, which has the effect of turning the THETA Z1 on when it is plugged in with a USB cable. </p> <p>Shun reported that with the Raspberry Pi 4, the script above did not work.  He wrote the script below for the RPi 4 which does work.</p> <pre><code>#!/bin/bash\n\nptpcam -R 0x1013\n\n/usr/local/bin/hub-ctrl -h 0 -P 1 -p 0\n/usr/local/bin/hub-ctrl -h 0 -P 2 -p 0\n/usr/local/bin/hub-ctrl -h 0 -P 3 -p 0\n/usr/local/bin/hub-ctrl -h 0 -P 4 -p 0\n/usr/local/bin/hub-ctrl -h 1 -P 1 -p 0\n/usr/local/bin/hub-ctrl -h 1 -P 2 -p 0\n/usr/local/bin/hub-ctrl -h 1 -P 3 -p 0\n/usr/local/bin/hub-ctrl -h 1 -P 4 -p 0\n/usr/local/bin/hub-ctrl -v\n\nsleep 5\n\n/usr/local/bin/hub-ctrl -h 0 -P 1 -p 1\n/usr/local/bin/hub-ctrl -h 0 -P 2 -p 1\n/usr/local/bin/hub-ctrl -h 0 -P 3 -p 1\n/usr/local/bin/hub-ctrl -h 0 -P 4 -p 1\n/usr/local/bin/hub-ctrl -h 1 -P 1 -p 1\n/usr/local/bin/hub-ctrl -h 1 -P 2 -p 1\n/usr/local/bin/hub-ctrl -h 1 -P 3 -p 1\n/usr/local/bin/hub-ctrl -h 1 -P 4 -p 1\n/usr/local/bin/hub-ctrl -v\n</code></pre>"},{"location":"demos/","title":"Demos","text":""},{"location":"demos/#ongoing-tests-with-linux-streaming","title":"Ongoing Tests with Linux Streaming","text":"<p>Using Nvidia Jetson Nano live streaming from a THETA V. Processing done with Python3, OpenCV 4.4. Scroll down for code.</p>"},{"location":"demos/#detectnet","title":"DetectNet","text":"<p>Running live on Jetson Nano with RICOH THETA Z1.</p> <p>DetectNet  applied to both single frame with SSD Mobilenet-v2 to assess accuracy and to live stream to assess framerate.  Works  good on both.</p> <p>Video demo with Jetson Nano.</p> <p>See Jetson Nano inference benchmarks.</p> <p>Code is available in the at  https://github.com/dusty-nv/jetson-inference</p> <p>There is super small text in the green box that says, \"person\".  The system accurately detected the only person in the image.</p> <p>It is 88.6 percent confident that I am a person.  Nice.</p> <p></p> <p>Despite the distorted view of my feet, the program does detect  the human form.</p> <p></p> <p>Even at night, in low-light conditions with me on the  side of the shutter button, the program did detect me.</p> <p></p> <p>However, there were many frames where I was not detected.</p> <p>To proceed, you will likely need a database of fisheye or  equirectangular images to build your own model. </p>"},{"location":"demos/#sample-code","title":"Sample Code","text":"<pre><code>import jetson.inference\nimport jetson.utils\n\nnet = jetson.inference.detectNet(\"ssd-mobilenet-v2\", threshold=0.5)\ncamera = jetson.utils.gstCamera(1280, 720, \"/dev/video0\")\ndisplay = jetson.utils.glDisplay()\n\nwhile display.IsOpen():\n    img, width, height = camera.CaptureRGBA()\n    detections = net.Detect(img, width, height)\n    display.RenderOnce(img, width, height)\n    display.SetTitle(\"RICOH THETA Detection | Network {:.0f} FPS\".format(net.GetNetworkFPS()))\n</code></pre>"},{"location":"demos/#opencv-python","title":"OpenCV Python","text":"<p>Works on live stream.  </p> <p></p>"},{"location":"demos/#procedure","title":"Procedure","text":"<ul> <li>install libuvc-theta</li> <li>install libuv-theta-sample</li> <li>install v4l2loopback</li> <li>load kernel modules for v4l2loopback and verify that /dev/video0 or equivalent shows THETA stream</li> <li>run Python script with cv2</li> </ul> <p>Recommend you recompile OpenCV 4.4 from source code. May take 2.5 hours if you compile on the Nano.</p>"},{"location":"demos/#simple-python-cv2-test","title":"Simple Python cv2 Test","text":"<p>Frame resize test.</p> <pre><code>import cv2\n\ncap = cv2.VideoCapture(0)\n\n# Check if the webcam is opened correctly\nif not cap.isOpened():\n    raise IOError(\"Cannot open webcam\")\n\nwhile True:\n    ret, frame = cap.read()\n    frame = cv2.resize(frame, None, fx=0.25, fy=0.25, interpolation=cv2.INTER_AREA)\n    cv2.imshow('Input', frame)\n\n    c = cv2.waitKey(1)\n    if c == 27:\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"demos/#build-opencv","title":"Build OpenCV","text":"<p>One script to install OpenCV 4.3 is from AastaNV here.</p> <p>The script I used is from mdegans here</p>"},{"location":"demos/#canny-edge-detection-test","title":"Canny Edge Detection Test","text":"<ul> <li>Code for OpenCV Demo with Canny from RICOH THETA V.  This is the edge detection demo with the white lines on black background. </li> <li>video demo</li> </ul> <pre><code>import sys\nimport argparse\nimport cv2\nimport numpy as np\n\ndef parse_cli_args():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--video_device\", dest=\"video_device\",\n                        help=\"Video device # of USB webcam (/dev/video?) [0]\",\n                        default=0, type=int)\n    arguments = parser.parse_args()\n    return arguments\n\n# On versions of L4T previous to L4T 28.1, flip-method=2\n# Use the Jetson onboard camera\ndef open_onboard_camera():\n    return cv2.VideoCapture(0)\n\n# Open an external usb camera /dev/videoX\ndef open_camera_device(device_number):\n    return cv2.VideoCapture(device_number)\n\n\ndef read_cam(video_capture):\n    if video_capture.isOpened():\n        windowName = \"main_canny\"\n        cv2.namedWindow(windowName, cv2.WINDOW_NORMAL)\n        cv2.resizeWindow(windowName,1280,720)\n        cv2.moveWindow(windowName,0,0)\n        cv2.setWindowTitle(windowName,\"RICOH THETA OpenCV Python Demo\")\n        showWindow=3  # Show all stages\n        showHelp = True\n        font = cv2.FONT_HERSHEY_PLAIN\n        helpText=\"'Esc' to Quit, '1' for Camera Feed, '2' for Canny Detection, '3' for All Stages. '4' to hide help\"\n        edgeThreshold=40\n        showFullScreen = False\n        while True:\n            if cv2.getWindowProperty(windowName, 0) &lt; 0: # Check to see if the user closed the window\n                # This will fail if the user closed the window; Nasties get printed to the console\n                break;\n            ret_val, frame = video_capture.read();\n            hsv=cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n            blur=cv2.GaussianBlur(hsv,(7,7),1.5)\n            edges=cv2.Canny(blur,0,edgeThreshold)\n            if showWindow == 3:  # Need to show the 4 stages\n                # Composite the 2x2 window\n                # Feed from the camera is RGB, the others gray\n                # To composite, convert gray images to color. \n                # All images must be of the same type to display in a window\n                frameRs=cv2.resize(frame, (640,360))\n                hsvRs=cv2.resize(hsv,(640,360))\n                vidBuf = np.concatenate((frameRs, cv2.cvtColor(hsvRs,cv2.COLOR_GRAY2BGR)), axis=1)\n                blurRs=cv2.resize(blur,(640,360))\n                edgesRs=cv2.resize(edges,(640,360))\n                vidBuf1 = np.concatenate( (cv2.cvtColor(blurRs,cv2.COLOR_GRAY2BGR),cv2.cvtColor(edgesRs,cv2.COLOR_GRAY2BGR)), axis=1)\n                vidBuf = np.concatenate( (vidBuf, vidBuf1), axis=0)\n\n            if showWindow==1: # Show Camera Frame\n                displayBuf = frame \n            elif showWindow == 2: # Show Canny Edge Detection\n                displayBuf = edges\n            elif showWindow == 3: # Show All Stages\n                displayBuf = vidBuf\n\n            if showHelp == True:\n                cv2.putText(displayBuf, helpText, (11,20), font, 1.0, (32,32,32), 4, cv2.LINE_AA)\n                cv2.putText(displayBuf, helpText, (10,20), font, 1.0, (240,240,240), 1, cv2.LINE_AA)\n            cv2.imshow(windowName,displayBuf)\n            key=cv2.waitKey(10)\n            if key == 27: # Check for ESC key\n                cv2.destroyAllWindows()\n                break ;\n            elif key==49: # 1 key, show frame\n                cv2.setWindowTitle(windowName,\"Camera Feed\")\n                showWindow=1\n            elif key==50: # 2 key, show Canny\n                cv2.setWindowTitle(windowName,\"Canny Edge Detection\")\n                showWindow=2\n            elif key==51: # 3 key, show Stages\n                cv2.setWindowTitle(windowName,\"Camera, Gray scale, Gaussian Blur, Canny Edge Detection\")\n                showWindow=3\n            elif key==52: # 4 key, toggle help\n                showHelp = not showHelp\n            elif key==44: # , lower canny edge threshold\n                edgeThreshold=max(0,edgeThreshold-1)\n                print ('Canny Edge Threshold Maximum: ',edgeThreshold)\n            elif key==46: # , raise canny edge threshold\n                edgeThreshold=edgeThreshold+1\n                print ('Canny Edge Threshold Maximum: ', edgeThreshold)\n            elif key==74: # Toggle fullscreen; This is the F3 key on this particular keyboard\n                # Toggle full screen mode\n                if showFullScreen == False : \n                    cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_FULLSCREEN)\n                else:\n                    cv2.setWindowProperty(windowName, cv2.WND_PROP_FULLSCREEN, cv2.WINDOW_NORMAL) \n                showFullScreen = not showFullScreen\n\n    else:\n     print (\"camera open failed\")\n\n\n\nif __name__ == '__main__':\n    arguments = parse_cli_args()\n    print(\"Called with args:\")\n    print(arguments)\n    print(\"OpenCV version: {}\".format(cv2.__version__))\n    print(\"Device Number:\",arguments.video_device)\n    if arguments.video_device==0:\n      video_capture=open_onboard_camera()\n    else:\n      video_capture=open_camera_device(arguments.video_device)\n    read_cam(video_capture)\n    video_capture.release()\n    cv2.destroyAllWindows()\n</code></pre>"},{"location":"demos/#openpose","title":"OpenPose","text":"<p>Works on live stream with Jetpack 4.3, not 4.4.</p> <p></p>"},{"location":"equipment/","title":"Equipment","text":""},{"location":"equipment/#hardware-requirements-for-linux-and-the-ricoh-theta","title":"Hardware Requirements for Linux and the RICOH THETA","text":""},{"location":"equipment/#jetson-nano-reference-platform","title":"Jetson Nano - Reference Platform","text":"<p>Our reference platform is the NVIDIA Jetson Nano, ref.  We are using B01, but A02 should also work. </p> <p>running JetPack 4.4, which is Ubuntu 18.04. The Nano is an ARM A57 with a 128-core Maxwell GPU, 4GB 64-bit LPDDR4.</p> <p>The nano is powered by a 5V 4A barrel connector, not the microUSB which is 5V 2A.  Our Nano has an external fan on the PWM header and a 64GB microSD card.</p>"},{"location":"equipment/#parts","title":"Parts","text":"<ul> <li>NVIDIA Jetson Nano Developer Kit B01</li> <li>SMAKIN DC 5V/4A power supply with barrel connector</li> <li>Waveshare 5V PWM fan - cheaper option - we used this one as we are frugal.  It worked. </li> <li>Noctua 5V PWM fan - better option, around $15 - most people use this one.</li> <li>For Z1 streaming 10' USB-C live streaming cable - it works for me, but it is over the recommended length.  I only have the long cable for convenience. You should use as short a cable as possible. </li> </ul>"},{"location":"equipment/#nvidia-jetson-xavier","title":"NVIDIA Jetson Xavier","text":"<p>The Xavier is better for testing.  However, it is more expensive.  If your  budget permits, it is better to get the Xavier.  You may have problems with 4K AI processing with the Nano.</p> <p>On Jetson Xavier, auto plugin selection of the gstreamer seems to be not working well, replacing \"decodebin ! autovideosink sync=false\" to \"nvv4l2decoder ! nv3dsink sync=false\" will solve the problem. Edit this  line in the sample code and recompile.</p>"},{"location":"equipment/#x86-linux","title":"x86 Linux","text":"<p>We've also tested the libuvc-theta (streaming) and libuvc-theta-sample (streaming sample application) on x86 64bit Linux using Ubuntu 20.04 LTS, Focal Fossa.</p> <p>Watch this build video walkthrough.</p> <p>A video showing latency on x86 is here.</p> <p></p> <p></p> <p>We've tested v4l2loopback with gst_loopback on a low-end Pentium x86 computer.  It works fine.  Thanks to commuity member Yu You for this fix to gst_view.c.  Note the addition of <code>qos=false</code> to the pipeline.  This is currently on  line 190.</p> <pre><code>if (strcmp(cmd_name, \"gst_loopback\") == 0)\n    pipe_proc = \"decodebin ! autovideoconvert ! \"\n        \"video/x-raw,format=I420 ! identity drop-allocation=true !\"\n        \"v4l2sink device=/dev/video0 qos=false sync=false\";\n</code></pre> <p>Screenshot of loopback running on <code>/dev/video0</code>, tested with vlc.</p> <p></p>"},{"location":"equipment/#addtional-x86-information","title":"Addtional x86 Information","text":"<p>If you're having problems after making the modification described above, you can try to temporarily disable your dedicated graphics card on x86. In our tests on Ubuntu 20, Focal Fossa, the gstreamer vaapi plugin can't use the hardware decoder on the NVIDIA GPU with the proprietary NVIDIA driver. The causes gstreamer to use a software decoder instead.</p> <p>This will likely cause many frame drops on your system. You can verify this by setting the GST_DEBUG environment variable to 2 or 3 and then running gst_loopback. You will likely see many frame drop messages.</p> <p>There are two possible workarounds:</p> <ol> <li>Use the nvdec plugin Although the nvdec plugin is a part of the gstreamer-plugins-bad, it is not included in binary distribution due to license problem. Thus, you have to build the plugin by yourself. You also need to modify the pipeline of the gst_loopback accordingly.</li> <li>Use hardware decoder on the iGPU You may need additional setup to run X server on the iGPU,</li> </ol> <p>You can try the X.Org driver. </p> <p></p> <p>This is a video test clip of a THETA Z1 running with the X.Org driver on Intel i7-6800K CPU and NVIDIA GeForce GTX 950 GPU.</p> <p>You can check the graphics driver with one of these commands.</p> <pre><code>$ glxinfo -B\n</code></pre> <p>or </p> <pre><code>$ sudo lshw -c video\n</code></pre>"},{"location":"equipment/#raspberry-pi","title":"Raspberry Pi","text":"<p>The Raspberry Pi will work great with the USB API.  However, you  will not have a good experience streaming 4K, even with the  Raspberry Pi 4.</p> <p>The Raspberry Pi's H.264 hardware decoder does not support 4K resolution even on the Raspberry Pi4. In addition, older Pis' (Pi to Pi3) memory bandwidth(32bit DDR2) is too poor to handle even FHD stream from THETA V/Z1.</p>"},{"location":"equipment/#heat-and-cooling-of-linux-computer","title":"Heat and Cooling of Linux Computer","text":"<p>You need to cool the Nano.  Without a fan, you may get thermal  throttling when live streaming with AI processing. </p> <p></p> <p></p> <p>The fan is 5V pwm.  I've also used a 12V fan before I ordered the 5V fan from Amazon.</p>"},{"location":"equipment/#hardware-acceleration","title":"Hardware Acceleration","text":"<p>You will need to use hardware acceleration to get reasonable performance.</p> <p>To verify that you are using GPU acceleration, you can use <code>tegrastats</code> on Jetson and <code>nvidia-smi</code> on x86. </p>"},{"location":"equipment/#jetson","title":"Jetson","text":"<p>You can either use tegrastats or jetson-stats to see information on your CPU and GPU to identify performance bottlenecks.</p>"},{"location":"equipment/#tegrastats","title":"tegrastats","text":"<p>On NVIDIA Jetson, <code>tegrastats</code> is useful for seeing information on the GPU. In the example below, I've inserted line breaks to make the output easier to read. The output is shown before streaming starts.</p> <pre><code>craig@jetson:~$ tegrastats \nRAM 1122/3964MB (lfb 28x4MB)\nSWAP 211/1982MB (cached 20MB)\nCPU [5%@102,9%@102,0%@102,0%@102]\nEMC_FREQ 0% \nGR3D_FREQ 0% \nPLL@25.5C CPU@27C PMIC@100C \nGPU@27.5C AO@35C thermal@27.5C \nPOM_5V_IN 1805/1805 \nPOM_5V_GPU 0/0 \nPOM_5V_CPU 123/123\n</code></pre> <p>Let's start the stream and review it again.</p> <pre><code>RAM 1288/3964MB (lfb 28x4MB) \nSWAP 210/1982MB (cached 20MB) \nCPU [100%@1479,89%@1479,85%@1479,86%@1479] \nEMC_FREQ 0% \nGR3D_FREQ 35% \nPLL@32C CPU@35C PMIC@100C \nGPU@30.5C AO@40.5C thermal@32.25C \nPOM_5V_IN 5607/5561 \nPOM_5V_GPU 118/98 \nPOM_5V_CPU 2843/2791\n</code></pre> <p>The GR3D_FREQ and POM_5V_GPU provide information on the GPU.  GR3D is the Jetson GPU engine.  More information on tegrastatus is  here.</p>"},{"location":"equipment/#jetson-stats","title":"jetson-stats","text":"<p>Another nice package is jetson-stats.</p> <p>You can verify if your base libraries such as OpenCV have features such as CUDA enabled.</p> <p></p> <p>Prior to streaming, your system should show very little load. </p> <p></p> <p>Once streaming starts, the load on your GPU should increase.  The example below shows OpenCV and a Python script for canny edge detection.</p> <p></p> <p>The example below is using OpenCV to convert the color space.</p> <p></p>"},{"location":"equipment/#x86","title":"x86","text":""},{"location":"equipment/#gstreamer-plug-in","title":"gstreamer plug-in","text":"<p>You can check to see if the nvdec plug-in is installed with:</p> <pre><code>$ gst-inspect-1.0 nvdec\n</code></pre> <p>If you see this, the plug-in is not installed.</p> <pre><code>No such element or plugin 'nvdec'\n</code></pre> <p>If nvdec and nvenc are installed, you should see this:</p> <pre><code>$ gst-inspect-1.0 | grep nvenc\nnvenc:  nvh264enc: NVENC H.264 Video Encoder\n$ gst-inspect-1.0 | grep nvdec\nnvdec:  nvdec: NVDEC video decoder\n</code></pre> <p>There are several online tutorials for installing nvdec and nvenc.</p> <ul> <li>LifeStyle transfer: How to install Nvidia Gstreamer plugins (nvenc, nvdec) on Ubuntu? by Taras Lishchenko</li> <li>README from gst-plugins-bad/sys/nvenc</li> <li>gist from corenel</li> </ul> <p>plugin build example</p> <pre><code>$ NVENCODE_CFLAGS=\"-I/home/craig/Development/gstreamer/gst-plugins-bad/sys/nvenc\" ./autogen.sh --disable-gtk-doc --with-cuda-prefix=\"/usr/local/cuda\"\n</code></pre> <p></p> <p></p>"},{"location":"equipment/#monitoring-tools","title":"Monitoring Tools","text":"<p>Prior to starting the stream.</p> <pre><code>$ nvidia-smi \nMon Sep 14 06:14:55 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 440.100      Driver Version: 440.100      CUDA Version: 10.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 950     Off  | 00000000:02:00.0  On |                  N/A |\n|  1%   52C    P5    14W /  99W |    355MiB /  1999MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|    0      1051      G   /usr/lib/xorg/Xorg                            52MiB |\n|    0      1619      G   /usr/lib/xorg/Xorg                           113MiB |\n|    0      1820      G   /usr/bin/gnome-shell                         102MiB |\n|    0      2822      G   ...AAAAAAAAAAAACAAAAAAAAAA= --shared-files    73MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <p>With gst-viewer running.</p> <pre><code>$ nvidia-smi \nTue Sep 29 16:29:33 2020       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce GTX 950     Off  | 00000000:02:00.0  On |                  N/A |\n|  9%   56C    P0    28W /  99W |    543MiB /  1999MiB |      9%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1100      G   /usr/lib/xorg/Xorg                 51MiB |\n|    0   N/A  N/A      1588      G   /usr/lib/xorg/Xorg                188MiB |\n|    0   N/A  N/A      1719      G   /usr/bin/gnome-shell              144MiB |\n|    0   N/A  N/A      2132      G   ...AAAAAAAAA= --shared-files       71MiB |\n|    0   N/A  N/A      6606      G   ...AAAAAAAAA= --shared-files       73MiB |\n+-----------------------------------------------------------------------------+\n</code></pre> <p></p>"},{"location":"examples/","title":"Usage Examples","text":""},{"location":"examples/#stream-to-youtube-with-ffmpeg","title":"stream to YouTube with ffmpeg","text":"<p>from Paul Gullett. post</p> <p><pre><code>ffmpeg -f lavfi -i anullsrc \\\n-f v4l2 -s 3480x1920 -r 10 -i /dev/video0 \\\n-vcodec libx264 -pix_fmt yuv420p -preset ultrafast \\\n-strict experimental -r 25 -g 20 -b:v 2500k \\\n-codec:a libmp3lame -ar 44100 -b:a 11025 -bufsize 512k \\\n-f flv rtmp://a.rtmp.youtube.com/live2/secret-key\n</code></pre> As my knowledge of ffmpeg is weak, I simplified Paul's video pipeline.</p> <pre><code> ffmpeg -f lavfi -i anullsrc -f v4l2 -s 1920x960 -r 10 -i /dev/video2 \\\n-vcodec libx264 -pix_fmt yuv420p \\\n -b:v 2500k \\\n-codec:a libmp3lame -ar 44100 -b:a 11025 -bufsize 512k \\\n-f flv rtmp://a.rtmp.youtube.com/live2/$SECRET_KEY\n</code></pre> <p></p>"},{"location":"examples/#stream-to-another-computer-with-gstreamer","title":"stream to another computer with gstreamer","text":"<p>by zdydek.  post</p> <p>in gst_viewer.c</p> <pre><code>pipe_proc = \" rtph264pay name=pay0 pt=96 ! udpsink host=127.0.0.1 port=5000 sync=false \";\n</code></pre> <p>with gst-rtsp-server</p> <pre><code>./test-launch \"( udpsrc port=5000 ! application/x-rtp, media=(string)video, clock-rate=(int)90000, encoding-name=(string)H264 ! rtph264depay ! h264parse ! rtph264pay name=pay0 pt=96 )\"\n</code></pre> <p>Receive on ROS.</p> <pre><code>GSCAM_CONFIG=\"rtspsrc location=rtspt://10.0.16.1:8554/test latency=400 drop-on-latency=true ! application/x-rtp, encoding-name=H264 ! rtph264depay ! decodebin ! queue ! videoconvert\"  roslaunch gscam_nodelet.launch\n</code></pre>"},{"location":"examples/#simplified-computer-to-computer-streaming-with-rtsp-and-gstreamer","title":"Simplified computer to computer streaming with rtsp and gstreamer","text":"<p>This was tested going from an x86 machine to a Jetson Nano. The THETA Z1 is connected to the x86 Linux machine.  It is not  working with the Jetson as the sender.</p> <p>On x86 computer sending THETA video.</p> <p>Modify the pipeline in <code>gst_viewer.c</code></p> <p>This example has the IP address hardcoded in.  Switch to a variable in your code.</p> <pre><code>pipe_proc = \" decodebin ! jpegenc ! rtpjpegpay ! udpsink host=192.168.2.100 port=5000 qos=false sync=false\";\n</code></pre> <p></p> <p>If you are looking for the IP address of the receiver, you can use arp-scan on  the command line.</p> <p>Example:</p> <pre><code>sudo arp-scan --interface=eth0 --localnet\n</code></pre> <p>On the receiving device, if the receiver is a NVIDIA Jetson Nano.</p> <p><pre><code>$ cat receive_udp.sh \ngst-launch-1.0 udpsrc port=5000 !  application/x-rtp,encoding-name=JPEG,payload=26 ! rtpjpegdepay ! jpegdec ! videoscale ! video/x-raw,width=640,height=320 ! nveglglessink\n</code></pre> If you're on x86, change nveglglessink to autovideosink.  You may want to make the width and height bigger as well. </p> <p></p>"},{"location":"examples/#save-to-file","title":"Save to File","text":"<p>by Les Wu aka snafu666.  post</p> <p>Using the v4l2loopback capability and thetaV loopback example, here are 2 example gstreamer pipelines to grab the video:</p> <p>As a lossless huffman encoded raw file:</p> <pre><code>gst-launch-1.0 v4l2src device=/dev/video99 ! video/x-raw,framerate=30/1 \\\n! videoconvert \\\n! videoscale \\\n! avenc_huffyuv \\\n! avimux \\\n! filesink location=raw.hfyu\n</code></pre> <p>And with default h.264 encoding on a Jetson:</p> <pre><code>gst-launch-1.0 v4l2src device=/dev/video99 ! video/x-raw,framerate=30/1 \\\n! nvvidconv \\\n! omxh264enc \\\n! h264parse ! matroskamux \\\n! filesink location=vid99.mkv\n</code></pre> <p>Pro tip, when you install v4l2loopback, use the video_nr option to create the video device somewhere high so it does not get displaced by PnP of other cameras.</p> <p>The Huffyuv format is a large file format.  VLC can play it.</p> <p>Here's a shot of me playing a file that I generated with Les's pipeline.</p> <p></p> <p>On x86, this is the pipeline I used to save to a H.264 file.</p> <pre><code>$ gst-launch-1.0 v4l2src device=/dev/video2 ! video/x-raw,framerate=30/1 ! autovideoconvert ! nvh264enc ! h264parse ! matroskamux ! filesink location=vid_test.mkv\n</code></pre> <p>Example of playing file with gst-launch.</p> <pre><code>gst-launch-1.0 playbin uri=file:///path-to-file/vid_test.mkv\n</code></pre> <p></p> <p>Community member Nakamura_Lab indicated that the he experienced significant frame loss when using lossless Huffman to save to file with the Xavier NX. He could save to file with H.264.  However for his use case with multiple face detection, he needed higher resolution than provided by H.264.</p> <p>He ended up using H.265 encoding to save to file, which provided both high quality and no frame loss.</p> <pre><code>gst-launch-1.0 v4l2src num-buffers=600 \\\ndevice=/dev/video0 \\\n! video/x-raw \\\n! nvvidconv \\\n! nvv4l2h265enc \\\n! h265parse \\\n! qtmux \\\n! filesink location=test.mp4 -e\n</code></pre>"},{"location":"examples/#stream-from-raspberry-pi-4-to-a-windows-pc","title":"Stream From Raspberry Pi 4 to a Windows PC","text":"<p>Thanks to Shun Yamashita of fulldepth for this solution to stream the Z1 video to a Raspberry Pi 4 with USB then restream it to a Windows PC.</p> <p>This is the process:</p> <ul> <li>Use GStreamer to stream UDP(RTP) to the Windows PC</li> <li>Do not use H264 decoding on the Raspberry Pi as the Windows machine is handling it and it's likely that the RPi4 can't use hardware decoding for 4K H.264.</li> <li>Tested with Raspberry Pi4 modelB with 4GB of RAM running Raspberry Pi OS</li> </ul> <p>On the Raspberry Pi, the following was changed in the GStreamer pipeline in <code>gst/gst_viewer.c</code>.</p> <pre><code>src.pipeline = gst_parse_launch(\n        \" appsrc name=ap ! queue ! h264parse ! queue\"\n        \" ! rtph264pay ! udpsink host=192.168.0.15 port=9000\",\n        NULL);\n</code></pre>"},{"location":"examples/#rviz-in-ros2-dual-camera-setup-with-opencv","title":"RViz in ROS2 Dual-Camera Setup with OpenCV","text":"<p>Robot displays live images from two THETA V cameras on RViz in ROS2 using the following pipeline and OpenCV.</p> <p>credit: H. Usuba from Kufusha - Robotics development</p> <p>original discussion</p> <pre><code>thetauvcsrc mode=2K ! queue ! h264parse ! nvv4l2decoder ! queue ! nvvidconv ! video/x-raw, format=BGRx ! videoconvert ! video/x-raw, format=BGR ! queue ! appsink\n</code></pre>"},{"location":"help/","title":"Help","text":""},{"location":"help/#getting-help","title":"Getting Help","text":"<ul> <li>Updated Docs and Events</li> <li>Community discussion - Linux Streaming</li> <li>Community discussion - USB API</li> <li>If you want to talk to someone, send email to jcasman@oppkey.com </li> </ul>"},{"location":"help/#faq","title":"FAQ","text":""},{"location":"help/#can-i-stream-indefinitely","title":"Can I stream indefinitely?","text":"<p>The THETA Z1 can power off the USB-C port and stream at the same time. Using USB 3.0 or better, the charge increases in our tests.  The camera does get hot.  Upgrade to the latest firmware.  If possible, attach a small fan to your tripod and point it at the body of the THETA.</p> <p>The V drains slowly.  It will last about 8 hours.  You may be able  to bypass the battery, but this is not tested.</p>"},{"location":"help/#devvideo0-freezes-on-x86","title":"/dev/video0 freezes on x86","text":"<p>Change line 190 of gst_viewer.c.</p> <pre><code>if (strcmp(cmd_name, \"gst_loopback\") == 0)\n    pipe_proc = \"decodebin ! autovideoconvert ! \"\n        \"video/x-raw,format=I420 ! identity drop-allocation=true !\"\n        \"v4l2sink device=/dev/video0 qos=false sync=false\";\n</code></pre>"},{"location":"help/#the-theta-is-not-appearing-on-devvideo0","title":"The THETA is not appearing on /dev/video0","text":"<p>Install v4l2loopback.</p>"},{"location":"help/#how-do-i-reduce-the-default-4k-stream-to-2k-to-improve-ai-processing","title":"How do I reduce the default 4K stream to 2K to improve AI processing?","text":"<p>If your AI processing is going to slowly, try to reduce resolution from 4K to 2K.  You likely need to do this on Jetson Nano as 4K often hangs due to limited resources on Nano.</p> <p>In gst_viewer.c, change line 248 from THETAUVC_MODE_UHD_2997 to THETAUVC_MODE_FHD_2997.</p> <p>Refer to thetauvc.c#L55 for definition.</p>"},{"location":"help/#i-cant-use-it-on-xavier","title":"I can't use it on Xavier","text":"<p>Change to:</p> <p>\"nvv4l2decoder ! nv3dsink sync=false\" </p>"},{"location":"help/#gstthetauvc-showing-_found-1-thetas-but-none-available","title":"gstthetauvc showing _Found 1 Theta(s), but none available","text":"<p>If you have another version of libuvc installed, please uninstall it first.</p> <p>See this closed issue on GitHub.</p> <p>This is the advice from nickel110, the author of the gstthetauvc.</p> <p>That is a typical error when the program is loading the original libuvc.so. If libuvc package is installed on your system, uninstall it.</p> <p>This was the error message.</p> <pre><code>Setting pipeline to PAUSED ...\nERROR: Pipeline doesn't want to pause.\nERROR: from element /GstPipeline:pipeline0/GstThetauvcsrc:thetauvcsrc0: Found 1 Theta(s), but none available.\nAdditional debug info:\ngstthetauvcsrc.c(495): gst_thetauvcsrc_start (): /GstPipeline:pipeline0/GstThetauvcsrc:thetauvcsrc0\nSetting pipeline to NULL ...\nFreeing pipeline ...\n</code></pre> <p>This is another error message from buburider.</p> <pre><code>ERROR: Pipeline doesn't want to pause.\nSetting pipeline to NULL ...\nERROR: from element /GstPipeline:pipeline0/GstThetauvcsrc:thetauvcsrc0: Found 1 Theta(s), but none available (No such device).\nAdditional debug info:\ngstthetauvcsrc.c(493): gst_thetauvcsrc_start (): /GstPipeline:pipeline0/GstThetauvcsrc:thetauvcsrc0\nFreeing pipeline ...\n0:00:00.761629738     8 0x55dc6da40c00 DEBUG            thetauvcsrc gstthetauvcsrc.c:270:gst_thetauvcsrc_finalize:&lt;thetauvcsrc0&gt; finalize\n</code></pre> <p>This is his solution.</p> <p>I\u2019ve just found the issue: my jetson computer had a previous libuvc library installed. So, instead of using libuvc-theta, the application loaded the native version. This is the reason why the same code was working in my laptop. In the laptop, the native version of libuvc didn\u2019t exist. After resolving that, it works fine.</p>"},{"location":"meetup/","title":"Meetup Archive","text":""},{"location":"meetup/#notes-archive","title":"Notes Archive","text":"<ul> <li>September 1, 2020 - First Linux Meetup</li> </ul>"},{"location":"optimization/","title":"gstreamer and ffmpeg hardware acceleration and optimization","text":"<p>Using gstreamer, we reduced latency from the default 550ms to 220ms. The latency is measured from the camera to the screen and may be higher with machine vision that may need to move the frame from the GPU to system memory.  We achieved this improvement by using two gstreamer plug-ins:</p> <ul> <li>nvdec hardware decoding plug-in for NVIDIA GPUs</li> <li>glimagesink OpenGL plug-in</li> </ul> <p>Using ffmpeg, community member Paul Gullet  reported that he doubled the framerate in his tests. For ffmpeg, you need to compile in support for:</p> <ul> <li>h264_nvmpi for hardware encoding and decoding</li> </ul>"},{"location":"optimization/#gstreamer-overview","title":"gstreamer overview","text":"<p>nvdec uses dedicated NVIDIA GPU hardware decoding features and fast copy to move frames between system and GPU memory. The THETA H.264 stream is decoded on the GPU and outputs buffers in raw format on the GPU.  </p> <p>Instead of downloading the frame from the GPU to system memory, we use glimagesink to display the OpenGL textures  to the computer monitor without having to transfer the frame to system memory.</p> <p>To use v4l2loopback, we show how to use gldownload to transfer the frame into system memory. Although this technique increases latency, it appears to be faster than streaming without hardware decoding on our test system.</p>"},{"location":"optimization/#audience","title":"Audience","text":"<p>If you already have streaming working with the THETA on your x86 Linux machine  and want to experiment with reducing latency, this article will guide you through installing and configuring hardware decoding on the GPU.</p> <p>If you are using NVIDIA Jetson boards, you do not need this article. On Nano hardware, you are likely already using hardware acceleration. If you are using NVIDIA Jetson Xavier hardware,  the hardware decoder is nvv4l2decoder and is included in JetPack, the Jetson OS you download from NVIDIA. On Jetson, the sink is nv3dsink.</p> <p>If you are using x86 and do not have streaming working at all, you should first try this pipeline.</p> <pre><code>pipe_proc = \" decodebin ! autovideosink sync=false qos=false\";\n</code></pre> <p>Note that both sync and qos are false.</p> <p>If you have enabled the pipeline above and your framerate is still extremely slow, you can try installing all the gstreamer plug-ins using apt from binaries.  </p> <p>If you're still stuck with unusable framerates, this article on using the dedicated video decoder on the GPU may help.  However, due to the number of steps involved in installing the gstreamer plug-in, the primary target audience is someone that already has  live streaming working and is interested in trying to reduce latency.</p>"},{"location":"optimization/#tests","title":"Tests","text":""},{"location":"optimization/#nvdec-and-glimagesink","title":"nvdec and glimagesink","text":"<pre><code>pipe_proc = \"nvdec ! glimagesink qos=false sync=false\";\n</code></pre> <p>foreground: 59.182</p> <p>THETA video: 58.932</p> <p>Latency: 250ms</p>"},{"location":"optimization/#default-decodebin-and-autovideosink","title":"Default decodebin and autovideosink","text":"<pre><code>pipe_proc = \" decodebin ! autovideosink sync=false\";\n</code></pre> <p>foreground: 691</p> <p>THETA video: 141</p> <p>Latency: 550ms</p>"},{"location":"optimization/#result-latency-reduced-by-50","title":"Result: Latency Reduced by 50%","text":""},{"location":"optimization/#equipment","title":"Equipment","text":"<ul> <li>Intel i7-6800K</li> <li>NVIDIA GTX 950 GPU</li> <li>RICOH THETA Z1 with firmware 1.60.1</li> </ul>"},{"location":"optimization/#software","title":"Software","text":"<ul> <li>Ubuntu 20.04</li> <li>NVIDIA Linux graphics driver 455.23</li> <li>CUDA Version: 11.1</li> <li>gstreamer 1.16.2</li> <li>NVIDIA Video Codec 11.0.10</li> </ul>"},{"location":"optimization/#overview-of-steps","title":"Overview of Steps","text":"<ol> <li>verify that you don't have nvdec installed.  If you have it installed, you can skip most of this document and go to the section on the gstreamer pipline configuration of gst_viewer.c</li> <li>Download and install gst-plugins-bad</li> <li> <p>Install NVIDIA CODEC SDK</p> </li> <li> <p>Modify gst_viewer pipeline to use the nvdec plug-in for hardware decoding and glimagesink for display to the screen</p> </li> </ol>"},{"location":"optimization/#tips","title":"Tips","text":""},{"location":"optimization/#verify-if-you-have-nvdec-installed","title":"Verify if you have nvdec installed.","text":"<pre><code>$ gst-inspect-1.0 nvdec\nNo such element or plugin 'nvdec'\n</code></pre> <p>If nvdec is installed, you will see this:</p> <pre><code>$ gst-inspect-1.0 | grep nvdec\nnvdec:  nvdec: NVDEC video decoder\n</code></pre>"},{"location":"optimization/#download-the-gst-plugins-bad","title":"Download the gst-plugins-bad","text":"<p>After you clone the repo, you need to checkout the branch that is the same as the version of gstreamer you have installed.</p> <p>Clone repo.</p> <pre><code>git clone git://anongit.freedesktop.org/gstreamer/gst-plugins-bad\ncd gst-plugins-bad/\n\n# verify gstreamer version\n$ gst-inspect-1.0 --version\ngst-inspect-1.0 version 1.16.2\nGStreamer 1.16.2\n\n$ git checkout 1.16.2\nHEAD is now at a6f26408f Release 1.16.2\n\n# verify that you're on the correct branch\n$ git branch\n* (HEAD detached at 1.16.2)\n  master\n</code></pre>"},{"location":"optimization/#install-nvidia-codec-sdk","title":"Install NVIDIA CODEC SDK","text":"<ol> <li>Download NVIDIA CODEC SDK.</li> <li>Unzip to <code>/path/to/video/codec/sdk</code></li> </ol> <pre><code>cd /path/to/video/codec/sdk\ncp /usr/local/cuda/include/cuda.h /path/to/gst-plugins-bad/sys/nvenc\ncp Interface/nvEncodeAPI.h /path/to/gst-plugins-bad/sys/nvenc\ncp Interface/cuviddec.h /path/to/gst-plugins-bad/sys/nvdec\ncp Interface/nvcuvid.h /path/to/gst-plugins-bad/sys/nvdec\n</code></pre>"},{"location":"optimization/#build-and-install-plug-in","title":"Build and Install Plug-in","text":"<p>Configure and build.</p> <pre><code>$ NVENCODE_CFLAGS=\"-I/home/craig/Development/gstreamer/gst-plugins-bad/sys/nvenc\" ./autogen.sh --disable-gtk-doc --with-cuda-prefix=\"/usr/local/cuda\"\n\ncd sys/nvenc\nmake\nsudo cp .libs/libgstnvenc.so /usr/lib/x86_64-linux-gnu/gstreamer-1.0/\ncd ../nvdec\nmake\nsudo cp .libs/libgstnvdec.so /usr/lib/x86_64-linux-gnu/gstreamer-1.0/\n</code></pre> <p>Confirm that autogen configured project to build nvdec.</p> <p></p>"},{"location":"optimization/#verify-install","title":"Verify Install","text":""},{"location":"optimization/#configure-gst_viewerc","title":"Configure gst_viewer.c","text":"<p>Pipeline is roughly around line 192.  GitHub permalink is here.</p> <pre><code>    else\n        // original pipeline\n        // pipe_proc = \" decodebin ! autovideosink sync=false\";\n        // use gstreamer plug-in for hardware acceleration\n        pipe_proc = \"nvdec ! glimagesink qos=false sync=false\";\n</code></pre>"},{"location":"optimization/#results","title":"Results","text":""},{"location":"optimization/#original-pipeline","title":"Original Pipeline","text":"<p>The left video is a Logitech C920 USB webcam. The right video is the THETA. </p> <p>Original pipeline.  There is a lag on the THETA video when I move my hand.   </p> <p></p>"},{"location":"optimization/#nvdec-pipeline","title":"nvdec pipeline","text":"<p>The THETA video stream is now much closer to the latency of the NVIDIA C920. </p> <p></p>"},{"location":"optimization/#configuration-with-v4l2loopack-on-devvideo","title":"Configuration with v4l2loopack on /dev/video*","text":"<p>To use nvdec with v4l2loopback, I needed to download the OpenGL textures from the GPU to video frames.  This introduced some latency. However, testing with vlc still showed  improvement over the standard pipeline.</p> <pre><code>    if (strcmp(cmd_name, \"gst_loopback\") == 0)\n    // original pipeline\n        // pipe_proc = \"decodebin ! autovideoconvert ! \"\n        //  \"video/x-raw,format=I420 ! identity drop-allocation=true !\"\n        //  \"v4l2sink device=/dev/video2 qos=false sync=false\";\n        //\n        //modified pipeline below\n        pipe_proc = \"nvdec ! gldownload ! videoconvert n-thread=0 ! \"\n            \"video/x-raw,format=I420 ! identity drop-allocation=true !\"\n            \"v4l2sink device=/dev/video2 qos=false sync=false\";     \n</code></pre> <p>More information on using gldownload is available here.</p>"},{"location":"optimization/#v4l2loopback-and-vlc-example","title":"v4l2loopback and vlc example","text":"<p>vlc is accessing the camera on <code>/dev/video2</code>.  I'm doing the test at night in a darkened room.</p> <p></p>"},{"location":"optimization/#nvidia-jetson","title":"NVIDIA Jetson","text":"<p>The Jetson is likely already using hardware accleration. You can get more examples on using gstreamer with <code>nvv4l2decoder</code>, <code>nvvidconv</code>, and <code>nv3dsink</code> in the NVIDIA Accelerated GStreamer guide.  Hopefuly, this link works. </p> <p>If the link is broken, use Google search or go to the NVIDIA Jetson Linux Developer Guide and  go to the Multimedia section.</p> <p></p>"},{"location":"optimization/#gstreamer-references","title":"gstreamer references","text":"<ul> <li>README for gst-plugins-bad nvenc</li> <li>How to install NVIDIA Gstreamer plugins (nvenc, nvdec) on Ubuntu by Taras Lishchenko on LifeStyleTransfer</li> <li>Install NVDEC and NVENC on Gstreamer plugins by Corenel on Gist</li> <li>NVIDIA Hardware accelerated video Encoding/Decoding (nvcodec) - Gstreamer by Naresh Ganesan on Medium</li> </ul>"},{"location":"optimization/#ffmpeg","title":"ffmpeg","text":"<p>Contributed by Paul Gullett.</p> <p>I haven\u2019t done much other than compiled it all and run it</p> <p>jocover/jetson-ffmpeg</p> <p>It gives me a doubling of the frame rate on the jetson. Use this as the guide for ffmpeg compilation on the jetson</p> <p>https://trac.ffmpeg.org/wiki/CompilationGuide/Ubuntu </p> <p>but use the ffmpeg git repo mentioned in the jetson-ffmpeg github as part of the ffmpeg compilation.</p>"},{"location":"software/","title":"Software Requirements","text":""},{"location":"software/#basic-software-for-live-streaming","title":"Basic Software for Live Streaming","text":"<p>You need to download the two GitHub repos below and compile the driver and sample code.</p> <ul> <li>libuvc-theta</li> <li>libuvc-theta-sample</li> </ul> <p>If you want to use <code>/dev/video0</code>, you will also need</p> <ul> <li>v4l2loopback</li> </ul> <p>In addition, there are numerous dependencies to compile the tools listed above.  However, have no fear, we will walk you through it.</p>"},{"location":"software/#using-gstreamer-and-opencv-without-v4l2loopback","title":"Using gstreamer and OpenCV without v4l2loopback","text":"<p>v4l2loopback add complexity and latency.  If your objective is to use OpenCV, consider using gstthetauvc instead.</p>"},{"location":"software/#how-to-compile-and-install-software","title":"How To Compile and Install Software","text":"<ul> <li>Build and install on x86 Ubuntu 20.04</li> <li>Jetson Nano with OpenCV and VLC on /dev/video0</li> <li>Compile libuvc-theta on Jetson Nano - silent screencast</li> <li>Build and run v4l2loopback on Jetson Nano. Needed for <code>/dev/video0</code></li> </ul>"},{"location":"software/#getting-stream-on-devvideo0","title":"Getting Stream on /dev/video0","text":"<p>Steps:</p> <ol> <li>compile and install libuvc-theta</li> <li>compile and install libuvc-theta-sample</li> <li>compile and install v4l2loopack</li> <li>run <code>gst_loopback</code> from <code>libuvc-theta-sample</code></li> <li>access the correct video device with OpenCV or any video 4 Linux 2 application such as VLC.  The video device is specified in the source code.</li> </ol>"},{"location":"software/#compile-and-install-v4l2loopback","title":"Compile and Install v4l2loopback","text":"<pre><code>git clone https://github.com/umlaeute/v4l2loopback.git\ncd v4l2loopback\nmake \nsudo make install\nsudo depmod -a\n</code></pre>"},{"location":"software/#load-and-use","title":"Load and use","text":"<p>This assumes that you have adjusted the video device in <code>gst_viewer.c</code>.</p> <pre><code>$ sudo modprobe v4l2loopback\n$ cd path_to_gst_loopback_directory\n$ ./gst_loopback\n$ cvlc v4l2:///dev/video2\nVLC media player 3.0.9.2 Vetinari (revision 3.0.9.2-0-gd4c1aefe4d)\n[0000556fc2bd6db0] dummy interface: using the dummy interface module...\n</code></pre>"},{"location":"software/#how-to-load-v4l2loopback-automatically","title":"How to Load v4l2loopback automatically","text":"<p>In the file <code>/etc/modules-load.d/modules.conf</code> add a new line <code>v4l2loopback</code>.</p> <pre><code>$ pwd\n/etc/modules-load.d\ncraig@jetson:/etc/modules-load.d$ cat modules.conf \n# /etc/modules: kernel modules to load at boot time.\n#\n# This file contains the names of kernel modules that should be loaded\n# at boot time, one per line. Lines beginning with \"#\" are ignored.\n\n# bluedroid_pm, supporting module for bluetooth\nbluedroid_pm\n# modules for camera HAL\nnvhost_vi\n# nvgpu module\nnvgpu\n\n# for RICOH THETA live streaming\n# v4l2loopback device on /dev/video0. specify in gst_viewer.c\nv4l2loopback\n\ncraig@jetson:/etc/modules-load.d$ \n</code></pre>"},{"location":"software/#check-kernel-module-load","title":"Check kernel module load","text":"<pre><code>$ lsmod\nModule                  Size  Used by\nbnep                   16562  2\nzram                   26166  4\noverlay                48691  0\nspidev                 13282  0\nv4l2loopback           37383  0\nnvgpu                1579891  18\nbluedroid_pm           13912  0\nip_tables              19441  0\nx_tables               28951  1 ip_tables\ncraig@jetson:/etc/modules-load.d$ \n</code></pre>"},{"location":"software/#v4l2loopback-tests-and-examples","title":"v4l2loopback tests and examples","text":""},{"location":"software/#gst-launch-10-pipeline","title":"gst-launch-1.0 pipeline","text":"<pre><code>$ gst-launch-1.0 v4l2src device=/dev/video2 ! video/x-raw,framerate=30/1 ! xvimagesink\nSetting pipeline to PAUSED ...\nPipeline is live and does not need PREROLL ...\nSetting pipeline to PLAYING ...\nNew clock: GstSystemClock\n</code></pre>"},{"location":"software/#vlc-command-line-example","title":"VLC command line example","text":"<pre><code>$ cvlc v4l2:///dev/video2\nVLC media player 3.0.9.2 Vetinari (revision 3.0.9.2-0-gd4c1aefe4d)\n[000055573aea4db0] dummy interface: using the dummy interface module...\n</code></pre>"},{"location":"software/#use-v4l2-ctl-to-get-video-device-output","title":"Use v4l2-ctl to get video device output","text":"<p>I\u2019ve modified the source to stream 2K video.</p> <pre><code>$ v4l2-ctl --list-formats-ext --device  /dev/video2\nioctl: VIDIOC_ENUM_FMT\n    Type: Video Capture\n\n    [0]: 'YU12' (Planar YUV 4:2:0)\n        Size: Discrete 1920x960\n            Interval: Discrete 0.033s (30.000 fps)\n</code></pre>"},{"location":"software/#using-gstthetauvc-to-eliminate-v4l2loopback","title":"Using gstthetauvc to eliminate v4l2loopback","text":"<p>gstthetauvc is an alternative to using libuvc-theta-sample with v4l2loopack.  In these example, there is no <code>/dev/video*</code>.  To get the stream into OpenCV, set <code>VideoCapture</code> to the pipeline as shown in the examples below.</p> <p>The plug-in is installed in <code>/usr/lib/x86_64-linux-gnu/gstreamer-1.0</code>.</p> <p></p> <p>Latency is about 360ms latency and the stream is stable.</p> <p></p>"},{"location":"software/#display-thetauvcsrc-to-monitor-with-gst-launch-10","title":"Display thetauvcsrc to monitor with gst-launch-1.0","text":"<p>This example is using hardware acceleration on x86 with NVIDIA card with Linux NVIDIA driver 510.</p> <pre><code>gst-launch-1.0 thetauvcsrc mode=4K \\\n  ! queue \\\n  ! h264parse \\\n  ! nvdec \\\n  ! queue \\\n  ! glimagesink sync=false \n</code></pre> <p></p>"},{"location":"software/#using-gstthetauvc-with-opencv","title":"Using gstthetauvc with OpenCV","text":"<pre><code>import cv2\n# pipeline below worked\n# cap = cv2.VideoCapture(\"thetauvcsrc \\\n#     ! decodebin \\\n#     ! autovideoconvert \\\n#     ! video/x-raw,format=BGRx \\\n#     ! queue ! videoconvert \\\n#     ! video/x-raw,format=BGR ! queue ! appsink\")\n\n# pipeline suggestion thanks to nickel110\n# attempt to force hardware acceleration\n# tested with NVIDIA 510.73 with old GTX 950 on Ubuntu 22.04\ncap = cv2.VideoCapture(\"thetauvcsrc \\\n    ! queue \\\n    ! h264parse \\\n    ! nvdec \\\n    ! gldownload \\\n    ! queue \\\n    ! videoconvert n-threads=0 \\\n    ! video/x-raw,format=BGR \\\n    ! queue \\\n    ! appsink\")\n\nif not cap.isOpened():\n    raise IOError('Cannot open RICOH THETA')\n\nwhile True:\n    ret, frame = cap.read()\n    frame = cv2.resize(frame, None, fx=0.25, fy=0.25, interpolation=cv2.INTER_AREA)\n    cv2.imshow('frame', frame)\n\n\n    c = cv2.waitKey(1)\n    if c == 27:\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"software/#usb-api","title":"USB API","text":"<ul> <li>libptp - next section for detailed walkthrough</li> </ul>"},{"location":"usb_api/","title":"USB API","text":""},{"location":"usb_api/#overview","title":"Overview","text":"<p>RICOH THETA cameras can be controlled and powered indefinitely over a USB cable using the THETA USB API. This is an extension of Media Transfer Protocol (MTP). Any library or application that uses MTP can access the camera.</p> <p>This document explains the most common applications, libraries and techniques to use the  RICOH THETA USB API from Linux.</p>"},{"location":"usb_api/#advantages","title":"Advantages","text":"<p>The USB API has the following advantages over the  Wi-Fi API:</p> <ul> <li>wake camera from sleep</li> <li>put camera to sleep</li> <li>power camer off</li> <li>switch to live streaming mode</li> <li>switch from live streaming mode to still image or video mode</li> <li>theoretical faster transfer speed with USB 3.0 (for the Z1) or USB 2.0 for the V and other models. In actual use, it appears that the USB does transfer faster than Wi-Fi in most cases as there is usually Wi-Fi signal interference or degradation.</li> <li>it is possible to use an unsupported workaround to turn the  camera on over the USB cable. This technique doesn't use the USB API and is not supported by RICOH. The technique is explained in the camera section.</li> </ul>"},{"location":"usb_api/#hardware-and-os","title":"Hardware and OS","text":"<p>We tested the USB API on the following platforms:</p> <ul> <li>x86 on Ubuntu 22.04.  In the past, we used 20.04, 18.04, 16.04, and 14.04.</li> <li>NVIDIA Jetson Nano with JetPack 4.6.1 R32 revision 7.1 (Ubuntu 18.04).  Other versions of hardware and software should work.</li> <li>Raspberry Pi 3 with Raspian 10, buster.  Any version and any model should work. Note that the Rasbperry Pi 3 and earlier models cannot stream the THETA over a USB cable.</li> </ul>"},{"location":"usb_api/#seeing-version-of-ubuntu","title":"Seeing Version of Ubuntu","text":"<pre><code>lsb_release -a\nNo LSB modules are available.\nDistributor ID: Ubuntu\nDescription:    Ubuntu 20.04.6 LTS\nRelease:    20.04\nCodename:   focal\n</code></pre>"},{"location":"usb_api/#getting-vesion-of-linux-tegra","title":"Getting Vesion of Linux Tegra","text":"<pre><code>cat /etc/nv_tegra_release \n# R32 (release), REVISION: 7.1\n</code></pre>"},{"location":"usb_api/#mtp-software","title":"MTP software","text":"<p>This document covers two different strategies:</p> <ol> <li>gphoto2 - command line and Python bindings to library</li> <li>libptp2 - either as library or with command line ptpcam</li> </ol> <p>As of July 2023, the most common approach is to use libgphoto2 with gphoto. Up until 2021, we were primarily using ptpcam, the command line interface to libptp2.</p> <p>You can put ptpcam and gphoto2 in a bash shell script or run a system process from the language you are using. For example, in Python, you can use the  subprocess module.</p> <p>In Dart, you can use the Process class. I am using the process_run package.</p> <p>As a demonstration of the USB API, I put a GUI wrapper around ptpcam using Flutter and Dart.</p> <p></p> <p>The next demo shows a common use of putting the camera to sleep and waking it up.</p> <p></p> <p>There are several workarounds and fixes that we're using. Feel free to ask questions.</p>"},{"location":"usb_api/#mtp-bindings","title":"MTP bindings","text":"<p>You can also use python-gphoto2 or other projects to access MTP libraries from inside of the language you are using.  Python is the most common language.  People also use bash.</p>"},{"location":"usb_api/#libptp-and-ptpcam","title":"libptp and ptpcam","text":"<p>libptp2 builds on libusb. The latest version is 2-1.2.0, which was last updated on 2016-01-12.</p> <p>The package builds against an older version of libusb, not the current version that ships with Ubuntu and Raspian.  To get around this, you can either install an older version of libusb from source or install libusb-compat-0.1 in addition to libusb-1xx-dev. </p> <p>Do not install libusb-compat-0.1 and libusb-0.1x on the same system.  Certain Linux distributions such as ArchLinux/Manjaro have packages for libusb-compat.</p> <p>For Ubuntu 20.04 and 18.04 (JetPack 4.4), I was able to build the package with libusb-dev.</p> <p>For Raspian 10 buster, I used the source code for libusb-compat.</p> <p>As there are many steps and possible places where you may get stuck feel free to post a question in our  forum.</p>"},{"location":"usb_api/#download-libptp-source","title":"Download libptp source","text":"<p>libptp - Picture Transfer Protocol lib</p> <p>Get the newest version, which is 2-1.2 right now.</p> <p></p>"},{"location":"usb_api/#build-libptp","title":"build libptp","text":"<pre><code>$ ./configure\n$ make\n</code></pre> <p>If you have a build error when compiling libusb, you may need to install the  development libraries for libusb.</p> <p></p>"},{"location":"usb_api/#install-libusb-dev","title":"install libusb-dev","text":"<p>For Ubuntu 18.04, 20.04 on x86 and JetPack 4.4 on Nano.</p> <pre><code>$ sudo apt install libusb-dev\n</code></pre> <p>You may not need this step if you already have the libusb development libraries installed.</p> <p></p> <p>Example on x86 Ubuntu 20.04. </p> <pre><code>$ sudo apt-get install libusb-dev\n</code></pre>"},{"location":"usb_api/#for-raspberry-pi","title":"For Raspberry Pi","text":"<p>Version.</p> <pre><code>Distributor ID: Raspbian\nDescription:    Raspbian GNU/Linux 10 (buster)\nRelease:    10\nCodename:   buster\n</code></pre> <p>Get the libusb compatibility layer.</p> <p></p> <p></p>"},{"location":"usb_api/#install-libptp","title":"install libptp","text":"<pre><code>$ sudo make install\n</code></pre> <p>On x86 Ubuntu 20.04.</p> <pre><code>$ tar zxvf libptp2-1.2.0.tar.gz \nlibptp2-1.2.0/\n</code></pre> <p><code>./configure</code> ran with no problems <code>make</code> ran with no problems <code>sudo make install</code> ran with no problems  </p> <pre><code>$ pwd\n/usr/local/lib\n$ ls -l libptp2.*\n-rw-r--r-- 1 root root 352640 Aug 31 11:54 libptp2.a\n-rwxr-xr-x 1 root root    941 Aug 31 11:54 libptp2.la\nlrwxrwxrwx 1 root root     16 Aug 31 11:54 libptp2.so -&gt; libptp2.so.1.1.5\nlrwxrwxrwx 1 root root     16 Aug 31 11:54 libptp2.so.1 -&gt; libptp2.so.1.1.5\n-rwxr-xr-x 1 root root 249352 Aug 31 11:54 libptp2.so.1.1.5\n</code></pre>"},{"location":"usb_api/#set-usrlocallib-in-library-path","title":"set /usr/local/lib in library path","text":"<p>The default location of the libptp install is <code>/usr/local/lib</code>. Make sure that this is in your library path.  If it isn't, add it to a file such as <code>libc.conf</code> in <code>/etc/ld.so.conf/</code>.</p> <pre><code>$ cd /etc/ld.so.conf.d/\n$ ls\n$ cat libc.conf\n</code></pre> <p></p>"},{"location":"usb_api/#run-ldconfig","title":"run ldconfig","text":"<p>Load the library configuration.</p> <pre><code>$ sudo /sbin/ldconfig -v\n</code></pre> <p></p> <p>On x86 Ubuntu 20.04.</p> <pre><code>$ cd /etc/ld.so.conf.d/\n$ l\nfakeroot-x86_64-linux-gnu.conf  x86_64-linux-gnu.conf\ni386-linux-gnu.conf             zz_i386-biarch-compat.conf\nlibc.conf\n$ cat libc.conf \n# libc default configuration\n/usr/local/lib\n$ sudo ldconfig\n$ \n</code></pre>"},{"location":"usb_api/#test-ptpcam","title":"Test ptpcam","text":"<p>Connect RICOH THETA to Jetson with a USB cable.</p> <p>Version of 2-1.2 of libptp has a bug in it.  Although ptpcam does take pictures and function normally, you will see an error about capture status.</p> <p></p> <p>On x86 Ubuntu.</p> <pre><code>$ ptpcam --info\n\nCamera information\n==================\nModel: RICOH THETA Z1\n  manufacturer: Ricoh Company, Ltd.\n  serial number: '10010104'\n  device version: 1.50.1\n  extension ID: 0x00000006\n  extension description: (null)\n  extension version: 0x006e\n\n$ cat /proc/cpuinfo \nprocessor   : 0\nvendor_id   : GenuineIntel\ncpu family  : 6\nmodel       : 60\nmodel name  : Intel(R) Pentium(R) CPU G3258 @ 3.20GHz\n</code></pre>"},{"location":"usb_api/#fix-problem-with-libptp-response","title":"Fix problem with libptp response","text":"<p>Go to line 77 of <code>ptp.h</code> and change <code>PTP_USB_INT_PACKET_LEN</code>  to <code>28</code>.</p> <p></p> <p>After modification, the code will look like this.</p> <p></p>"},{"location":"usb_api/#using-usb-api-with-ptpcam-libptp","title":"Using USB API with ptpcam (libptp)","text":""},{"location":"usb_api/#test-ptpcam-response-again","title":"test ptpcam response again","text":"<p>Take a still image picture with <code>ptpcam --capture</code>.</p> <p></p>"},{"location":"usb_api/#set-camera-to-live-streaming-mode","title":"Set camera to live streaming mode","text":"<p>Check on camera mode.</p> <pre><code>$ ptpcam --show-property=0x5013\n</code></pre> <p>Set to live streaming mode.</p> <pre><code>$ ptpcam --set-property=0x5013 --val=0x8005\n</code></pre> <p></p> <p>Using the official  RICOH USB API documentation, you can verify that <code>0x8005</code> is live streaming mode.  The camera LED should show that the THETA is in LIVE mode.</p> <p></p> <p>In our tests, the RICOH THETA Z1 could charge while streaming over a USB 3.0 port (blue insert).  </p> <p>After hours of streaming, the Z1 LED looks like this.</p> <p></p> <p>The response codes are shown below.</p> <pre><code>    0x0001 = single-shot shooting\n    0x0003 = Interval shooting\n    0x8002 = Movie shooting\n    0x8003 = Interval composite shooting\n    0x8004 = Multi bracket shooting\n    0x8005 = Live streaming\n    0x8006 = Interval shooting - tripod stabilizatio is off \n             (top/bottom correction and stitching optimized)\n    0x8007 = Interval shooting - tripod stabilization is on\n</code></pre> <p>If you set the camera back to still image, single shot mode, you will see this response.</p> <pre><code>$ ptpcam --set-property=0x5013 --val=0x0001\n\nCamera: RICOH THETA V\n'Still Capture Mode' is set to: 0x8005 (-32763)\nChanging property value to 0x0001 [(null)] succeeded.\n</code></pre>"},{"location":"usb_api/#wake-camera-from-sleep","title":"Wake Camera From Sleep","text":"<p>In this test, I have the Z1 power off disabled.  I left the camera in sleep mode overnight.  When I woke up in the morning, I woke the Z1 up using an ssh session into the Jetson Nano and  running this command.</p> <pre><code>$ ptpcam --set-property=0xD80E --val=0x00\n\nCamera: RICOH THETA Z1\n'UNKNOWN' is set to: 1\nChanging property value to 0x00 [(null)] succeeded.\n</code></pre> <p>I tested the camera with the info command.</p> <pre><code>$ ptpcam --info\n\nCamera information\n==================\nModel: RICOH THETA Z1\n  manufacturer: Ricoh Company, Ltd.\n  serial number: '10010104'\n  device version: 1.50.1\n  extension ID: 0x00000006\n  extension description: (null)\n  extension version: 0x006e\n</code></pre> <p>In my initial tests, I had to run the <code>info</code> command twice after I woke the camera up from sleep. The first time, I could not open the session.</p> <p>I got this error.</p> <pre><code>$ ptpcam --info\nERROR: Could not open session!\n</code></pre> <p>In the future, I'll run more tests using the camera FunctionalMode to check status.</p> <p>This is another example with x86.  Initially, the camera is asleep.</p> <pre><code>craig@cube:~$ ptpcam --info\n\nCamera information\n==================\nERROR: Could not open session!\ncraig@cube:~$ ptpcam --info\n\nCamera information\n==================\nModel: RICOH THETA Z1\n  manufacturer: Ricoh Company, Ltd.\n  serial number: '10010104'\n  device version: 1.50.1\n  extension ID: 0x00000006\n  extension description: (null)\n  extension version: 0x006e\n\ncraig@cube:~$ ptpcam --set-property=0xd80e --val=0\n\nCamera: RICOH THETA Z1\n'UNKNOWN' is set to: 1\nChanging property value to 0 [(null)] succeeded.\n</code></pre> <p>At this point, the camera is awake.</p>"},{"location":"usb_api/#put-camera-to-sleep","title":"Put camera to sleep","text":"<pre><code>$ ptpcam --set-property=0xd80e --val=0x01\n\nCamera: RICOH THETA Z1\n'UNKNOWN' is set to: 0\nChanging property value to 0x01 [(null)] succeeded.\n</code></pre> <p>The camera is asleep.</p>"},{"location":"usb_api/#auto-power-off-delay","title":"Auto Power Off Delay","text":"<p>Disable auto power off.</p> <p>$ ptpcam --set-property=0xd81b=0</p> <p>Camera: RICOH THETA Z1 'UNKNOWN' is set to: 0</p> <p>Verify that auto power off is disabled.</p> <pre><code>$ ptpcam --show-property=0xd81b\n\nCamera: RICOH THETA Z1\n'UNKNOWN' is set to: 0\n</code></pre>"},{"location":"usb_api/#shutdown-camera","title":"Shutdown Camera","text":"<p>This will completely power off the camera and put into lowest battery mode.</p> <pre><code>$ ptpcam -R 0x1013\nCamera: RICOH THETA Z1\nSending generic request: reqCode=0x1013, params=[0x00000000,0x00000000,0x00000000,0x00000000,0x00000000]\nPTP: I/O error\nERROR: Could not close session!\n</code></pre> <p>To turn the camera back on, you must disconnect and then reconnect the USB cable of the camera.  You can also replicate this process in software.</p>"},{"location":"usb_api/#put-camera-in-still-image-mode","title":"Put Camera in Still Image Mode","text":"<p>You may want to take a detailed picture of the scene based on triggers from the live stream.</p> <p>To do this, you need to take the camera out of live streaming mode and put it into still image mode.  In the example below, I wrapped ptpcam in a script that explains the hexcode properties of the mode settings.  This helps me with testing.</p> <pre><code>$ ptpcam --set-property=0x5013 --val=0x0001\n\nCamera: RICOH THETA Z1\n'Still Capture Mode' is set to: [Normal]\nChanging property value to 0x0001 [(null)] succeeded.\n\n    0x0001 = single-shot shooting\n    0x0003 = Interval shooting\n    0x8002 = Movie shooting\n    0x8003 = Interval composite shooting\n    0x8004 = Multi bracket shooting\n    0x8005 = Live streaming\n    0x8006 = Interval shooting - tripod stabilizatio is off \n             (top/bottom correction and stitching optimized)\n    0x8007 = Interval shooting - tripod stabilization is on\n</code></pre> <p>You can verify the mode of with 0x5013.</p> <pre><code>$ ptpcam --show-property=0x5013\n\nCamera: RICOH THETA Z1\n'Still Capture Mode' is set to: [Normal]\n</code></pre> <p>Compare this with the result when the camera is in live streaming mode.</p> <pre><code>$ ptpcam --show-property=0x5013\n\nCamera: RICOH THETA Z1\n'Still Capture Mode' is set to: 0x8005 (-32763)\n\n    0x0001 = single-shot shooting\n    0x0003 = Interval shooting\n    0x8002 = Movie shooting\n    0x8003 = Interval composite shooting\n    0x8004 = Multi bracket shooting\n    0x8005 = Live streaming\n    0x8006 = Interval shooting - tripod stabilizatio is off \n             (top/bottom correction and stitching optimized)\n    0x8007 = Interval shooting - tripod stabilization is on\n</code></pre>"},{"location":"usb_api/#start-video-capture","title":"Start Video Capture","text":"<p>This records video to file. </p> <pre><code>ptpcam -R 0x101c,0,0,1\n</code></pre>"},{"location":"usb_api/#using-raw-ptp-commands","title":"Using Raw PTP Commands","text":"<p>Get camera info. </p> <pre><code>$ ptpcam -R 0x1001\nCamera: RICOH THETA Z1\nSending generic request: reqCode=0x1001, params=[0x00000000,0x00000000,0x00000000,0x00000000,0x00000000]\n64 00 06 00 00 00 6e 00 00 00 00 33 00 00 00 01 - d.....n....3....\n</code></pre>"},{"location":"usb_api/#using-multiple-cameras-with-ptpcam","title":"Using Multiple Cameras with ptpcam","text":""},{"location":"usb_api/#test-environment","title":"Test Environment","text":"<ul> <li>Ubuntu 20.04 on x86</li> <li>libptp and ptpcam.  compiled from source with patches.  v 2-1.2.0  (assuming you have this working.  If not please post again)</li> <li>Camera with dev id 42 is Z1 with firmware 1.50.1</li> <li>camera with dev id 41 is V with fimrware 3.40.1</li> </ul> <pre><code>craig@cube:~$ ptpcam --list-devices\n\nListing devices...\nbus/dev vendorID/prodID device model\n003/042 0x05CA/0x036D   RICOH THETA Z1\n003/041 0x05CA/0x2714   RICOH THETA V\n\n$ ptpcam --dev=042 --info\n\nCamera information\n==================\nModel: RICOH THETA Z1\n  manufacturer: Ricoh Company, Ltd.\n  serial number: '10010104'\n  device version: 1.50.1\n  extension ID: 0x00000006\n  extension description: (null)\n  extension version: 0x006e\n\ncraig@cube:~$ ptpcam --dev=041 --info\n\nCamera information\n==================\nModel: RICOH THETA V\n  manufacturer: Ricoh Company, Ltd.\n  serial number: '00105377'\n  device version: 3.40.1\n  extension ID: 0x00000006\n  extension description: (null)\n  extension version: 0x006e\n\ncraig@cube:~$ ptpcam --dev=041 --capture\n\nInitiating captue...\nObject added 0x00000226\nCapture completed successfully!\ncraig@cube:~$ ptpcam --dev=042 --capture\n\nInitiating captue...\nObject added 0x00000011\nCapture completed successfully!\n</code></pre> <p>Slightly more complex, list files:</p> <pre><code>~$ ptpcam --dev=042 --list-files\n\nListing files...\nCamera: RICOH THETA Z1\nHandler:           Size:    Captured:       name:\n0x0000000e:      9086912    2019-01-01 08:03    R0010001.JPG\n0x0000000f:      7968843    2019-01-01 08:00    R0010002.JPG\n0x00000010:      7990763    2019-01-01 08:01    R0010003.JPG\n0x00000011:      8008310    2019-01-01 08:03    R0010004.JPG\n</code></pre> <p>I have several hundred pictures on the V, but it showed it.</p> <pre><code>$ ptpcam --dev=041 --list-files\n\nListing files...\nCamera: RICOH THETA V\nHandler:           Size:    Captured:       name:\n0x00000142:      4152882    2020-06-17 20:59    R0010273.JPG\n0x00000143:      3979605    2020-06-17 21:03    R0010274.JPG\n0x00000147:      4413502    2020-06-17 21:43    R0010277.JPG\n...\n</code></pre>"},{"location":"usb_api/#test-with-two-cameras-in-streaming","title":"Test with Two Cameras in Streaming","text":"<pre><code>$ ptpcam --dev=041 --set-property=0x5013 --val=0x8005\n\nCamera: RICOH THETA V (bus 0, dev 41)\n'Still Capture Mode' is set to: [Normal]\nChanging property value to 0x8005 [(null)] succeeded.\ncraig@cube:~$ \n\n$ ptpcam --dev=042 --set-property=0x5013 --val=0x8005\n\nCamera: RICOH THETA Z1 (bus 0, dev 42)\n'Still Capture Mode' is set to: [Normal]\nChanging property value to 0x8005 [(null)] succeeded.\n</code></pre> <p>At this stage, I now have two cameras streaming into the same devices.  I need to do more tests to manipulate both streams. </p> <p>however, if your application is handing the stream and image processing already, then you should be good to go.</p>"},{"location":"usb_api/#other-ways-to-grab-device-id","title":"Other Ways to Grab Device ID","text":"<p>You can also grab the device ID with <code>lsusb</code> or libusb.</p> <p></p> <p>Compare the device IDs to <code>ptpcam --list-devices</code>.  The IDs should be the same.</p> <p></p>"},{"location":"usb_api/#gphoto2","title":"gphoto2","text":""},{"location":"usb_api/#command-line","title":"Command Line","text":""},{"location":"usb_api/#fixing-command-line-error-could-not-claim-the-usb-device","title":"Fixing Command Line Error - Could not claim the USB device","text":"<p>You may get this error.</p> <pre><code>$ gphoto2 --capture-image\n\n*** Error ***              \nAn error occurred in the io-library ('Could not claim the USB device'): Could not claim interface 0 (Device or resource busy). Make sure no other program (gvfs-gphoto2-volume-monitor) or kernel module (such as sdc2xx, stv680, spca50x) is using the device and you have read/write access to the device.\nERROR: Could not capture image.\nERROR: Could not capture.\n*** Error (-53: 'Could not claim the USB device') ***       \n</code></pre> <p>Fix for current session is to kill gvfs-gphoto2-volume-monitor and gvfsd-gphoto2 spawner.</p> <pre><code>$ ps aux |grep gvfs\n...\ncraig       2422  0.0  0.0 442504 13528 ?        Sl   08:19   0:00 /usr/libexec/gvfsd-gphoto2 --spawner :1.3 /org/gtk/gvfs/exec_spaw/1\n...\ncraig       1969  0.0  0.0 249860 10032 ?        Ssl  08:19   0:00 /usr/libexec/gvfs-gphoto2-volume-monitor\n...\n\n$ kill 2422\n$ kill 1969\n$ gphoto2 --capture-image\nNew file is in location /store_00020001/DCIM/100RICOH/R0010376.JPG on the camera\n$ \n</code></pre>"},{"location":"usb_api/#removing-gvfs-backend-permanently","title":"Removing gvfs-backend permanently","text":"<p>If you don't mount the THETA as a storage device with gphoto, you can remove gvfs-backend.  This is a workaround for the  conflict when you use gphoto2 from the command line to talk to the THETA.</p> <pre><code>$ sudo apt remove gvfs-backends\n[sudo] password for craig: \nReading package lists... Done\nBuilding dependency tree       \n</code></pre> <p>Reboot to test. </p> <p>After reboot. </p> <pre><code>$ gphoto2 -l\nThere is 1 folder in folder '/'.                                               \n - store_00020001\nThere is 1 folder in folder '/store_00020001'.\n - DCIM\nThere are 2 folders in folder '/store_00020001/DCIM'.\n - 100RICOH\n - SingleLensShooting\nThere is 1 folder in folder '/store_00020001/DCIM/100RICOH'.\n - HDR07-22_18-13\nThere are 0 folders in folder '/store_00020001/DCIM/100RICOH/HDR07-22_18-13'.\nThere are 0 folders in folder '/store_00020001/DCIM/SingleLensShooting'.\n</code></pre> <p>It works!</p>"},{"location":"usb_api/#check-camera-mode-still-image-video-streaming","title":"Check Camera Mode (still image, video, streaming)","text":"<p>StillCaptureMode API reference</p> <pre><code>$ gphoto2 --get-config=5013\nLabel: Still Capture Mode\nReadonly: 0\nType: MENU\nCurrent: 1\nChoice: 0 1\nChoice: 1 3\nChoice: 2 32770\nChoice: 3 32771\nChoice: 4 32772\nChoice: 5 32773\nChoice: 6 32774\nChoice: 7 32775\nEND\ncraig@craig-desktop:~$\n</code></pre>"},{"location":"usb_api/#unmount-camera","title":"unmount camera","text":"<p>If the camera is mounted, the commands may not work.</p> <p></p>"},{"location":"usb_api/#set-to-video-mode","title":"set to video mode","text":"<p>Using the API reference, we can see that video mode is hex <code>0x8002</code> or 32770 in base 10.</p> <pre><code>$ gphoto2 --set-config=5013=32770\n</code></pre>"},{"location":"usb_api/#start-video","title":"start video","text":"<pre><code>$ gphoto2 --set-config movie=1\n</code></pre>"},{"location":"usb_api/#stop-video","title":"stop video","text":"<p>this tip contributed by hugues</p> <pre><code>$ gphoto2 --set-config=/main/actions/opcode=0x1018,0xFFFFFFFF\n</code></pre>"},{"location":"usb_api/#start-video-and-stop-after-specified-time","title":"start video and stop after specified time","text":"<pre><code>$ gphoto2 --set-config movie=1 --wait-event=2s --set-config movie=0 \n</code></pre>"},{"location":"usb_api/#python-bindings","title":"Python bindings","text":"<p>From community member mhenrie</p> <p>original post</p> <pre><code>\"\"\"\nUSB api for added performance over http\n\nTheta api reference:\nhttps://developers.theta360.com/en/docs/v2/usb_reference/\n\nUnable to get mtp or ptp to connect to the camera; After some pain was able to get gphoto2 working\n\"\"\"\n\nimport os\nimport time\n\nimport gphoto2 as gp\n\n# Properties\nSHUTTER_SPEED = 'd00f'\nEXPOSURE_INDEX = '500f'\nF_NUMBER = '5007'\nAUDIO_VOLUME = '502c'\nCOLOR_TEMPERATURE = 'd813'\nEXPOSURE_PROGRAM_MODE = '500e'\n\n# milliseconds\nTIMEOUT = 10\nTIMEOUT_CAPTURE_DNG = 10000\n\n\ndef wait_for_event(camera, timeout=TIMEOUT, event_type=gp.GP_EVENT_TIMEOUT):\n    \"\"\"\n    Wait for event_type to to be triggered.\n    :param camera:\n    :param timeout:\n    :param event_type:\n    :return: event_data\n    \"\"\"\n    while True:\n        _event_type, event_data = camera.wait_for_event(timeout)\n        if _event_type == gp.GP_EVENT_TIMEOUT:\n            return\n        if _event_type == event_type:\n            return event_data\n\n\ndef set_config_by_index(config, index):\n    \"\"\"Set config using choice index\"\"\"\n    value = config.get_choice(index)\n    config.set_value(value)\n\n    return config\n\n\n# def list_files(camera, path='/'):\n#     result = []\n#     # get files\n#     for name, value in camera.folder_list_files(path):\n#         result.append(os.path.join(path, name))\n#     # read folders\n#     folders = []\n#     for name, value in camera.folder_list_folders(path):\n#         folders.append(name)\n#     # recurse over subfolders\n#     for name in folders:\n#         result.extend(list_files(camera, os.path.join(path, name)))\n#     return result\n#\n#\n# def get_file_info(camera, path):\n#     folder, name = os.path.split(path)\n#     return camera.file_get_info(folder, name)\n\n\nclass CameraUsb(object):\n    \"\"\"\n    Define API for multiple exposure\n    \"\"\"\n    def __init__(self, verbose=False):\n        self.verbose = verbose\n\n        self.camera = gp.Camera()\n\n        self.camera_config = None\n        self.status_config = None\n        self.other_config = None\n        self.shutter_speed_config = None\n        self.shutter_speed_options = []\n\n    def init(self):\n        \"\"\"\n        Set manual exposure and other defaults\n        :return: config\n        \"\"\"\n        try:\n            self.camera_config = self.camera.get_config()\n        except gp.GPhoto2Error:\n            raise RuntimeError(\"Unable to connect to Camera\")\n\n        self.other_config = self.camera_config.get_child_by_name('other')\n\n        # Manual/f-stop/iso\n        exposure_program_mode = self.other_config.get_child_by_name(EXPOSURE_PROGRAM_MODE)\n        if not exposure_program_mode.get_value() == '1':\n            print('Setting camera to Manual exposure program')\n            exposure_program_mode.set_value('1')\n            self.camera.set_config(self.camera_config)\n            wait_for_event(self.camera)\n\n            # When switching exposure program, we need to refresh the configs\n            self.camera_config = self.camera.get_config()\n            self.other_config = self.camera_config.get_child_by_name('other')\n\n        self.status_config = self.camera_config.get_child_by_name('status')\n\n        self.shutter_speed_config = self.other_config.get_child_by_name(SHUTTER_SPEED)\n        self.shutter_speed_options = [str(x) for x in self.shutter_speed_config.get_choices()]\n        if len(self.shutter_speed_options) != 61:\n            raise RuntimeError('Unble to determine shutter speed options; restart app')\n\n        fstop = self.other_config.get_child_by_name(F_NUMBER)\n        fstop.set_value('560')\n\n        iso = self.other_config.get_child_by_name(EXPOSURE_INDEX)\n        iso.set_value('80')\n\n        self.camera.set_config(self.camera_config)\n        wait_for_event(self.camera)\n\n    def get_info(self):\n        \"\"\"\n        :return: Dict containing serialnumber, batterylevel, remainingpictures, etc\n        \"\"\"\n        if not self.camera_config:\n            self.init()\n\n        battery_level = self.status_config.get_child_by_name('batterylevel').get_value()\n        # Convert '67%' to int\n        battery_level = int(''.join([x for x in battery_level if x.isdigit()]))\n\n        info = {'serialnumber': self.status_config.get_child_by_name('serialnumber').get_value(),\n                'cameramodel': self.status_config.get_child_by_name('cameramodel').get_value(),\n                'deviceversion': self.status_config.get_child_by_name('deviceversion').get_value(),\n                'batterylevel': battery_level,\n                'remainingpictures': int(self.camera.get_storageinfo()[0].freeimages)}\n        return info\n\n    def take_picture(self, shutter_speed_index=None, color_temperature=None, volume=None):\n        \"\"\"\n        Set camera options and take picture\n        Blocking\n        :param shutter_speed_index: int in range 0-60 (0 fastest shutter)\n        :param color_temperature: in in range 2500-10000 by 100 increment\n        :param volume: int in range 0-100\n        :return: (jpg_path, dng_path)\n        \"\"\"\n        t1 = time.time()\n        if not self.camera_config:\n            self.init()\n\n        if shutter_speed_index is not None:\n            self.shutter_speed_config.set_value(self.shutter_speed_options[shutter_speed_index])\n\n        if color_temperature is not None:\n            self.other_config.get_child_by_name(COLOR_TEMPERATURE).set_value(color_temperature)\n\n        if volume is not None:\n            self.other_config.get_child_by_name(AUDIO_VOLUME).set_value(str(volume))\n\n        self.camera.set_config(self.camera_config)\n        # We need this even though no event is triggered\n        wait_for_event(self.camera)\n\n        gp_jpg_path = self.camera.capture(gp.GP_CAPTURE_IMAGE)\n\n        gp_dng_path = wait_for_event(self.camera, timeout=TIMEOUT_CAPTURE_DNG, event_type=gp.GP_EVENT_FILE_ADDED)\n        if not gp_dng_path:\n            raise RuntimeError('Unable to copy DNG')\n\n        jpg_path = os.path.join(gp_jpg_path.folder, gp_jpg_path.name)\n        dng_path = os.path.join(gp_dng_path.folder, gp_dng_path.name)\n\n        print('Capture took %0.03f sec' % (time.time() - t1, ))\n        return jpg_path, dng_path\n\n    def download_file(self, src_path, dst_path, delete=True):\n        \"\"\"Copy the file from the camera src_path to local dst_path\"\"\"\n        t1 = time.time()\n\n        src_folder, src_name = os.path.split(src_path)\n        src_file = self.camera.file_get(src_folder, src_name, gp.GP_FILE_TYPE_NORMAL)\n        print('Download %s -&gt;\\n\\t%s' % (src_path, dst_path))\n        src_file.save(dst_path)\n        wait_for_event(self.camera)\n        print('Download took %0.03f sec' % (time.time() - t1, ))\n\n        if delete:\n            t1 = time.time()\n            print('Delete %s' % src_path)\n            self.camera.file_delete(src_folder, src_name)\n            wait_for_event(self.camera)\n            print('Delete took %0.03f sec' % (time.time() - t1, ))\n\n\ndef _unittest():\n    \"\"\"test a short exposure sequence\"\"\"\n    # temporary directory\n    dst_template = '/tmp/theta/capture.%04d.%s'\n\n    t1 = time.time()\n    camera = CameraUsb()\n\n    camera.init()\n\n    print(camera.get_info())\n\n    frame = 1\n    jpg_path, dng_path = camera.take_picture(0)\n    print(jpg_path, dng_path)\n    camera.download_file(dng_path, dst_template % (frame, 'dng'))\n    frame += 1\n\n    jpg_path, dng_path = camera.take_picture(24)\n    print(jpg_path, dng_path)\n    camera.download_file(dng_path, dst_template % (frame, 'dng'))\n    frame += 1\n\n    jpg_path, dng_path = camera.take_picture(42)\n    print(jpg_path, dng_path)\n    camera.download_file(dng_path, dst_template % (frame, 'dng'))\n    frame += 1\n    print('Done in %0.03f sec' % (time.time() - t1, ))\n\n\nif __name__ == \"__main__\":\n\n    _unittest()\n</code></pre>"},{"location":"viewer/","title":"Image Viewer","text":"<p>FSPViewer is a free tool to view 360 images from the camera.</p> <p>It is comparable to the RICOH THETA desktop application viewer.</p>"},{"location":"viewer/#installation","title":"Installation","text":"<ul> <li>Download</li> </ul> <p>You may see the following error.</p> <pre><code>FSPViewer-2.1.0-64$ ./FSPViewer64 \n./FSPViewer64: error while loading shared libraries: libpng12.so.0: cannot open shared object file: No such file or directory\n</code></pre> <p>You can get the required libpng12.so.0 library from linuxuprising.</p> <pre><code>sudo add-apt-repository ppa:linuxuprising/libpng12\nsudo apt update\nsudo apt install libpng12-0\n</code></pre>"},{"location":"viewer/#usage","title":"Usage","text":"<p>Drag and drop file onto FSPViewer</p> <p></p>"},{"location":"meetup_archive/2020_09_01/","title":"September 1, 2020 - First Linux Meetup","text":"<p>45 people signed up and 17 people attended.  Due to the  US health situation, the meetup was online only using Zoom Pro. It  lasted 20 minutes past the planned 60 minutes due to active questions.</p> <p>In addition to configuration questions for libuvc-theta and  libuvc-theta-sample, we showed two Linux streaming  demos from a Jetson Nano with real-time processing using Python OpenCV.</p>"},{"location":"meetup_archive/2020_09_01/#slides","title":"Slides","text":"<p>A copy of the slides used in the meetup are here</p>"},{"location":"meetup_archive/2020_09_01/#community-projects-ros-and-opencv","title":"Community Projects - ROS and OpenCV","text":""},{"location":"meetup_archive/2020_09_01/#ros-package-for-ricoh-theta-s","title":"ROS package for RICOH THETA S","text":"<p>RICOH THETA S usage package with Ubuntu 16.04 and ROS Kinetic</p> <p>Robotic Intelligence Lab (RobInLab) ROS package</p>"},{"location":"meetup_archive/2020_09_01/#rgbd-stereo-360-camera","title":"rgbd stereo 360 camera","text":"<p>rgbd_stereo_360camera</p> <ul> <li>convert fisheye images to equirectangular projection</li> <li>obtain depth image from stereo equirectangular image pair using basic SGBM algorithm to estimate disparity</li> <li>depth information using WLS filter in addition to SGBM algorithm</li> </ul>"},{"location":"meetup_archive/2020_09_01/#q-and-a","title":"Q and A","text":""},{"location":"meetup_archive/2020_09_01/#how-do-i-load-v4l2loopback-automatically-when-i-reboot","title":"How do I load v4l2loopback automatically when I reboot?","text":"<p>In the file <code>/etc/modules-load.d/modules.conf</code> add a new line <code>v4l2loopback</code>. </p>"},{"location":"meetup_archive/2020_09_01/#how-do-i-save-video-to-file-with-the-usb-api","title":"How do I save video to file with the USB API?","text":"<p>If you are using the USB to save video to file (not streaming), this command works with the V and Z1.</p> <pre><code>ptpcam -R 0x101c,0,0,1\n</code></pre>"},{"location":"meetup_archive/2020_09_01/#can-i-use-openvslam","title":"Can I use OpenVSLAM?","text":"<p>It works with a video from file.  We didn't test it in the past with a live stream because we didn't have a way to get the live video onto Linux.  Please test it now and report back.  Discussion. </p> <p>Demo 3 in the video below is with a RICOH THETA V at 1920x960 with FPS of 10. Video</p>"},{"location":"meetup_archive/2020_09_01/#how-do-i-access-camera-acceleration-or-change-of-position","title":"How do I access camera acceleration or change of position?","text":"<p>You need to use a plug-in to access camera sensor data.  Discussion</p> <p>Related info is in this video for a real-time demo. Article summary in English.  GitHub repo here. Integrated with TensorFlow Lite using internal camera OS NDK.  There is a related article  here that shows camera Yaw and Pitch on the OLED of the Z1.  gist</p>"},{"location":"meetup_archive/2020_09_01/#do-you-have-plans-to-additional-deep-learning","title":"Do you have plans to additional deep learning?","text":"<p>We may based on additional feedback.  A good next step is to assess the VOC-360 image dataset that can be used to train models for 360 images.  More information on the model and how to download the dataset is here. TensorFlow demo running inside the camera is here. FDDB-360 contains 17,052 fisheye-looking images and a total of 26,640 annotated faces.</p>"},{"location":"meetup_archive/2020_09_01/#did-you-use-detectnet-on-4k-frames","title":"Did you use DetectNet on 4K frames?","text":"<p>(from Craig) For DetectNet, I was only able to test it at 2K on the Jetson Nano due to limited resources. I believe it works at 4k on Jetson Xavier.  But, I don't have a Jetson Xavier due to cost</p>"},{"location":"meetup_archive/2020_09_01/#is-motionjpeg-higher-per-frame-bitrate","title":"Is motionJPEG higher per frame bitrate?","text":"<p>Craig: The MotionJPEG is lower fps and higher latency, but it is convenient. There's a demo video of MotionJPEG here: https://youtu.be/5eSdqEudu5s  The code is available for testing.  Though MotionJPEG is not recommended for AI processing.  However, feel free to give it a try.</p>"},{"location":"meetup_archive/2020_09_01/#can-we-use-other-thetas-or-only-v","title":"Can we use other THETAs? or only V?","text":"<p>Craig: Z1 works</p>"},{"location":"meetup_archive/2020_09_01/#can-the-theta-s-be-used-for-live-streaming","title":"Can the THETA S be used for live streaming?","text":"<p>From craig to Everyone:  10:39 AM The V and Z1 are the only models that support live streaming of equirectangular The THETA S streams motionJPEG. It should work with Linux in dual-fisheye the V and Z1 stream in UVC 1.5 in equirectangular From john to Everyone:  10:41 AM yes, it works</p>"},{"location":"meetup_archive/2020_09_01/#there-is-latency-of-2-3-seconds-with-ubuntu-2004","title":"There is latency of 2-3 seconds with Ubuntu 20.04","text":"<p>John \u2192 Had latency of 2-3 seconds \u2192 Couldn\u2019t text it on 16-18 \u2192 How to reduce the latency, what changes need to be made on Ubuntu 16/18?  Craig: I used Ubuntu 20. Discrete GPU, John might be using software rendering (if it cannot use GPU). Craig uses x86, did not use software rendering </p> <p>From john to Everyone:  10:55 AM I saw your description in the link and tested for Ubuntu 20.04, but with latency. From john to Everyone:  10:55 AM But couldn't make it work in Ubuntu 16 and 18. Do you know of what modification is required? From craig to Everyone:  10:55 AM I didn't have latency. did you install the gstreamer full plug-in pack?</p> <p>After meetup, ran this test to show latency.</p>"},{"location":"meetup_archive/2020_09_01/#you-mention-it-doesnt-work-with-raspberry-pi-now-we-were-wondering-the-reason-is-it-because-the-os-or-the-cpu-architecture-ie-arm64","title":"you mention it doesn\u2019t work with Raspberry Pi now. We were wondering the reason.  Is it because the OS or the CPU architecture (i.e. arm64) ?","text":"<p>From Luke Lu to Everyone:  11:01 AM Thanks for your tutorial. In your forum, you mention it doesn\u2019t work with Raspberry Pi now. We were wondering the reason.  Is it because the OS or the CPU architecture (i.e. arm64) ? Craig: Cut down resolution and try again. Craig couldn\u2019t get it to work. Nano has DDR4, Pi3 has DDR3. Hardware acceleration. H.264 acceleration doesn\u2019t do decoding. Cut stream down to 2k and try again. Try Pi4 (newest), 2k, assess to see if it works. Speed of decoding is the limiting factor.</p>"},{"location":"meetup_archive/2020_09_01/#do-i-need-to-recompile-to-change-the-resolution","title":"Do I need to recompile to change the resolution","text":"<p>After the meetup, we ran this test using command line arguments.  Using the test, you can specify the resolution with:</p> <pre><code>$ ./gst_loopback --format 4K\nstart, hit any key to stop\n\n$ ./gst_loopback --format 2K\nstart, hit any key to stop\n</code></pre> <p>If the THETA is on <code>/dev/video2</code>, you can confirm resolution with:</p> <pre><code>$ v4l2-ctl --device /dev/video2 --list-formats-ext\nioctl: VIDIOC_ENUM_FMT\n    Type: Video Capture\n\n    [0]: 'YU12' (Planar YUV 4:2:0)\n        Size: Discrete 1920x960\n            Interval: Discrete 0.033s (30.000 fps)\n</code></pre> <p>Or if in 4K,</p> <pre><code>$ v4l2-ctl --device /dev/video2 --list-formats-ext\nioctl: VIDIOC_ENUM_FMT\n    Type: Video Capture\n\n    [0]: 'YU12' (Planar YUV 4:2:0)\n        Size: Discrete 3840x1920\n            Interval: Discrete 0.033s (30.000 fps)\n</code></pre>"}]}