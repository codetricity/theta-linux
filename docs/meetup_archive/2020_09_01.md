# September 1, 2020 - First Linux Meetup


45 people signed up and 17 people attended.  Due to the 
US health situation, the meetup was online only using Zoom Pro. It 
lasted 20 minutes past the planned 60 minutes due to active
questions.

In addition to configuration questions for libuvc-theta and 
libuvc-theta-sample, we showed two Linux streaming 
demos from a Jetson
Nano with real-time processing using Python OpenCV.

## Slides

A copy of the slides used in the meetup are [here](https://docs.google.com/presentation/d/11bNQEhwfHwKTzSaw6HM30L6Okly3vuIvbNiyo6LPafU/edit?usp=sharing)


## Q and A

### How do I save video to file with the USB API?

If you are using the USB to save video to file (not streaming), this command works with the V and Z1.

``` 
ptpcam -R 0x101c,0,0,1
```

### Can I use OpenVSLAM?

It works with a video from file.  We didn't test it in the past with a live stream because we didn't have a way to get the live video onto Linux.  Please test it now and report back.  [Discussion](https://community.theta360.guide/t/slam-with-ricoh-theta-using-openvslam/5104). 

Demo 3 in the video below is with a RICOH THETA V at 1920x960 with FPS of 10.
[Video](https://www.youtube.com/watch?v=Ro_s3Lbx5ms&feature=emb_logo)

### How do I access camera acceleration or change of position?

You need to use a plug-in to access camera sensor data. 
[Discussion](https://community.theta360.guide/t/howto-use-theta-motion-sensors/4145)

Related info is in
[this video](https://youtu.be/EfnU7E-jsQk) for a real-time demo. Article summary in English.  GitHub repo [here](https://community.theta360.guide/t/extended-livepreview-sample-code-for-theta-plug-in-with-webui/5272?u=craig).
Integrated with TensorFlow Lite using internal camera OS NDK. 
There is a related article 
[here](https://community.theta360.guide/t/tensorflow-lite-object-recognition-with-theta-plug-in/5387) that shows camera Yaw and Pitch on the OLED of the Z1. 
[gist](https://gist.github.com/codetricity/ef2e4f99739efd81c1a5786570d23d77)

### Do you have plans to additional deep learning?

We may based on additional feedback.  A good next step is to assess the VOC-360 image dataset that can be used to train models for 360 images.  More information on the model and how to download the dataset is
[here](http://www.sfu.ca/~ibajic/#data). TensorFlow demo running inside the camera is
[here](https://community.theta360.guide/t/theta-auto-trigger-plug-in-by-amine-amri/4665?u=craig). FDDB-360 contains 17,052 fisheye-looking images and a total of 26,640 annotated faces.

